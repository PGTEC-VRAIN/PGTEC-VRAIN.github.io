{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Information of PGTEC <p>A data platform that periodically retrieves climate  and weather historical data and predictions from multiple APIs, standardizes them using Smart Data Models and stores them in a FIWARE Context Broker with historical persistence for machine learning and deep learning models.</p> <ul> <li> <p> Getting started</p> <p>Start by learning the basics of PGTEC.</p> <p> Learn more</p> </li> <li> <p> Data Sources</p> <p>Information of data sources for climate emergency management.   </p> <p> Learn more</p> </li> <li> <p> Data Flow</p> <p>The guide of a complete environment for the project.</p> <p> Learn more</p> </li> <li> <p> Smart Data Models </p> <p>Description of the data models used.</p> <p> Learn more</p> </li> <li> <p> Smart Flow</p> <p>Python-based scripts designed with Airflow and FastAPI to manage and automate data processing pipelines.</p> <p> Learn more</p> </li> </ul>"},{"location":"welcome/","title":"Welcome","text":"<p>This site serves as a comprehensive documentation hub for the PGTEC project.  </p>"},{"location":"welcome/#about-pgtec","title":"About PGTEC","text":"<p>Following the devastating consequences of the DANA floods of 2024 in the province of Valencia, PGTEC aims to develop a platform for climate emergency prevention and management, providing advanced tools for the collection, analysis, and modeling of environmental and climate data.  </p> <p>This solution is integrated into interoperable data spaces, enabling public administrations, emergency management agencies, and companies in the climate sector to access real-time information for informed decision-making.</p>"},{"location":"welcome/#platform","title":"Platform","text":"<p>The platform will contribute to the digitalization of the climate resilience and emergency management sector, fostering the creation of data-driven products and services aligned with the principles of the European Data Strategy and the Recovery, Transformation and Resilience Plan.</p>"},{"location":"welcome/#beneficiary-sectors","title":"Beneficiary sectors","text":"<ul> <li>Emergency management and civil protection</li> <li>Environment and climate change  </li> <li>Public administration and smart cities </li> <li>Mobility and logistics</li> </ul> <p>Next steps</p> <p>Click on Getting started in the bottom navigation bar to advance to the next section.</p> <p>You can use the buttons in the bottom navigation bar to navigate between the previous and next pages or jump to a section with the side navigation bars.</p>"},{"location":"DID/","title":"Decentralized Identifiers (DIDs)","text":"<p>Decentralized Identifiers (DIDs) are a core component of Self-Sovereign Identity (SSI) systems, providing a unique, persistent, and cryptographically verifiable way to identify any entity (person, organization, device, etc.) without relying on a central authority.</p> <p>A DID is a simple URI with three parts: the scheme (did:), the DID method identifier, and the method-specific identifier. For example, in did:web:example.com, web is the method identifier.</p> <p>DIDs resolve to a DID Document, which is a JSON file containing public keys (for authentication and verification) and service endpoints (for communication).</p>"},{"location":"DID/#didweb-method","title":"did:web method","text":"<p>The did:web method leverages existing web infrastructure, specifically HTTPS and Domain Name System (DNS), to create and resolve DIDs.</p> <p>A did:web DID is constructed directly from a domain name (and optional path), making it highly readable and familiar. For example: <code>did:web:example.com</code></p> <p>Resolution: The DID document is hosted on the corresponding web server at a well-known location like this: <code>https://example.com/.well-known/did.json</code></p>"},{"location":"DID/#didkey-method","title":"did:key method","text":"<p>The did:key method is the simplest form of DID, as the identifier is directly derived from a cryptographic public key.</p> <p>The DID method-specific identifier is an encoded representation of the public key material. For example: <code>did:key:z6MkjBWPPa1njEKygyr3LR3pRKkqv714vyTkfnUdP6ToFSH5</code></p> <p>Resolution: The DID document can be generated entirely from the DID string itself without requiring any external network lookup. It is self-contained.</p>"},{"location":"DID/#automated-setup-with-docker","title":"Automated setup with Docker","text":"<p>We use a docker image to create the DIDs from a key pair and a certificate.</p> <p>We can create the key pair and the certificate like this:</p> <pre><code>mkdir example\n\n# Create a key pair.\nopenssl ecparam -genkey -name prime256v1 -noout -out example/key-pair.pem\n\n# That can be read like this:\nopenssl ec -in example/key-pair.pem -text -noout\n\n# Create a self-signed certificate from the key pair.\nopenssl req -x509 -new -key example/key-pair.pem -out example/cert.pem -days 365 -nodes\n\n# That can be read like this:\nopenssl x509 -in example/cert.pem -text -noout\n</code></pre>"},{"location":"DID/#didweb-setup","title":"did:web setup","text":"<p>Once we have the certificate associated to our key pair we can generate the did.json running a docker container:</p> <pre><code>docker run -p 8080:8080 -v ./example/cert.pem:/certs/tls.crt -it mortega5/did-helper:0.0.1 -didType web -hostUrl example.com -outputFormat json_jwk -certPath /certs/tls.crt\n</code></pre> <p>This will print out the contents of a JSON file. That content should be accessible from <code>example.com/.well-known/did.json</code> if we are using a home page like example.com.</p> <p>If we want to use a deep link we have to run the docker container with the full link:</p> <pre><code>docker run -p 8080:8080 -v ./example/cert.pem:/certs/tls.crt -it mortega5/did-helper:0.0.1 -didType web -hostUrl example.com/specific/page -outputFormat json_jwk -certPath /certs/tls.crt\n</code></pre> <p>And make the JSON that it prints accessible from <code>example.com/specific/page/did.json</code>. The idintifier in this case would be did:web:example.com:specific:page</p>"},{"location":"DID/#didkey-setup","title":"did:key setup","text":"<p>Once we have the certificate associated to our key pair we can generate the did:key and did.json by running a docker container:</p> <pre><code>docker run -p 8080:8080 -v ./example/cert.pem:/certs/tls.crt -v ./example/key-pair.pem:/certs/tls.key -it mortega5/did-helper:0.0.2 -didType key -certPath /certs/tls.crt -keyPath /certs/tls.key\n</code></pre> <p>If we get the error <code>exec /did-helper/did-helper: exec format error</code> we have to run:</p> <pre><code>docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n</code></pre> <p>before rerunning the docker.</p> <p>We don't need to host the did:key's did.json anywhere. We just use out identifier and use out private key (stored in example/key-pair.pem) to prove we are the holders of that did:key.</p>"},{"location":"DID/#license","title":"License","text":"<p>Distributed under the AGPL-3.0 License. See <code>LICENSE</code> for more information.</p>"},{"location":"DID/#contact","title":"Contact","text":"<p>Project Link: https://github.com/PGTEC-VRAIN</p>"},{"location":"SmartDataModels/","title":"Smart Data Models","text":""},{"location":"SmartDataModels/#overview","title":"Overview","text":"<p>The Smart Data Models initiative, led by FIWARE Foundation, provides a set of open and interoperable data models designed to promote common standards for data exchange. These models define the structure, semantics, and units of measurement for key environmental and urban variables, allowing data to be easily reused and understood across multiple platforms and applications.</p> <p>Within the PGTEC project, the adoption of Smart Data Models plays a key role in ensuring that data collected from diverse environmental and meteorological sources can be seamlessly integrated and understood. The project aims to harmonize and translate heterogeneous datasets\u2014originating from different agencies and monitoring systems\u2014into a unified, standardized format that facilitates interoperability, data sharing, and contextual analysis. By using standardized FIWARE-based schemas, the project ensures that variables\u2014such as temperature, precipitation, or relative hummidity, among others, are represented consistently, regardless of their origin. This harmonization makes easier the training of the TETIS hydrological model using different data sources such as Open-Meteo or AEMET. It also enables more accurate analysis and improved decision-making.</p>"},{"location":"SmartDataModels/#smart-data-models-for-pgtec","title":"Smart Data Models for PGTEC","text":"<p>To achieve this, PGTEC implements and extends several Smart Data Models tailored to the project\u2019s specific needs. The main models used include WeatherObserved, which harmonizes meteorological real time data from different meteorological agencies; WeatherForecastSeries, an extended version of the FIWARE WeatherForecast model adapted to handle meteorological predictions; and StreamFlow, a new model specifically designed to represent hydrological variables such as flow and gauging data from the Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar (CHJ).</p> <p>As mention above, three different Smart Data Models have been designed, each focused on a specific type of data source. These include real-time data sources, climate model forecast sources, and hydrological data sources.</p> <ul> <li> <p>WeatherObserved: Fiware smart data model used to translate all the real time data. For more information about the model go to the json schema. This model will be used in different climate data sources such as:</p> </li> <li> <p>AEMET: Agencia Estatal de Meteorolog\u00eda</p> </li> <li>CHJ: Confederaci\u00f3n Hidrogr\u00e1frica del J\u00facar</li> <li>SiAR: Sistema de Informaci\u00f3n Agroclim\u00e1tica para el regad\u00edo</li> <li> <p>AVAMET: Agencia Estatal de Meteorolog\u00eda</p> </li> <li> <p>WeatherForecastSeries:. An extension of the smart data model WeatherForecast adaptad to time series data. Fore more information visit the json schema.</p> </li> <li> <p>StreamFlow: A new Smart Data Model focused on CHJ hydrometeorological data. It standardizes variables such as gauging and flow rates into a consistent format using the FIWARE Smart Data Models ontology. For deep understanding of the structure, visit the github.</p> </li> </ul> <p></p>"},{"location":"SmartDataModels/StreamFlow/","title":"StreamFlow","text":"<p>This section presents a proposal for the creation of a new Smart Data Model designed to represent hydrological information from the Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar (CHJ). The objective is to standardize the collection, integration, and exchange of river discharge and reservoir data within the PGTEC project framework. Specifically, data from three different types of hydrological data will be stored:</p> <ul> <li>River discharge data</li> <li>Precipitation data</li> <li>Reservoirs data</li> </ul>"},{"location":"SmartDataModels/StreamFlow/#consequently-the-proposed-smart-data-model-will-serve-to-standardize-diverse-climatic-data-all-sourced-from-the-chj","title":"Consequently, the proposed Smart Data Model will serve to standardize diverse climatic data, all sourced from the CHJ.","text":""},{"location":"SmartDataModels/StreamFlow/#purpose","title":"Purpose","text":"<p>The StreamFlow model aims to provide a unified structure to represent real-time and historical hydrological variables obtained from CHJ monitoring stations. By aligning with the FIWARE Smart Data Models ontology, this new model ensures semantic interoperability with other environmental and meteorological datasets used in the project.</p>"},{"location":"SmartDataModels/StreamFlow/#main-variables-of-interest","title":"Main Variables of Interest","text":"Parameter Description Unit Inflow Volume of incoming river water m\u00b3/s Outflow Volume of outgoing river water m\u00b3/s Water Level Elevation of the river or reservoir surface m Volume Total stored water in the reservoir m\u00b3 Filling Percentage Calculated percentage of capacity (0.0 to 1.0) % Water Flow Rate of water discharge at a specific point m\u00b3/s Stream Flow Flow rate of the specific stream/tributary m\u00b3/s Sediment Flow Rate of sediment transport in the water kg/s Precipitation Amount of water in 1 hour mm/s <p>These variables are critical for hydrological modeling, water resource management, and integration with simulation tools such as TETIS. These data will be used for getting the initial conditions of the basins of interest. These initial conditions are part of the TETIS input data. The rest of the data needed to train the TETIS model are the predictions that will come from another data sources such as Open-Meteo. To understand better the predictions environment created to extract different forecasts from a lot of climate models check the SmartFlow Environment.</p>"},{"location":"SmartDataModels/StreamFlow/#model-definition","title":"Model Definition","text":"<p>The proposed StreamFlow entity introduces the necessary attributes and metadata fields to harmonize CHJ hydrological data with the FIWARE ecosystem. It supports contextual information management through the Orion Context Broker, enabling real-time ingestion and storage into the TimeScaleDB which is a Postgres database designed for time series data that enables the historization of real time data for future uses such as dashboards or training new climate models. For more information about the real time data pipeline go to the DataFlow section.</p>"},{"location":"SmartDataModels/StreamFlow/#repository","title":"Repository","text":"<p>The model implementation in python can be found in the following Github repository:</p> <p>\ud83d\udd17 GitHub \u2013 StreamFlow Model Definition</p>"},{"location":"SmartDataModels/WeatherForecastSeries/","title":"WeatherForecastSeries","text":"<p>The WeatherForecastSeries Smart Data Model is an extension of the official WeatherForecast model from FIWARE\u2019s Smart Data Models initiative. Its main goal is to enable the representation of forecast time series (rather than a single prediction) within a single entity.</p>"},{"location":"SmartDataModels/WeatherForecastSeries/#key-modifications","title":"Key Modifications","text":"<ul> <li> <p>Array-based attributes   Most of the variable types defined in the original <code>WeatherForecast</code> model have been converted into arrays of their base type.   This structure allows the model to store multiple forecasted values (e.g., temperature, wind speed, precipitation) corresponding to different timestamps for a single point.</p> </li> <li> <p>Inlined definitions   Some variables originally referenced from Weather-Commons have now been defined directly inside the WeatherForecastSeries schema, improving self-containment and making the model easier to use independently.</p> </li> <li> <p>New <code>timestamps</code> attribute   A new property, <code>timestamps</code>, has been introduced to specify the exact prediction times corresponding to each value in the variable arrays.   This makes it possible to directly associate each forecasted parameter with its valid time, simplifying temporal analysis.</p> </li> </ul>"},{"location":"SmartDataModels/WeatherForecastSeries/#purpose-and-advantages","title":"Purpose and Advantages","text":"<p>This model enables the aggregation of complete forecast time series (e.g., hourly or daily predictions) in a single JSON object, reducing redundancy and improving data retrieval efficiency.</p> <p>In the PGTEC project, it is used to:</p> <ul> <li>Store weather predictions retrieved from different data sources such as AEMET, Open-Meteo, and Copernicus.  </li> <li>Provide these standardized forecasts as input for the TETIS hydrological model, ensuring that all variables share a common format and temporal alignment.  </li> <li>Facilitate interoperability between APIs, Airflow workflows, and visualization dashboards.</li> </ul>"},{"location":"SmartDataModels/WeatherForecastSeries/#example-entity","title":"Example Entity","text":"<p>Below is an example of a <code>WeatherForecastSeries</code> entity representing 3-hourly weather predictions for a given location:</p> <pre><code>{\n  \"id\": \"WeatherForecastSeries:Valencia:2025-03-01\",\n  \"type\": \"WeatherForecastSeries\",\n  \"location\": {\n    \"type\": \"Point\",\n    \"coordinates\": [-0.3763, 39.4699]\n  },\n  \"timestamps\": [\n    \"2025-03-01T00:00:00Z\",\n    \"2025-03-01T03:00:00Z\",\n    \"2025-03-01T06:00:00Z\"\n  ],\n  \"temperature\": [14.5, 13.8, 13.1],\n  \"precipitation\": [0.0, 0.2, 1.1],\n  \"windSpeed\": [3.4, 2.8, 4.1],\n  \"relativeHumidity\": [65, 70, 72],\n  \"source\": \"AEMET\",\n  \"address\": {\n    \"addressLocality\": \"Valencia\",\n    \"addressCountry\": \"ES\"\n  },\n  \"dateIssued\": \"2025-03-01T00:00:00Z\",\n  \"validFrom\": \"2025-03-01T00:00:00Z\",\n  \"validTo\": \"2025-03-01T06:00:00Z\"\n}\n</code></pre>"},{"location":"SmartDataModels/WeatherObserved/","title":"WeatherObserved","text":"<p>The WeatherObserved Smart Data Model is used to represent real-time meteorological observations within the PGTEC project. It follows the official FIWARE definition and does not require any structural modification, ensuring full compatibility and interoperability with other FIWARE-based systems.</p> <p>This model serves as the standard framework for integrating real-time data collected from multiple external sources, including:</p> <ul> <li>AEMET \u2014 Agencia Estatal de Meteorolog\u00eda</li> <li>CHJ \u2014 Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar </li> <li>AVAMET \u2014 Agencia Valenciana de Meteorolog\u00eda</li> <li>SiAR \u2014 Sistema de informaci\u00f3n agroclim\u00e1tica para el regad\u00edo</li> </ul> <p>By adopting the WeatherObserved schema, all incoming data are harmonized under a common structure, ensuring consistency in variable names, measurement units, and metadata representation across the entire data ecosystem.</p>"},{"location":"SmartDataModels/WeatherObserved/#main-variables-of-interest","title":"Main Variables of Interest","text":"<p>The following table describes the most important variables used in the PGTEC project:</p> Description FIWARE Attribute Unit Precipitation <code>precipitation</code> mm, l/m\u00b2 Air temperature <code>temperature</code> \u00b0C Relative humidity <code>relativeHumidity</code> % Snow depth <code>snowHeight</code> mm Wind speed <code>windSpeed</code> m/s, km/h"},{"location":"SmartDataModels/WeatherObserved/#role-within-pgtec","title":"Role within PGTEC","text":"<p>Within PGTEC, the WeatherObserved model is primarily used for the ingestion of real-time meteorological data. This approach ensures a unified and interoperable data layer, allowing seamless integration between the Airflow pipelines, FastAPI services, and the TETIS dashboard, where users can visualize or request the latest standardized weather observations.</p>"},{"location":"SmartFlow/","title":"Smart Flow","text":""},{"location":"SmartFlow/#about-the-project","title":"About The Project","text":"<p>This section is part of the tasks developed within the PGTEC project. Its main objective is to describe and provide the infrastructure required to deploy a data space using FIWARE technologies, offering a detailed and easy-to-follow guide adaptable to different environments. Specifically, it explains the logic used to download data from multiple sources, transform it into a common language using Smart Data Models, and make it available in two complementary ways:</p> <ul> <li> <p>Airflow workflows: Python scripts that use Airflow logic to automate the execution of data retrieval and transformation tasks, ensuring that climate predictions are periodically processed and ready to use.</p> </li> <li> <p>FastAPI services: Python scripts built with FastAPI that expose the processed data through RESTful APIs. These services feed a dashboard where users can select the data sources and points of interest required to run the TETIS hydrological model. Once the user makes a selection, TETIS automatically triggers the corresponding FastAPI Python scripts described in this section to retrieve and process the climate predictions, which are then used as input for the model\u2019s execution.</p> </li> </ul> <p>This section specifically describes the Python scripts used to:</p> <ul> <li> <p>Retrieve data from multiple climate data sources such as AEMET, CHJ, Open-Meteo, and Copernicus.</p> </li> <li> <p>Convert the raw data into FIWARE Smart Data Models to standardize the format.</p> </li> <li> <p>The creation of automated Airflow DAGs for pipeline execution.</p> </li> <li> <p>The creation of FastAPI scripts to provide data as a service neede to provide a dashboard where users select the data sources to run TETIS model (Hydrological model from IIAMA-UPV)</p> </li> </ul> <p>All the python files are in the SmartFlow Github Repository</p>"},{"location":"SmartFlow/#built-with","title":"Built With","text":"<p>The project is built using the following main components:</p>"},{"location":"SmartFlow/#getting-started","title":"Getting Started","text":"<p>To get a local copy up and running follow these simple steps in ubuntu command line:</p> <ol> <li>Clone the repo and navigate to the project folder</li> </ol> <pre><code>git clone https://github.com/PGTEC-VRAIN/SmartFlow\ncd SmartFlow\n</code></pre> <ol> <li>Initialize docker:</li> </ol> <pre><code>sudo systemctl start docker\n</code></pre> <ol> <li>Initialize docker containers    <pre><code>docker compose up --build -d\n</code></pre></li> </ol>"},{"location":"SmartFlow/#airflow-data-sources-overview","title":"Airflow Data Sources Overview","text":"<p>This section contains links to detailed explanations of the Python scripts that programmatically download forecasts from different models using Airflow. Specifically, each script description includes:</p> <ul> <li> <p>The name of the Python file in the SmartFlow Github Repository</p> </li> <li> <p>The data source accessed and the variables of interest</p> </li> <li> <p>The Smart Data Model used to standardize the data</p> </li> <li> <p>The required API key (if applicable)</p> </li> <li> <p>The suggested execution frequency to keep the data up to date</p> </li> </ul> <ul> <li> <p> HARMONIE/AROME- AEMET</p> </li> <li> <p> ARPEGE \u2013 OpenMeteo</p> </li> <li> <p> DWD_ICON \u2013 OpenMeteo</p> </li> <li> <p> AIFS_ECMWF \u2013 OpenMeteo</p> </li> <li> <p> GEPS_ENS_CNC \u2013 OpenMeteo</p> </li> <li> <p> GFS_NOAA \u2013 OpenMeteo</p> </li> <li> <p> IFS9km_ECMWF \u2013 OpenMeteo</p> </li> <li> <p> Seas5_ECMWF \u2013 OpenMeteo</p> </li> <li> <p> EFAS \u2013 Copernicus</p> </li> <li> <p> EFFIS \u2013 Copernicus</p> </li> </ul>"},{"location":"SmartFlow/#usage","title":"Usage","text":"<p>This section describes how to run and test the main components of the PGTEC platform \u2014 the Airflow workflows for data processing and the FastAPI services for data delivery and dashboard integration. Both components can be deployed together using the provided Docker Compose environment.</p> <p>It is supposed that the enviroment has been cloned following the instructions of Getting Started section.</p>"},{"location":"SmartFlow/#1-running-airflow-workflows","title":"1. Running Airflow Workflows","text":"<p>The Airflow DAGs automate the process of retrieving and transforming climate and hydrological data from different sources (AEMET, CHJ, Open-Meteo, Copernicus...).</p> <p>\ud83e\udde9 Steps</p> <p>1.1 Start the containers:</p> <pre><code>docker-compose up -d\n</code></pre> <p>The -d option runs the containers in detached mode, hiding Airflow logs and keeping the terminal clean.</p>"},{"location":"SmartFlow/#11-access-the-airflow-web-interface","title":"1.1. Access the Airflow web interface:","text":"<p>\ud83d\udc49 http://localhost:8080</p>"},{"location":"SmartFlow/#12-enable-the-dags","title":"1.2. Enable the DAGs:","text":"<p>Inside the Airflow UI, activate the desired workflows (e.g., AEMET_HARMONIE_AROME, AIFS_ECMWF, etc.).</p>"},{"location":"SmartFlow/#13-monitor-execution","title":"1.3. Monitor execution:","text":"<p>You can visualize the data extraction and transformation progress directly in the DAG view.</p> <p>Below is an example of a Python script running in the Airflow user interface, showing the logs of its execution:</p> <p></p> <p>In the screenshot, the DWD_ICON.py workflow is being executed. In just 4.04 seconds, it retrieves several points of interest from the Valencian Community and stores them using the WeatherForecastSeries Smart Data Model format.</p>"},{"location":"SmartFlow/#2-running-fastapi-scripts","title":"2. Running FastAPI scripts.","text":"<p>The FastAPI services expose the processed data as REST APIs, allowing other applications \u2014 such as the TETIS dashboard \u2014 to access the latest standardized data stored in the FIWARE Context Broker.</p> <p>Each service corresponds to a specific data source or Smart Data Model and can be easily extended to include new ones.</p> <p>\ud83e\udde9 Steps</p>"},{"location":"SmartFlow/#21-make-sure-the-docker-environment-is-running","title":"2.1. Make sure the Docker environment is running:","text":"<pre><code>docker-compose up -d\n</code></pre>"},{"location":"SmartFlow/#22-access-the-fastapi-documentation","title":"2.2. Access the FastAPI documentation:","text":"<p>\ud83d\udc49 http://localhost:8000/docs</p> <p>This interface allows you to explore and test all available endpoints interactively.</p>"},{"location":"SmartFlow/#23-test-an-endpoint","title":"2.3. Test an endpoint:","text":"<p>For example, you can retrieve the latest weather forecast data by calling:</p> <pre><code>GET /weather?source=AEMET&amp;variable=temperature\n</code></pre>"},{"location":"SmartFlow/#24-integration-with-the-tetis-dashboard","title":"2.4 Integration with the TETIS dashboard:","text":"<p>The FastAPI services feed the TETIS dashboard, allowing users to select:</p> <ul> <li> <p>The data sources (e.g., AEMET, DWD, ECMWF)</p> </li> <li> <p>The points of interest (catchments, stations, or coordinates)</p> </li> </ul> <p>Once the user selects these options, TETIS automatically triggers the corresponding API calls \u2014 which execute the same Python scripts described in the SmartFlow section \u2014 to retrieve and process the forecast data used as inputs for hydrological simulations.</p> <p>\ud83d\udcc1 Example folder structure</p> <pre><code>SmartFlow/FastAPI/\n\u251c\u2500\u2500 main.py           # FastAPI entry point\n\u251c\u2500\u2500 routes/\n\u2502   \u251c\u2500\u2500 AEMET.py      # Endpoints for WeatherObserved / Forecast data\n\u2502   \u251c\u2500\u2500 DWD_ICON.py   # Endpoints for CHJ flow data\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 models/\n    \u251c\u2500\u2500 AEMET.py\n    \u2514\u2500\u2500 DWD_ICON.py\n</code></pre> <p>\ud83e\udde0 Tip: You can add new endpoints easily by creating a new Python file in the routes/ folder and registering it in main.py.</p>"},{"location":"SmartFlow/#license","title":"License","text":"<p>Distributed under the AGPL-3.0 License. See <code>LICENSE</code> for more information.</p>"},{"location":"SmartFlow/#contact","title":"Contact","text":"<p>Project Link: https://github.com/PGTEC-VRAIN</p>"},{"location":"SmartFlow/#references","title":"References","text":"<ul> <li>Readme Template</li> <li>Smart Data Models Weather Smart Data Model - Fiware</li> </ul>"},{"location":"SmartFlow/metadata_scripts/AIFS_ECMWF/","title":"Model AIFS - Europe Centre Medium Weather Forecasts","text":"<p>This section describes the AIFS data integration process.</p> <ul> <li> <p><code>AIFS_ECMWF.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/ARPEGE/","title":"Model ARPEGE - MeteoFrance","text":"<p>This section describes the ARPEGE data integration process.</p> <ul> <li> <p><code>ARPEGE.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Relative Humidity</li> <li>Solar Radiation</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/DWD_ICON/","title":"Model ICON_EU - DWD","text":"<p>This section describes the ICON_EU data integration process.</p> <ul> <li> <p><code>DWD_ICON_EU.py</code>: Python script that retrieves weather data from the Open-Meteo API programmatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul> <p>\u2b05\ufe0f Back to overview</p>"},{"location":"SmartFlow/metadata_scripts/EFAS/","title":"Model EFAS  - Copernicus","text":"<p>This section describes the data integration process of EFAS model from Copernicus Early Warning Data Store (EWDS).</p> <ul> <li> <p><code>EFAS.py</code>: Python script that retrieves weather data from the EWDS dataset (https://ewds.climate.copernicus.eu/datasets/efas-forecast?tab=overview) and download data programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are NetCDF (.nc) files containing the following weather variable:</p> <ul> <li>River discharge last 6 hours</li> </ul> </li> <li> <p>API Key: An API key is required for execution. To get an API key, you need to register on the Copernicus Climate Data Store website: https://cds.climate.copernicus.eu</p> </li> <li> <p>Run script: Run this script daily because Copernicus updates the data once a day. It has been configured to automatically detect the latest available forecast.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/EFFIS/","title":"Model EFFIS  - Copernicus","text":"<p>This section describes the data integration process of EFFIS model from Copernicus Climate Data Store (CDS).</p> <ul> <li> <p><code>EFFIS.py</code>: Python script that retrieves weather data from the dataset (https://cds.climate.copernicus.eu/datasets/sis-tourism-fire-danger-indicators?tab=overview) and download data programamatically using airflow sintaxis. </p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are NetCDF (.nc) files containing various weather variables:</p> <ul> <li>Daily fire weather index</li> </ul> </li> <li> <p>API Key: An API key is required for execution. To get an API key, you need to register on the Copernicus Climate Data Store website: https://cds.climate.copernicus.eu</p> </li> <li> <p>Run script: Run this script daily because Copernicus updates the data once a day. It has been configured to automatically detect the latest available forecast.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/GEPS_ENS_CNC/","title":"Model GEPS Ensemble - Canadian Meteorological Centre","text":"<p>This section describes the data integration process of GEPS Ensemble model.</p> <ul> <li> <p><code>GEPS_ENS_CNC.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/GFS_NOAA/","title":"Model GFS  - National Oceanic and Atmospheric Administration (NOAA)","text":"<p>This section describes the data integration process of GFS model.</p> <ul> <li> <p><code>GFS_NOAA.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed</li> <li>Solar Radiation</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/Harmonie_AEMET/","title":"Model HARMONIE/AROME - AEMET","text":"<p>This section describes the AEMET data integration process.</p> <ul> <li> <p><code>AEMET_HARMONIE_AROME.py</code>: Python script that retrieves weather forecast data from the AEMET website and processes it programatically using airflow sintaxis</p> </li> <li> <p>Data: All the data is processed into a standardized format defined by the <code>WeatherForecastSeries.py</code> Smart Data Model. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed </li> </ul> </li> <li> <p>Raw Data: The input data are GeoTIFF (.tif) files containing weather variables encoded as color values. Using a color scale provided by AEMET, the script converts these color codes (RGBA) into real physical values such as temperature, wind speed, and precipitation.</p> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from AEMET website.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul> <p>\u2b05\ufe0f Back to overview</p>"},{"location":"SmartFlow/metadata_scripts/IFS9km_ECMWF/","title":"Model IFS  - Europe Centre Medium Weather Forecasts (ECMWF)","text":"<p>This section describes the data integration process of IFS model.</p> <ul> <li> <p><code>IFS9km_ECMWF.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/Seas5_ECWMF_copernicus/","title":"Model Seas5  - Europe Centre Medium Weather Forecasts (ECMWF)","text":"<p>This section describes the data integration process of Seas5 model from Copernicus.</p> <ul> <li> <p><code>Seas5_ECWMF_copernicus.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed</li> <li>Solar Radiation</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul> <p>The last two scrips retrieve data from Copernicus Climate Data Store (CDS) API and Early Warning Data Store (EWDS):</p>"},{"location":"data_sources/","title":"Information","text":"<p>This section will detail the different data sources available to feed the prediction models and algorithms that will enable us to anticipate extreme weather events, thereby contributing to a more effective response to meteorological emergencies. Their usefulness, quality and availability will be evaluated.</p>"},{"location":"data_sources/#data-sources","title":"Data Sources","text":"<ul> <li> <p> Conferencia Hidrogr\u00e1fica del J\u00facar \u2013 SAIH</p> <p>Public body responsible for water management in the J\u00facar River Basin District.</p> <p> Learn more</p> </li> <li> <p> Sistema de Informaci\u00f3n Agroclim\u00e1tica para el Regad\u00edo (SiAR)</p> <p>Network of over 520 weather stations that captures, records, and disseminates agroclimatic data necessary to determine the water demand of irrigated farms.</p> <p> Learn more</p> </li> <li> <p> Agencia Estatal de Meteorolog\u00eda (AEMET)</p> <p>Official Spanish government agency responsible for observing, forecasting and studying meteorological phenomena.  </p> <p> Learn more</p> </li> <li> <p> AEMET (ROCIO)</p> <p>AEMET Observational Grid with Optimal Interpolation. </p> <p> Learn more</p> </li> <li> <p> AEMET (Spain02)</p> <p>Institutional collaboration between AEMET and the Santander Meteorology Group (SMG) at the University of Cantabria-CSIC.  </p> <p> Learn more</p> </li> <li> <p> Copernicus</p> <p>Project coordinated and managed by the European Commission.</p> <p> Learn more</p> </li> <li> <p> MSWEP</p> <p>Multi-Source Weighted-Ensemble Precipitation (MSWEP) is a high-resolution global precipitation dataset.  </p> <p> Learn more</p> </li> <li> <p> CEDEX</p> <p>It is the centre for studies and experimentation in public works.</p> <p> Learn more</p> </li> <li> <p> AVSRE</p> <p>The Valencian Agency for Security and Emergency Response (AVSRE) manages civil protection, emergency management, firefighting and public safety in the Valencian Community.</p> <p> Learn more</p> </li> </ul>"},{"location":"data_sources/metadata_datasources/aemet_datasource/","title":"Agencia Estatal de Meteorolog\u00eda (AEMET)","text":"<p>The State Meteorological Agency (AEMET) is the official Spanish government body responsible for observing, predicting and studying meteorological phenomena. As the main authority, AEMET provides reliable, consistent and standardised data, following the guidelines of the World Meteorological Organisation (WMO). </p> <p>Its AEMET OpenData service is the designated platform for disseminating this information, offering access to a wide range of meteorological and climatological products. The quality of its data is backed by rigorous validation processes and by the National Plan for the Prediction and Monitoring of Adverse Meteorological Phenomena (Meteoalerta), which establishes the risk thresholds for alerts. </p> <p>In the Valencian Community, there are approximately 40 meteorological stations that transmit data on a regular basis and are therefore used for live weather monitoring. </p> <p>It should be noted that, although real-time data is subject to automatic checks, it may contain occasional errors. Even so, the data collected by AEMET is considered to be of high quality due to its official nature. Consolidated climate data undergoes more exhaustive validation processes before its final publication. </p>"},{"location":"data_sources/metadata_datasources/aemet_datasource/#api","title":"API","text":"<p>The AEMET API is based on REST architecture and returns data mainly in JSON format, using international standards such as CAP (Common Alerting Protocol) for warnings and GeoJSON to represent affected areas.</p> <p>Data is available on:</p> <ul> <li>Meteorological observations</li> <li>Weather forecasts</li> <li>Radar and lightning</li> <li>Alerts</li> </ul> <p>With regard to historical data, there is no fixed start year for the data in the \u2018climatological values\u2019 section of the API. For daily, monthly and annual climatology and extreme values, the start date of the records depends on each individual weather station. For normal climatology, it returns average values calculated over the period from 1991 to 2020. </p> <p>The API is public, but registration is required. An API Key must be requested on the official AEMET OpenData. It is valid for three months and is linked to an email address. </p>"},{"location":"data_sources/metadata_datasources/aemet_rocio_datasource/","title":"AEMET (ROCIO)","text":"<p>The abbreviation ROCIO stands for Rejilla Observacional Con Interpolaci\u00f3n \u00d3ptima (Optimal Interpolation Observation Grid). AEMET has two types of grids to cover all island areas. The first high-resolution grid, approximately 5 km, covering mainland Spain and the Balearic Islands is called ROCIO_IBEB, and the second 2.5 km grid covering the Canary Islands is ROCIO_CAN. In both, the daily data on accumulated precipitation in 24 hours, maximum temperature and minimum temperature from a large number of stations of the State Meteorological Agency have been interpolated.   </p> <p>Therefore, for each grid there are two types of data sets available:  </p> <ul> <li> <p>Precipitation. </p> </li> <li> <p>Maximum and minimum temperatures. </p> </li> </ul> <p>With regard to the ROCIO_IBEB grid, the version 2 precipitation dataset has been generated using all the stations available in the AEMET National Climate Data Bank, i.e. 3,236 rainfall stations. This grid covers the period from 1951 to 2022 and is updated periodically.  </p> <p>The extreme temperature (maximum and minimum) datasets, version 1, have been generated using all the stations available in AEMET's National Climate Data Bank, 1,800 thermometric stations. These grids begin in 1951 and are updated until December 2022. Unlike the precipitation grid, they use climatology based on historical analyses of the HIRLAM numerical prediction model operated by AEMET as initial information, or first estimate, which is corrected by observations.  </p> <p>With regard to the ROCIO_CAN grid, the precipitation and temperature datasets have been compiled in the same way as those for ROCIO_IBEB, but with projections using regular coordinates (latitude and longitude) thanks to the use of the HARMONIE model. In addition, the data cover the period from 1990 to 2022 but are updated periodically.  </p>"},{"location":"data_sources/metadata_datasources/aemet_rocio_datasource/#api","title":"API","text":"<p>The ROCIO grid data is available on the AEMET website and also from the THREDDS server at the University of Cantabria, displayed using the OpeNDAP protocol. This protocol allows access to variables such as precipitation and temperatures (maximum and minimum) on a 20 km grid, covering mainland Spain and the Balearic Islands. The dataset includes historical series since 1950 and is not updated in real time, as it consists of interpolated observational data. Therefore, there is no real-time data from the ROCIO grid. As a result, update times are unknown. </p>"},{"location":"data_sources/metadata_datasources/aemet_spain_datasource/","title":"AEMET (Spain02)","text":"<p>The Spain02 dataset is the result of institutional collaboration between the State Meteorological Agency (AEMET) and the Santander Meteorology Group (SMG) at the University of Cantabria-CSIC.  </p> <p>The dataset consists of a 20 km resolution grid covering mainland Spain and the Balearic Islands, generated using interpolation techniques based on daily observations at meteorological stations. The aim is to provide a homogeneous and continuous basis of climate information, suitable for regional analyses and climate change impact studies. </p> <p>Similar to the ROCIO grid, Spain02 includes precipitation and maximum and minimum temperature series, offering a homogeneous and continuous product over time, suitable for regional analyses, hydrological studies, climate change impact assessments and climate modelling. The availability of multiple interpolation versions and resolutions allows the data to be adapted to different scales of analysis and modelling methodologies, ensuring the robustness of the scientific studies that use them. </p>"},{"location":"data_sources/metadata_datasources/aemet_spain_datasource/#api","title":"API","text":"<p>The only way to access the SPAIN02 grid data is through the THREDDS data centre, which stores a replica of the data files. This data centre is developed and maintained by the University of Cantabria. This server allows you to browse the catalogue of available datasets and, using the OPeNDAP (Open-source Project for a Network Data Access Protocol) protocol, remotely query and extract only the necessary variables or subsets, without having to download the entire files. </p> <p>Update times are unknown. The data files only cover up to the year 2022, so there is no real-time data available. With regard to historical data, the data files contain data from 1951 onwards, so there are large volumes of historical data available. </p>"},{"location":"data_sources/metadata_datasources/avsre_datasource/","title":"AVSRE","text":"<p>The Valencian Agency for Security and Emergency Response (AVSRE) is the body of the Valencian Regional Government responsible for directing and managing civil protection, emergency management, firefighting and public safety in the Valencian Community. Its main function is to coordinate the various emergency services and ensure a rapid and effective response to any type of incident, including those of climatic origin. </p> <p>As a coordinating body, the AVSRE centralises a large amount of critical information for emergency management. This information comes from the following sources:</p> <ul> <li> <p>Calls to the 112 emergency telephone number: records of incidents reported by the public.</p> </li> <li> <p>First responders: police, fire brigade, health services, etc. </p> </li> <li> <p>Meteorological agencies: data and alerts from the State Meteorological Agency (AEMET) and the Valencian Cartographic Institute (ICV). </p> </li> <li> <p>Hydrographic confederations: information on the state of rivers and reservoirs. </p> </li> <li> <p>Firefighting consortia and civil protection units. </p> </li> </ul> <p>The AVSRE processes this information to generate alerts, coordinate the mobilisation of resources and provide up-to-date information to the public and the authorities.  </p> <p>The quality of the data managed by the AVSRE is considered high in terms of reliability and official status, as it is the body that centralises and verifies the information from all the agencies involved in an em</p>"},{"location":"data_sources/metadata_datasources/avsre_datasource/#api","title":"API","text":"<p>The AVSRE publishes institutional information, plans and notices, but the operational data needed for flood management usually comes from various connected providers (AEMET, Hydrographic Confederations, local councils, local sensor networks). </p> <p>The Regional Government maintains an open data catalogue (GVA Oberta / data portal) where data sets and REST APIs are published for many regional and municipal services; the AVSRE consumes and coordinates these flows in its operation.</p> <ul> <li>Open data portal 'Dades Obertes GVA'</li> <li>Real-time information (Portal 112cv.gva.es and GVA Oberta)</li> </ul>"},{"location":"data_sources/metadata_datasources/cedex_datasource/","title":"CEDEX","text":"<p>CEDEX is the centre for public works studies and experimentation. It is a cutting-edge public body in the field of public works, mobility, inland and marine waters, the environment and climate change. It is attached to the Ministry of Transport and Sustainable Mobility, which also reports to the Ministry for Ecological Transition and Demographic Challenge.  </p> <p>CEDEX is divided into eight specialised technical units called Centres and Laboratories. From each centre, CEDEX provides high-level technical assistance, carries out applied research and technological development, and transfers knowledge through courses, conferences and seminars. Of the eight centres, the Centre for Hydrographic Studies stands out, providing the climate data necessary for this project. </p> <p>Its functions include the evaluation and certification of materials and techniques, the development of standards and regulations, and the promotion of R&amp;D&amp;I projects in line with national and European plans. In addition, it provides specialised technical assistance to administrations and private entities, collaborates in the conservation of infrastructure heritage, and promotes knowledge transfer through publications, courses and technical meetings. It also promotes national and international cooperation in the scientific-technical field and, in certain cases, acts as an arbitrator in disputes related to its area of competence. </p>"},{"location":"data_sources/metadata_datasources/cedex_datasource/#api","title":"API","text":"<p>CEDEX has a large amount of data available on its website. As such, it does not have an API for accessing the data. The data is fully accessible from the website itself.  </p> <p>Among the available data, CEDEX presents eight different databases, each covering a different area of those described above. The databases are as follows: </p> <ul> <li> <p>Environmental Restoration Portal </p> </li> <li> <p>Capacity Yearbook </p> </li> <li> <p>Noise Mapping System </p> </li> <li> <p>Coast and Sea Information System (INFOMAR) </p> </li> <li> <p>Spanish Precipitation Isotope Monitoring Network (REVIP) </p> </li> <li> <p>Spanish Water Information System (Hispagua) </p> </li> <li> <p>Catalogue of waste materials that can be used in construction </p> </li> </ul> <p>The database of interest for the project is the Flow Measurement Yearbook. This database is an official publication that compiles daily, monthly and annual data on flow rates measured at river and reservoir gauging stations. </p> <p>It includes hydrological records such as: </p> <ul> <li> <p>Average daily, monthly and annual flow rates. </p> </li> <li> <p>Precipitation and inflows in the basins.</p> </li> <li> <p>Information on the measuring stations (location, technical characteristics, etc.).</p> </li> <li> <p>Evapotranspiration data such as temperature and precipitation. </p> </li> </ul> <p>The data files are divided by each basin organisation. Among them is the J\u00facar Hydrographic Conference (CHJ), which has all the data on the reservoirs and the J\u00facar River, which is the basin of interest for the project. </p> <p>The update times are not defined on the website. The time range covered by the data is from the last century to a few years ago. Depending on the dataset downloaded, the time span is different. For example, for the CHJ data, specifically the daily data files on reservoirs, canals, rivers and evaporation, the time span is different. </p>"},{"location":"data_sources/metadata_datasources/copernicus_datasource/","title":"Copernicus","text":"<p>The Copernicus programme is a project coordinated and managed by the European Commission, which aims to achieve a comprehensive, continuous and autonomous high-quality Earth observation capability, the results of which are freely accessible to the scientific community, authorities and individuals.  </p> <p>The overall objective is to provide accurate, reliable and continuous information in order to, among other things, improve environmental management and conservation, understand and mitigate the effects of climate change, and ensure civil security. </p> <p>It aims to bring together different sources of information from environmental satellites and ground-based stations to provide a global view of the Earth's \u2018state of health\u2019. </p> <p>To this end, Copernicus has two sites where the different data it collects can be accessed:</p> <ul> <li> <p>Climate Data Store (CDS): A tool for accessing more than 140 data sets with information on the state of our climate around the world over the last 100 years. </p> </li> <li> <p>Atmosphere Data Store (ADS): A tool for accessing more than 100 data sets with satellite data from around the world and over the last 100 years. </p> </li> </ul> <p>The data sets presented are as follows:</p> <ul> <li> <p>ERA5-Land hourly data from 1950 to the present. </p> </li> <li> <p>RA5 hourly data from 1940 to the present. </p> </li> </ul>"},{"location":"data_sources/metadata_datasources/copernicus_datasource/#api","title":"API","text":"<p>The CDS, developed within the framework of the European Commission's Copernicus programme, makes a wide range of climate and atmospheric data available to the public. To access this information, the CDS API, called cdsapi, is mainly used, which is designed for use in Python. </p> <p>The API usage flow is simple and combines both the web interface and script automation: </p> <ul> <li> <p>Data selection on the website: From the Climate Data Store web portal, users can browse more than 140 available datasets, as well as more than 100 offered by the Atmosphere Data Store. Each dataset has filters to specify variables, time intervals, spatial resolution, and output format. </p> </li> <li> <p>Automatic script generation: Once the desired parameters have been selected, the platform automatically generates a Python script that uses the cdsapi library. This script can be run locally to download data, facilitating reproducibility and automation of the process. </p> </li> <li> <p>Download via API: The cdsapi library manages the connection to the CDS servers, authenticates the user's request, and downloads the selected files in the specified format (e.g., NetCDF or GRIB). </p> </li> </ul> <p>The API has a large amount of data available. For example, for the ERA5 hourly data set from 1940 to the present available from the Climate Data Store, we can download a large amount of data: temperature at different levels and soil types, wind, radiation, heat, clouds, lakes, water evaporation, precipitation and rain, snow, soil, vertical integrals, vegetation, and waves. </p> <p>Regarding real-time data, it should be noted that for the data sets analysed, the most recent data is from 6 days prior to the current day. The update time is daily, so each day that passes, data from day t-6 is added, where t is the current day.  </p> <p>With regard to historical data, ERA 5 and ERA5-Land have records dating back to 1940, providing data from almost 85 years ago. Figure 12 shows the years of data available for the ERA5 data set. </p>"},{"location":"data_sources/metadata_datasources/mswep_datasource/","title":"MSWEP","text":"<p>Multi-Source Weighted-Ensemble Precipitation (MSWEP) is a high-resolution global precipitation dataset. It does not include variables such as temperature, humidity, etc. Its main feature is that it combines or merges different types of data sources to obtain the most accurate and reliable rainfall estimate possible. The amount of precipitation is expressed in mm/3h. It is not an instantaneous precipitation rate. To obtain daily values, the eight 3-hour intervals that make up a day must be added together. To obtain average hourly rates, the 3-hour value would be divided by three.  </p> <p>The data is obtained through: </p> <ul> <li> <p>Data from land-based weather stations. </p> </li> <li> <p>Satellite estimates (including oceans and remote areas). </p> </li> <li> <p>Climate reanalysis data (atmospheric models that reconstruct past climate). </p> </li> </ul> <p>By merging each source, MSWEP is able to correct for the weaknesses of each source individually.  </p>"},{"location":"data_sources/metadata_datasources/mswep_datasource/#api","title":"API","text":"<p>MSWEP does not offer an API to which requests are made in real time. Instead, access to data is managed through standard protocols and formats for handling large scientific data sets, mainly through files. </p> <p>As mentioned above, the only data it contains is precipitation. It is presented as the amount of rainfall over a 3-hour period, measured in millimetres (mm).  </p> <p>There are two versions of the data: </p> <ul> <li> <p>Near-Real-Time (NRT): updated with a delay of approximately 3 to 12 hours.  </p> </li> <li> <p>Research version: this is a corrected and more accurate version that is updated with a delay of several months. </p> </li> </ul> <p>One of its features is full access to the historical record. Data is available from 1979 to the present, allowing for the analysis of trends, past extreme events, and the training of models with a very solid database. </p> <p>Access to MSWEP data is public for research and educational purposes. To access it, please contact MSWEP:</p> <ul> <li> <p>If you are affiliated with a commercial entity and would like to try MSWEP, please fill out the application form.  </p> </li> <li> <p>If you do not have a commercial affiliation and intend to use the product for non-commercial purposes, please use the Apply here form  in the Data licence section. Once you have completed the form, you will receive a link to the shared Google Drive folder once your request has been approved. </p> </li> </ul>"},{"location":"data_sources/metadata_datasources/siah_datasource/","title":"Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar \u2013 SAIH","text":"<p>The J\u00facar Hydrographic Confederation (CHJ) is the public body responsible for water management in the J\u00facar River Basin District, which covers a large part of the Valencian Community and areas of Castilla La Mancha, Aragon and Catalu\u00f1a. Its mission is to ensure sustainable water use, prevent risks associated with droughts and floods, and protect river ecosystems. </p> <p></p> <p>The J\u00facar Automatic Hydrological Information System (SAIH) is a technological network managed by the CHJ that collects real-time hydrological, hydraulic and meteorological data from across the entire basin. It consists of automatic sensors, measuring stations and a control centre that processes the information. Through the SAIH J\u00facar, the CHJ offers a comprehensive information system that connects institutional management with real-time monitoring technology. </p> <p>The SAIH collects data from three types of stations: </p> <ul> <li> <p>Gauges: measure the flow rate in rivers, tributaries and watercourses. They provide essential data for forecasting floods, assessing droughts and determining the availability of water resources. </p> </li> <li> <p>Reservoirs: record the volume of water stored, total capacity and daily variations. Key information for resource management, flood control and agricultural and urban planning.</p> </li> <li> <p>Rain gauges: collect accumulated precipitation at different time intervals. This allows for analysis of rainfall distribution and anticipation of extreme events. </p> </li> </ul>"},{"location":"data_sources/metadata_datasources/siah_datasource/#api","title":"API","text":"<p>The CHJ does not have a public API, although it does have a public dashboard, allowing access to all information without any verification via SAIH.</p> <p>To access SAIH data via its private API, you must contact the CHJ and make the relevant request.</p> More information <p>To access SAIH data through its private API, you must contact the CHJ and make the relevant request.  After this, you will obtain a username and password with which we will generate an authorisation token  that will be used to authenticate future requests.  </p> <p>In addition, you must be able to access their private network, specifically port 8000, either through  the IP whitelist or via a VPN connection.  </p> <p>The API address is: https://api-saih.chj.es:8000.  </p> <p>A graphical interface for the API is also available through the open source swagger tool. It can be accessed at https://api-saih.chj.es:8000/docs.</p> <p>Through the API, we can access real-time data from different sensors, updated every five minutes, which for the purposes of the project are classified into three types: </p> <ul> <li> <p>Rain gauges: these record accumulated rainfall in 1h, 4h, 12h and 24h and their operational status. </p> </li> <li> <p>Gauges: measure instantaneous flow with reference thresholds (low, medium and high) along with the date of the last measurement.  </p> </li> <li> <p>Reservoirs: report the elevation (metres above sea level (m a.s.l.)), the stored volume (hm\u00b3) and the inflow and outflow rates (m\u00b3/s).  </p> </li> </ul> Reservoirs Gauges Rain gauges 25 180 78 <p>In addition to real-time data, the J\u00facar Hydrographic Confederation also has historical data, although this is not accessible via API as it is not stored in a unified database but distributed across different time series. However, it is possible to perform an initial dump of this information into the platform's database, which would allow for its integration and subsequent exploitation. </p>"},{"location":"data_sources/metadata_datasources/siar_datasource/","title":"Sistema de Informaci\u00f3n Agroclim\u00e1tica para el Regad\u00edo (SiAR)","text":"<p>The Agroclimatic Information System for Irrigation (SiAR) of the Ministry of Agriculture, Fisheries and Food is a network of more than 520 weather stations that captures, records and disseminates agroclimatic data necessary to determine the water demand of irrigated farms. </p> <p>The Sub-Directorate General for Irrigation, Natural Roads and Rural Infrastructure of the Ministry of Agriculture has been developing, maintaining and updating SiAR for more than 20 years. SiAR is currently the largest source of agroclimatic information provided by both the Ministry and the autonomous communities. </p> <p>The SiAR network places special emphasis on maintaining the high quality and accuracy of the data it provides. </p> <p>There are more than 360 stations managed by the Ministry and more than 160 managed by the autonomous communities. The SiAR network in the Valencian Community has 55 stations located in irrigated areas. </p>"},{"location":"data_sources/metadata_datasources/siar_datasource/#api","title":"API","text":"<p>SiAR allows access to all your data via REST API, provided that the maximum number of requests and the maximum amount of data queried per day and per minute are respected. Each user has different restrictions regarding the maximum number of requests. </p> More information <p>The technical manual for using the SIAR API specifies: </p> <p>\"Each web client is restricted in the number of requests it can make throughout a day and within a one-minute interval. Similarly, and for identical reasons, the number of agroclimatic data that can be consulted in a day and in a minute will also be limited\"  (Ministry of Agriculture, Fisheries and Food [MAPA], 2023, p. 12). </p> <p>To find out about the restrictions, each user can check their limitations via the following URL: https://servicio.mapama.gob.es/apisiar/API/v1/Info/Accesos?ClaveAPI={PUT_API_KEY}. When using the API key provided, the SIAR returns a response in JSON format indicating the limitations of API use to the user. The results are:</p> <ul> <li> <p>Maximum number of accesses per minute: 15</p> </li> <li> <p>Maximum number of accesses per day: 240</p> </li> <li> <p>Maximum number of records per minute: 100</p> </li> <li> <p>Maximum number of records per day: 10,000 </p> </li> </ul> <p>With regard to the amount of data available, SIAR allows data to be downloaded at the autonomous community, province and station level for the whole of Spain. For example, there are 41 weather stations in the province of Valencia. </p> <p>The SIAR has been storing data since 1999, so the time span is 25 years. This means that historical data is accessible. </p> <p>With regard to real-time data, attempts have been made to download the most recent data for several stations, and the most recent data returned by the API is from two hours ago.</p> <p>The data update times are not known exactly. The update time is estimated to be 30 minutes, given that the finest temporal granularity is every 30 minutes. </p> <p>Regarding temporal resolution, the technical manual explains the different granularities for accessing data according to the required time span (hourly, daily, weekly, monthly, etc.). </p>"},{"location":"getting_started/","title":"Getting started","text":"<p>Embarking on your journey with PGTEC is made simple through our comprehensive Getting Started guide. This section is designed to help users familiarize themselves with the essential aspects of the PGTEC project.</p> <p>Next steps</p> <p>Ready to get started? Just click \"Next\" on the bottom navigation bar to continue!</p>"},{"location":"pipeline/","title":"Data Flow","text":""},{"location":"pipeline/#overview","title":"Overview","text":"<p>The data space aims to centralise and standardise climate information from various sources so that it is available to organisations interested in early prevention of climate emergencies.</p> <p>The first phase in building the data space is data collection. To do this, it is not enough to simply collect information from different sources: it is necessary to ensure that the data is updated in real time, is in a standardised format and is easily accessible to different organisations and applications.</p> <p>To this end, a structured data flow is designed that covers several consecutive stages, from obtaining the data at source to its storage and distribution in the data space. This flow ensures that each piece of data collected goes through a process of capture, translation, standardisation, management and historical persistence, so that it can ultimately be integrated correctly and consistently into the data ecosystem.</p>"},{"location":"pipeline/#components-of-the-data-flow","title":"Components of the data flow","text":"<ul> <li> <p>Airflow: An orchestration tool that allows you to schedule and automate data collection tasks. It is responsible for periodically executing scripts that query APIs or download files from different sources.</p> </li> <li> <p>IoT Agent Ultralight: Acts as a translator for the data received. It converts information from sensors or APIs into a common, standardised format (in this case based on FIWARE Smart Data Models), ensuring consistency in variable names and units of measurement.</p> </li> <li> <p>Orion-LD (Context Broker): Core of the system. It manages, stores and distributes contextual information in JSON-LD format, allowing data to be queried in real time by different applications or services.</p> </li> <li> <p>QuantumLeap: Component specialised in historical storage. It receives data from the Context Broker and stores it in a temporary database, in this case CrateDB, thus facilitating its consultation and subsequent use for analysis, report generation or artificial intelligence model training.</p> </li> </ul>"},{"location":"pipeline/#stages-of-data-flow","title":"Stages of data flow","text":"<ul> <li> <p>Data collection: Climate data is collected periodically from different sources, mostly through REST APIs, although some comes from direct downloads from web pages. This process is automated with Apache Airflow, which, thanks to the use of DAGs (Directed Acyclic Graphs) in Python, allows dependencies to be defined, execution to be scheduled (e.g., hourly) and regular, reliable data updates to be ensured.</p> </li> <li> <p>Standardisation and translation: Each source uses different variables and formats, so it is necessary to unify names, units, and structures. This task is performed by the IoT Agent, which translates the information into common models defined by FIWARE's Smart Data Models. In this case, a climate model is used to ensure data interoperability for future applications.</p> </li> <li> <p>Contextual management and distribution: Standardised data is managed by the Orion-LD Context Broker, which stores and distributes it in JSON-LD format. This makes it easier for multiple applications or services to consume it consistently.</p> </li> <li> <p>Historical and analytical persistence: Finally, real-time data is stored in a historical database through QuantumLeap, which inserts it into CrateDB as time series. In this way, complete records are built that allow IIAMA to train machine learning and deep learning models, which are key to the prediction and early detection of adverse climate phenomena.</p> </li> </ul> <p></p>"},{"location":"pipeline/example_copernicus/","title":"Example","text":"<p>This is an example to use the environment using the Copernicus data as an example with the DAG:</p> <p>flujo_coperniocus_orion_quantumleap.py</p>"},{"location":"pipeline/example_copernicus/#instructions","title":"Instructions","text":"<ol> <li>Get a free Copernicus Climate Data Store API Key at Copernicus Climate Data Store</li> </ol> <p> 2. Clone the repository  <pre><code>git clone https://github.com/PGTEC-VRAIN/Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap.git\n</code></pre> 3. Run Docker service <pre><code>sudo service docker start\n</code></pre> 4. Access to the correct repository with the environment Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap <pre><code>cd Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap\n</code></pre> 5. Change API Key at the script flujo_coperniocus_orion_quantumleap.py <pre><code> c = cdsapi.Client(url=\"https://cds.climate.copernicus.eu/api\",\n                   key=\"PUT_YOUR_API_KEY_HERE\")\n</code></pre> 6. Run the docker compose yaml <pre><code>docker compose up --build\n</code></pre> 7. Check the status of the containers in a new terminal <pre><code>docker ps\n</code></pre> 8. Access to Airflow to see the DAG running (You may create an account first)  9. Check CreateDB to get historical data You may create an account first  10. Shut down the docker <pre><code>docker compose down\n</code></pre></p>"},{"location":"pipeline/requirements/","title":"Requirements","text":""},{"location":"pipeline/requirements/#what-do-i-need","title":"What do I need?","text":"<p>To correctly deploy and operate the data pipeline, the following components and requirements must be available:</p> <ul> <li> <p>Python: Required for implementing auxiliary scripts, data preprocessing tasks, and integration logic with external services.</p> </li> <li> <p>Docker Compose: Used to orchestrate and deploy the FIWARE stack.</p> </li> <li> <p>Airflow: Provides workflow orchestration, ensuring the automated and scheduled execution of data collection, transformation, and loading tasks.</p> </li> <li> <p>Access credentials: An API key or endpoint URL to connect to the external data sources supplying the raw information.</p> </li> <li> <p>Networking and FIWARE configuration: Proper network setup and FIWARE component configuration to allow seamless communication between all services in the stack.</p> </li> </ul>"},{"location":"pipeline/requirements/#core-components","title":"Core Components","text":"<ul> <li>IoT Agent: Interface for receiving IoT device data and forwarding it to the appropriate FIWARE context entities.</li> <li>Orion-LD Context Broker: Core NGSI-LD\u2013based context management engine responsible for storing, updating, and distributing contextual information.</li> <li>QuantumLeap: Time-series database service that persists historical events and temporal data, supporting advanced analytics and AI model training.</li> <li>CreateDB: Serves as the historical storage backend within the FIWARE stack.</li> <li>MongoDB: Provides context data persistence for the Orion-LD Context Broker.</li> <li>Apache Airflow: Workflow orchestration through Directed Acyclic Graphs (DAGs), enabling automation and scheduling of data-processing pipelines.</li> </ul> <p>This project was developed and tested on: </p> <ul> <li>Ubuntu 24.04.1 LTS</li> </ul> <p>These are the necessary requirements to be able to execute the project:</p> Software Version / Note Docker 27.2.0 Docker Compose 27.2.0 Python Docker container Airflow Docker container IoT Agent-UL Docker container Orion-LD Context Broker Docker container QuantumLeap Docker container / CreateDB backend"},{"location":"pipeline/setup/","title":"Set up","text":"<p>This repository provides a complete environment for orchestration and management of IoT data, integrating a set of FIWARE-based components and workflow automation tools.</p>"},{"location":"pipeline/setup/#repository-contents","title":"Repository Contents","text":"<ul> <li> <p>docker-compose.yml: Defines and configures all services and containers within the stack.</p> </li> <li> <p>dags/: Directory containing Airflow DAGs for automated workflows.</p> </li> <li> <p>Scripts:</p> <ul> <li>AEMET.py: Fetches meteorological data from the AEMET API.</li> <li>Copernicus.py: Retrieves Copernicus climate datasets for the province of Valencia using Python\u2019s cdsapi library.</li> <li>Flujo_Copernicus_orion.py: DAG that collects Copernicus data, registers it in the IoT Agent, and stores it in Orion-LD.</li> <li>Flujo_copernicus_orion_quantumleap.py: Enhanced DAG that extends the previous workflow by integrating QuantumLeap for historical data persistence in CrateDB. (Recommended DAG for production use.)</li> </ul> </li> </ul>"},{"location":"pipeline/setup/#install-the-software","title":"Install the software","text":"<p>The following commands can be used to install some of the necessary software:</p>"},{"location":"pipeline/setup/#docker","title":"Docker","text":"<pre><code>sudo apt update\nsudo apt install -y apt-transport-https ca-certificates curl software-properties-common\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt update\nsudo apt install -y docker-ce docker-ce-cli containerd.io\n</code></pre>"},{"location":"pipeline/setup/#docker-compose","title":"Docker Compose","text":"<pre><code>sudo curl -L \"https://github.com/docker/compose/releases/download/v2.20.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n</code></pre>"},{"location":"pipeline/setup/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/PGTEC-VRAIN/Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap.git\ncd Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap\n</code></pre>"},{"location":"pipeline/setup/#python-environment-from-anaconda-miniconda","title":"Python (Environment from anaconda / miniconda)","text":"<pre><code># Download Miniconda installer (Linux x86_64)\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh\n\n# Run the installer\nbash ~/miniconda.sh\n\n# Follow the prompts (accept license, choose install path, initialize conda)\n\n# Initialize conda for bash\nsource ~/.bashrc\n\n# Create a project environment (Python 3.12.3)\nconda create -n pgtec_env python=3.12.3 -y\nconda activate pgtec_env\n</code></pre>"},{"location":"pipeline/setup/#setup-instructions","title":"Setup instructions","text":""},{"location":"pipeline/setup/#clone-the-repository_1","title":"Clone the repository","text":"<pre><code>git clone https://github.com/PGTEC-VRAIN/Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap.git\ncd Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap\n</code></pre>"},{"location":"pipeline/setup/#start-the-containers","title":"Start the containers","text":"<pre><code>docker-compose up -d --build\n</code></pre> <ul> <li> <p>Use --build only if changes were made to the docker-compose.yml.</p> </li> <li> <p>The -d option runs services in the background (detached mode).</p> </li> </ul>"},{"location":"pipeline/setup/#verify-running-services","title":"Verify running services","text":"<pre><code>docker ps\n</code></pre>"},{"location":"pipeline/setup/#shut-down-the-environment","title":"Shut down the environment","text":"<ul> <li>From a new terminal: <pre><code>docker-compose down\n</code></pre></li> <li>Or interrupt the current process with CTRL + C.</li> </ul>"}]}