{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Information of PGTEC <p>A climate and weather data space that connects providers and users through shared standards, governance and interoperability, enabling trusted reuse of information for early warning and risk management.</p> <ul> <li> <p> Getting started</p> <p>Start by learning the basics of PGTEC.</p> <p> Learn more</p> </li> <li> <p> Data Sources</p> <p>Information of data sources for climate emergency management.   </p> <p> Learn more</p> </li> <li> <p> Data Flow</p> <p>The guide of a complete environment for the project.</p> <p> Learn more</p> </li> <li> <p> Smart Data Models </p> <p>Description of the data models used.</p> <p> Learn more</p> </li> <li> <p> Smart Flow</p> <p>Python-based scripts designed with Airflow and FastAPI to manage and automate data processing pipelines.</p> <p> Learn more</p> </li> <li> <p> Data Space</p> <p>Overview of the PGTEC data space architecture, governance model, identities and access control.</p> <p> Learn more</p> </li> </ul>"},{"location":"DataSpace/","title":"Data Space","text":""},{"location":"DataSpace/#pgtec-data-space","title":"PGTEC Data Space","text":"<p>The PGTEC data space is a climate and hydrometeorological data space designed to share information between organisations in a governed and interoperable way. It is not just a file repository, but an infrastructure where data, models and services can be exchanged under clear rules for access and quality.</p> <p>This section explains the role of the data space inside the project and serves as an entry point to the rest of the technical documentation.</p>"},{"location":"DataSpace/#what-is-the-pgtec-data-space","title":"What is the PGTEC data space?","text":"<p>The PGTEC data space connects organisations that:</p> <ul> <li>Produce climate, hydrological and meteorological data</li> <li>Exploit that data for prediction, risk analysis and early warning services</li> <li>Need guarantees about governance, traceability and responsible use of information</li> </ul> <p>Some key principles of the data space are:</p> <ul> <li> <p>Semantic interoperability    Data is represented using common information models, based on Smart Data Models, so that different systems can understand each other.</p> </li> <li> <p>Technical interoperability    Data exchange happens through standardised APIs and components, using FIWARE and NGSI technologies.</p> </li> <li> <p>Governance and trust    Participants are identified using cryptographic identifiers and, progressively, verifiable credentials that describe their roles and permissions.</p> </li> <li> <p>Service orientation    Besides raw datasets, the data space exposes processing flows, models and services that can be directly integrated into external applications.</p> </li> </ul>"},{"location":"DataSpace/#main-building-blocks","title":"Main building blocks","text":"<p>The PGTEC data space is built on several functional blocks, each described in its own section:</p> <ul> <li> <p>Data Sources and Services    Description of climate and hydrological data providers, their variables, frequency, coverage and usage conditions.   See Data Sources and Data and services.</p> </li> <li> <p>Common information models    Shared data models for forecast series, observations and streamflow, based on Smart Data Models.   See Smart Data Models.</p> </li> <li> <p>Data Flow and processing    Ingestion, normalisation, historical storage and publication of data, implemented with SmartFlow and FIWARE components.   See Data Flow and SmartFlow.</p> </li> <li> <p>Identity, trust and access    The identity layer of the data space, where participants, their identifiers and the credentials that control access are managed.   See Identities and verifiable credentials.</p> </li> </ul>"},{"location":"DataSpace/#how-to-use-this-section","title":"How to use this section?","text":"<p>If you are new to PGTEC and want to understand its data space, a reasonable path is:</p> <ol> <li>Read this introduction.  </li> <li>Review the data space architecture to see the key components and how they interact.  </li> <li>Dive into identities and verifiable credentials if you are interested in access and trust.  </li> <li>Go to Join data space to see the planned steps for onboarding as a participant.</li> </ol>"},{"location":"DataSpace/arquitecture/","title":"Arquitecture","text":""},{"location":"DataSpace/arquitecture/#pgtec-data-space-arquitecture","title":"PGTEC Data Space Arquitecture","text":"<p>This section describes the technical architecture of the PGTEC data space: </p> <ul> <li>How participants connect?</li> <li>Ehich common services are provided?</li> <li>How FIWARE components are used to guarantee interoperability, security and governance?</li> </ul> <p>It complements the high-level introduction given in the main Data Space page.</p> <p>The architecture is designed to:</p> <ul> <li>Enable multiple organisations to share climate and hydrological data and services</li> <li>Keep each participant in control of the data they expose</li> <li>Apply common rules for trust, access control and interoperability</li> </ul> <p>At the centre of this architecture are FIWARE Data Space Connectors (FDSC) and a set of shared trust services deployed in a cloud-native environment.</p>"},{"location":"DataSpace/arquitecture/#participants-and-roles","title":"Participants and Roles","text":"<p>From the perspective of the data space, the main participants are grouped into roles.</p> <ul> <li> <p>Data Providers   Organisations that expose observed or forecast data through their own connector.   Examples: AEMET, CHJ, SIAR, AVAMET, Copernicus and other climate data providers.</p> </li> <li> <p>Service Providers   Organisations that expose value-added services (hydrological models, dashboards, simulations) that consume data from the space and publish results back as services.   Examples: IIAMA, VRAIN.</p> </li> <li> <p>Data Consumers   Any participant that queries the data space to consume datasets or services published by providers.</p> </li> </ul> <p>Most real participants play more than one role. For example, IIAMA and VRAIN consume climate data and provide services based on that information. </p>"},{"location":"DataSpace/arquitecture/#high-level-architecture","title":"High-level architecture","text":"<p>At a high level, the PGTEC data space is composed of:</p> <ol> <li>A set of FIWARE Data Space Connectors, one per participating organisation</li> <li>Common trust and governance services: Trust Anchor, Verifiable Credentials Issuer and Marketplace</li> <li>A cloud-native infrastructure that hosts these components on Kubernetes in AWS</li> </ol> <p>Each participant connects to the data space through its own FIWARE Data Space Connector. The connector acts as a standardised gateway where:</p> <ul> <li>Incoming requests are authenticated and authorised</li> <li>Usage policies are enforced</li> <li>Data is exposed through interoperable APIs based on NGSI-LD and TM Forum standards</li> </ul> <p>Shared components such as the Trust Anchor and Marketplace provide ecosystem-wide services for identity, policy and discovery.</p>"},{"location":"DataSpace/arquitecture/#common-trust-and-governance-services","title":"Common trust and governance services","text":""},{"location":"DataSpace/arquitecture/#trust-anchor","title":"Trust Anchor","text":"<p>The Trust Anchor (TA) is the main trust service in the PGTEC data space. It ensures that interactions and data exchanges happen only between authenticated, authorised and compliant participants. </p> <p>Its responsibilities include:</p> <ul> <li>Maintaining a Trusted Issuers List with the entities allowed to issue verifiable credentials within the ecosystem</li> <li>Providing endpoints that connectors can call to validate identities and credentials</li> <li>Acting as the root of trust that other components use when deciding whether to accept a connection</li> </ul> <p>Technically, the TA runs as a microservice inside the Kubernetes cluster, backed by a relational database where trusted issuers and related trust configuration are stored. It is typically exposed via an ingress controller so that connectors can reach it over HTTPS from within the cloud or from other networks.</p>"},{"location":"DataSpace/arquitecture/#marketplace","title":"Marketplace","text":"<p>A Marketplace component is planned for later stages of the project. Its purpose is to:</p> <ul> <li>Allow providers to publish the data sets and services they offer</li> <li>Allow consumers to discover available resources</li> <li>Support negotiation of access conditions, prices or usage policies</li> <li>Integrate with contract management mechanisms from the FDSC</li> </ul> <p>For the first Minimum Viable Data Space the full Marketplace is not strictly required, because each connector already exposes TM Forum APIs that allow programmatic discovery of catalogues and offers. This keeps the initial deployment simpler while preserving the core functionality.</p>"},{"location":"DataSpace/arquitecture/#verifiable-credentials-issuer","title":"Verifiable Credentials Issuer","text":"<p>The Verifiable Credentials (VC) Issuer is another planned shared service. </p> <ul> <li>Issue verifiable credentials to organisations, services and possibly users</li> <li>Integrate with the Trust Anchor and its Trusted Issuers List</li> <li>Provide the credentials that connectors use when authenticating incoming requests</li> </ul> <p>In early stages of the project, identities and access are managed with simpler mechanisms. As the data space opens to more external participants, the VC Issuer becomes crucial to support self-sovereign identities and more advanced trust models.</p>"},{"location":"DataSpace/arquitecture/#fiware-data-space-connector","title":"FIWARE Data Space Connector","text":"<p>The FIWARE Data Space Connector (FDSC) is the main technical building block of the PGTEC data space. Each provider or consumer uses its own connector instance to interface with the rest of the ecosystem. </p> <p>Conceptually, the FDSC provides:</p> <ul> <li>Authentication and authorisation based on verifiable credentials and policies</li> <li>Data publication and discovery through standard APIs</li> <li>Contract and policy management for data usage</li> <li>Actual data exchange between connectors using the Dataspace protocol</li> </ul> <p>Internally, the FDSC is itself composed of several subsystems, which can be grouped by function: </p>"},{"location":"DataSpace/arquitecture/#authentication-and-authorisation","title":"Authentication and authorisation","text":"<ul> <li> <p>Policy Decision Point (PDP) plugin   Evaluates access policies written in Open Digital Rights Language (ODRL) and decides whether a request should be allowed.</p> </li> <li> <p>APISIX API Gateway   Terminates incoming HTTP(S) traffic, enforces the PDP decisions and routes authorised requests to internal services such as brokers or APIs.</p> </li> <li> <p>VC Verifier   Checks the validity of verifiable credentials presented by participants.</p> </li> <li> <p>Trusted Issuer List and Credentials Config Service   Maintain the configuration and state related to trusted issuers and stored credentials.</p> </li> </ul>"},{"location":"DataSpace/arquitecture/#data-publication-and-discovery","title":"Data publication and discovery","text":"<ul> <li> <p>Scorpio NGSI-LD Broker   Manages publication, query and subscription of context data. This is where harmonised entities (for example, observations or forecasts) are stored and exposed using NGSI-LD.</p> </li> <li> <p>TM Forum APIs and Contract Management   Allow providers to publish catalogues and assets, and to define and manage usage contracts between participants.</p> </li> </ul>"},{"location":"DataSpace/arquitecture/#dataspace-protocol-and-dataplane","title":"Dataspace protocol and dataplane","text":"<ul> <li> <p>Rainbow (IDSA Dataspace Protocol implementation)   Implements the International Data Spaces Association protocol for secure data exchange between connectors.</p> </li> <li> <p>Trusted Policy Propagator (TPP)   Distributes policies between participants to ensure that usage constraints are consistently enforced.</p> </li> <li> <p>DID and dataplane services   Manage decentralised identifiers and the actual data transfer channel, ensuring that policies and security rules are applied during data movement.</p> </li> </ul> <p>This modular design allows each connector to be extended with additional components when needed. For example, climate data providers add components such as Orion-LD, TimescaleDB, Mintaka, Airflow or SmartFlow API to handle their specific data ingestion and exposure needs. </p>"},{"location":"DataSpace/arquitecture/#minimum-viable-data-space-mvds","title":"Minimum Viable Data Space (MVDS)","text":"<p>Before deploying the full architecture, PGTEC defines a Minimum Viable Data Space (MVDS) that focuses on validating the interaction between key components and participants.</p> <p>The MVDS includes:</p> <ul> <li>Trust Anchor</li> <li>Connectors for a subset of data providers (climate agencies)</li> <li>A connector for IIAMA, acting as consumer of climate data and provider of hydrological services</li> </ul> <p>In this first deployment:</p> <ul> <li>The Marketplace is not deployed, since all necessary discovery can be performed through the TM Forum APIs already available in each connector.</li> <li>The Verifiable Credentials Issuer is not yet required, because only internal PGTEC participants are involved.</li> <li>The connector for VRAIN can be added later, once the real-time dashboard service is ready to be integrated.</li> </ul> <p>Even in this reduced configuration, the MVDS provides a functional dataspace where realistic scenarios can be simulated and the behaviour of connectors, models and services can be evaluated.</p>"},{"location":"DataSpace/arquitecture/#infrastructure-and-deployment","title":"Infrastructure and Deployment","text":"<p>The PGTEC data space is deployed on a public cloud infrastructure, specifically on AWS, using a Kubernetes cluster as the underlying orchestration platform.</p> <p>The main reasons for this choice are:</p> <ul> <li> <p>Scalability   The cluster can be scaled up or down depending on the computational and storage needs (for example, when ingesting large climate datasets or running intensive simulations).</p> </li> <li> <p>Resilience   Distributed services with built-in redundancy ensure operational continuity, even in the presence of failures.</p> </li> <li> <p>Security   Integration with cloud identity and access management, network segmentation and encryption aligns with European digital trust standards.</p> </li> <li> <p>Flexibility   Native support for containers and microservices simplifies the deployment of connectors, Trust Anchor, and additional components such as Orion-LD, Mintaka, TimescaleDB, Airflow or SmartFlow.</p> </li> </ul> <p>Within this cluster, each connector runs in its own namespace, along with its auxiliary services. This separation allows providers to manage their own components, while still benefiting from a shared, managed infrastructure.</p>"},{"location":"DataSpace/arquitecture/#relation-with-other-documentation","title":"Relation with other documentation","text":"<p>This architecture page connects to the rest of the technical documentation as follows:</p> <ul> <li>The Data Sources and Data and services pages describe which datasets and services are actually exposed through the connectors.</li> <li>The Data Flow and SmartFlow pages detail how observed and forecast data are ingested, harmonised, stored and exposed.</li> <li>The Identities and verifiable credentials page explains the identity and trust layer that runs on top of this architecture.</li> <li>The DIDs page shows how decentralised identifiers are generated and used to support the identity layer in practice.</li> </ul>"},{"location":"DataSpace/did/","title":"DID","text":""},{"location":"DataSpace/did/#decentralized-identifiers","title":"Decentralized Identifiers","text":"<p>In the PGTEC data space, decentralized identifiers (DIDs) are used to represent identities in a portable and cryptographically verifiable way. They are especially useful to identify organisations, services or components without relying on a single central authority.</p> <p>This page gives a practical introduction to DIDs and shows how to generate did:web and did:key identifiers using Docker, as used in the PGTEC experiments.</p> <p>For a higher-level description of how identities and credentials are used in the data space, see the page on identities and verifiable credentials in the Data Space section.</p>"},{"location":"DataSpace/did/#what-is-a-did","title":"What is a DID?","text":"<p>A decentralized identifier (DID) is a URI-like identifier that is:</p> <ul> <li>Globally unique and persistent</li> <li>Bound to one or more public keys</li> <li>Resolvable to a DID document, which is a JSON file containing public keys (for authentication and verification) and service endpoints (for communication).</li> </ul> <p>A DID has three parts:</p> <ul> <li>The scheme, always <code>did:</code></li> <li>The DID method name</li> <li>The method-specific identifier</li> </ul> <p>For example, in:</p> <pre><code>did:web:example.com\n\n## did:web method\nThe did:web method leverages existing web infrastructure, specifically HTTPS and Domain Name System (DNS), to create and resolve DIDs.\n\nA did:web DID is constructed directly from a domain name (and optional path), making it highly readable and familiar. For example: ```did:web:example.com```\n\nResolution: The DID document is hosted on the corresponding web server at a *well-known* location like this: ```https://example.com/.well-known/did.json```\n\n## did:key method\nThe did:key method is the simplest form of DID, as the identifier is directly derived from a cryptographic public key.\n\nThe DID method-specific identifier is an encoded representation of the public key material. For example: ```did:key:z6MkjBWPPa1njEKygyr3LR3pRKkqv714vyTkfnUdP6ToFSH5```\n\nResolution: The DID document can be generated entirely from the DID string itself without requiring any external network lookup. It is self-contained.\n\n## Automated setup with Docker\n\nWe use a docker image to create the DIDs from a key pair and a certificate.\n\nWe can create the key pair and the certificate like this:\n\n```sh\nmkdir example\n\n# Create a key pair.\nopenssl ecparam -genkey -name prime256v1 -noout -out example/key-pair.pem\n\n# That can be read like this:\nopenssl ec -in example/key-pair.pem -text -noout\n\n# Create a self-signed certificate from the key pair.\nopenssl req -x509 -new -key example/key-pair.pem -out example/cert.pem -days 365 -nodes\n\n# That can be read like this:\nopenssl x509 -in example/cert.pem -text -noout\n</code></pre>"},{"location":"DataSpace/did/#didweb-setup","title":"did:web setup","text":"<p>Once we have the certificate associated to our key pair we can generate the did.json running a docker container:</p> <pre><code>docker run -p 8080:8080 -v ./example/cert.pem:/certs/tls.crt -it mortega5/did-helper:0.0.1 -didType web -hostUrl example.com -outputFormat json_jwk -certPath /certs/tls.crt\n</code></pre> <p>This will print out the contents of a JSON file. That content should be accessible from <code>example.com/.well-known/did.json</code> if we are using a home page like example.com.</p> <p>If we want to use a deep link we have to run the docker container with the full link:</p> <pre><code>docker run -p 8080:8080 -v ./example/cert.pem:/certs/tls.crt -it mortega5/did-helper:0.0.1 -didType web -hostUrl example.com/specific/page -outputFormat json_jwk -certPath /certs/tls.crt\n</code></pre> <p>And make the JSON that it prints accessible from <code>example.com/specific/page/did.json</code>. The idintifier in this case would be did:web:example.com:specific:page</p>"},{"location":"DataSpace/did/#didkey-setup","title":"did:key setup","text":"<p>Once we have the certificate associated to our key pair we can generate the did:key and did.json by running a docker container:</p> <pre><code>docker run -p 8080:8080 -v ./example/cert.pem:/certs/tls.crt -v ./example/key-pair.pem:/certs/tls.key -it mortega5/did-helper:0.0.2 -didType key -certPath /certs/tls.crt -keyPath /certs/tls.key\n</code></pre> <p>If we get the error <code>exec /did-helper/did-helper: exec format error</code> we have to run:</p> <pre><code>docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n</code></pre> <p>before rerunning the docker.</p> <p>We don't need to host the did:key's did.json anywhere. We just use out identifier and use out private key (stored in example/key-pair.pem) to prove we are the holders of that did:key.</p>"},{"location":"DataSpace/did/#license","title":"License","text":"<p>Distributed under the AGPL-3.0 License. See <code>LICENSE</code> for more information.</p>"},{"location":"DataSpace/did/#contact","title":"Contact","text":"<p>Project Link: https://github.com/PGTEC-VRAIN</p>"},{"location":"DataSpace/identities_and_credentials/","title":"Overview","text":""},{"location":"DataSpace/identities_and_credentials/#overview","title":"Overview","text":""},{"location":"DataSpace/identities_and_credentials/#identities-and-verifiable-credentials","title":"Identities and verifiable credentials","text":"<p>The PGTEC Data Space needs a reliable way to identify organisations, services and technical components that participate in it. This identity layer is the foundation for governance, access control and auditability.</p> <p>This page describes, at a high level, how identities and verifiable credentials are used in the data space. For concrete details on the DID formats and how to generate them with Docker, see Decentralized identifiers.</p>"},{"location":"DataSpace/identities_and_credentials/#roles-in-the-data-space","title":"Roles in the Data Space","text":"<p>From the point of view of identity and access, the main roles in the PGTEC data space are:</p> <ul> <li> <p>Data Space operator    Manages the core services of the data space, defines governance rules and operates shared infrastructure.</p> </li> <li> <p>Data Provider    Publishes climate, hydrological or risk-related data into the data space using agreed information models and interfaces.</p> </li> <li> <p>Data Consumer    Accesses data and services from the data space for visualisation, modelling, decision support or research.</p> </li> <li> <p>Service Provider   Offers applications or analytical services that run on top of PGTEC data (for example, early warning tools or hydrological simulations).</p> </li> <li> <p>Technical integrator    Develops and operates connectors, pipelines or adapters that link external systems to the data space.</p> </li> </ul> <p>Each organisation can hold one or more of these roles, and the identity and credential system needs to reflect that.</p>"},{"location":"DataSpace/identities_and_credentials/#decentralized-identifiers-in-pgtec","title":"Decentralized Identifiers in PGTEC","text":"<p>To represent identities in a portable and cryptographically verifiable way, PGTEC uses decentralized identifiers (DIDs). A DID:</p> <ul> <li>Is a URI-like identifier that is globally unique and persistent</li> <li>Resolves to a DID document containing public keys and service endpoints</li> <li>Is not tied to a single central registry or certificate authority</li> </ul> <p>In PGTEC experiments, two DID methods are used:</p> <ul> <li>did:web, which binds identities to existing web domains and HTTPS infrastructure</li> <li>did:key, which derives identifiers directly from public keys and is convenient for tests and internal services</li> </ul> <p>The practical details of did:web and did:key, including Docker-based helpers, are described in Decentralized identifiers.</p>"},{"location":"DataSpace/identities_and_credentials/#verifiable-credentials","title":"Verifiable Credentials","text":"<p>On top of DIDs, the data space uses verifiable credentials to express claims about identities in a standardised way.</p> <p>A verifiable credential typically contains:</p> <ul> <li>An issuer identifier (the DID of the entity that issues the credential)</li> <li>A subject identifier (the DID of the entity the credential talks about)</li> <li>A set of claims (for example, roles, permissions or attributes)</li> <li>Cryptographic proofs that allow any verifier to check authenticity and integrity</li> </ul> <p>Verifiable credentials are designed to be:</p> <ul> <li>Machine-readable and interoperable between systems</li> <li>Selective, so that holders can disclose only what is necessary</li> <li>Verifiable without contacting the issuer each time, using public keys published in DID documents</li> </ul>"},{"location":"DataSpace/identities_and_credentials/#example-credential-types-in-pgtec","title":"Example credential types in PGTEC","text":"<p>In the context of the PGTEC data space, typical credential types include:</p> <ul> <li> <p>Membership and Role Credentials    State that a given organisation is a recognised data provider, data consumer or service provider in PGTEC.</p> </li> <li> <p>Data Access Credentials    Grant the holder the right to access specific categories of data, spatial domains, temporal ranges or service endpoints.</p> </li> <li> <p>Connector Atestation Credentials    Describe a deployed connector or technical component, including its operator, capabilities and compliance with PGTEC policies.</p> </li> <li> <p>Delegation Credentials    Allow an organisation to delegate certain rights or actions to another organisation or to a specific technical component.</p> </li> </ul> <p>The precise schemas for these credentials can evolve as the data space matures, but they all follow the same basic pattern of issuer, subject, claims and proof.</p>"},{"location":"DataSpace/identities_and_credentials/#credential-lifecycle","title":"Credential Lifecycle","text":"<p>The identity and credential model follows a simple lifecycle:</p> <ol> <li> <p>Registration     An organisation or component is identified and onboarded into the data space. The operator verifies its identity and assigns an initial DID.</p> </li> <li> <p>Issuance     The operator or another trusted issuer creates and signs verifiable credentials that describe the roles and rights of that subject.</p> </li> <li> <p>Storage     The holder stores the credentials in a secure wallet or internal system, under its control.</p> </li> <li> <p>Presentation     When accessing data or services, the holder presents one or more credentials to prove that it has the required roles or permissions.</p> </li> <li> <p>Verification     The receiving service (verifier) checks the proofs on the credentials, validates the issuer\u2019s DID document and applies its local access policies.</p> </li> <li> <p>Revocation and update     Credentials can be revoked or replaced if roles change, access is withdrawn or errors are found.</p> </li> </ol>"},{"location":"DataSpace/identities_and_credentials/#using-identities-and-credentials-for-access-control","title":"Using identities and credentials for access control","text":"<p>In the PGTEC data space, services use identities and credentials to make access decisions. A typical pattern is:</p> <ul> <li>Authenticate the caller using its DID and cryptographic material</li> <li>Request verifiable credentials that prove specific roles or attributes</li> <li>Evaluate those credentials against local policy (for example, \u201conly holders of a valid data-provider credential for region X can publish entities in this context\u201d)</li> <li>Grant or deny the requested operation based on the result</li> </ul> <p>This approach allows:</p> <ul> <li>Fine-grained access control that is independent of any single platform</li> <li>Explicit governance rules encoded in credentials and policies</li> <li>Better traceability of who accessed what, under which authorisation</li> </ul>"},{"location":"DataSpace/identities_and_credentials/#current-status","title":"Current status","text":"<p>The identity and credential layer in PGTEC is being developed progressively:</p> <ul> <li>DIDs and DID documents are used in experimental setups to model participants and services.</li> <li>Docker-based tools are provided to generate DIDs from existing keys and certificates.</li> <li>The verifiable credential schemas and onboarding workflows are being refined based on project requirements.</li> </ul> <p>As the data space moves towards a more operational state, the processes described in Join data space will rely increasingly on this identity and credential model to onboard, manage and audit participants in a scalable way.</p>"},{"location":"SmartDataModels/","title":"Smart Data Models","text":""},{"location":"SmartDataModels/#overview","title":"Overview","text":"<p>The Smart Data Models initiative, led by FIWARE Foundation, provides a set of open and interoperable data models designed to promote common standards for data exchange. These models define the structure, semantics, and units of measurement for key environmental and urban variables, allowing data to be easily reused and understood across multiple platforms and applications.</p> <p>Within the PGTEC project, the adoption of Smart Data Models plays a key role in ensuring that data collected from diverse environmental and meteorological sources can be seamlessly integrated and understood. The project aims to harmonize and translate heterogeneous datasets\u2014originating from different agencies and monitoring systems\u2014into a unified, standardized format that facilitates interoperability, data sharing, and contextual analysis. By using standardized FIWARE-based schemas, the project ensures that variables\u2014such as temperature, precipitation, or relative hummidity, among others, are represented consistently, regardless of their origin. This harmonization makes easier the training of the TETIS hydrological model using different data sources such as Open-Meteo or AEMET. It also enables more accurate analysis and improved decision-making.</p>"},{"location":"SmartDataModels/#smart-data-models-for-pgtec","title":"Smart Data Models for PGTEC","text":"<p>To achieve this, PGTEC implements and extends several Smart Data Models tailored to the project\u2019s specific needs. The main models used include WeatherObserved, which harmonizes meteorological real time data from different meteorological agencies; WeatherForecastSeries, an extended version of the FIWARE WeatherForecast model adapted to handle meteorological predictions; and StreamFlow, a new model specifically designed to represent hydrological variables such as flow and gauging data from the Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar (CHJ).</p> <p>As mention above, three different Smart Data Models have been designed, each focused on a specific type of data source. These include real-time data sources, climate model forecast sources, and hydrological data sources.</p> <ul> <li> <p>WeatherObserved: Fiware smart data model used to translate all the real time data. For more information about the model go to the json schema. This model will be used in different climate data sources such as:</p> </li> <li> <p>AEMET: Agencia Estatal de Meteorolog\u00eda</p> </li> <li>CHJ: Confederaci\u00f3n Hidrogr\u00e1frica del J\u00facar</li> <li>SiAR: Sistema de Informaci\u00f3n Agroclim\u00e1tica para el regad\u00edo</li> <li> <p>AVAMET: Agencia Estatal de Meteorolog\u00eda</p> </li> <li> <p>WeatherForecastSeries:. An extension of the smart data model WeatherForecast adaptad to time series data. Fore more information visit the json schema.</p> </li> <li> <p>StreamFlow: A new Smart Data Model focused on CHJ hydrometeorological data. It standardizes variables such as gauging and flow rates into a consistent format using the FIWARE Smart Data Models ontology. For deep understanding of the structure, visit the github.</p> </li> </ul> <p></p>"},{"location":"SmartDataModels/StreamFlow/","title":"StreamFlow","text":""},{"location":"SmartDataModels/StreamFlow/#streamflow-model","title":"StreamFlow Model","text":"<p>This section presents a proposal for the creation of a new Smart Data Model designed to represent hydrological information from the Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar (CHJ). The objective is to standardize the collection, integration, and exchange of river discharge and reservoir data within the PGTEC project framework. Specifically, data from three different types of hydrological data will be stored:</p> <ul> <li>River discharge data</li> <li>Precipitation data</li> <li>Reservoirs data </li> </ul> <p>Consequently, the proposed Smart Data Model will serve to standardize diverse climatic data, all sourced from the CHJ.</p>"},{"location":"SmartDataModels/StreamFlow/#purpose","title":"Purpose","text":"<p>The StreamFlow model aims to provide a unified structure to represent real-time and historical hydrological variables obtained from CHJ monitoring stations. By aligning with the FIWARE Smart Data Models ontology, this new model ensures semantic interoperability with other environmental and meteorological datasets used in the project.</p>"},{"location":"SmartDataModels/StreamFlow/#main-variables-of-interest","title":"Main Variables of Interest","text":"Parameter Description Unit Inflow Volume of incoming river water m\u00b3/s Outflow Volume of outgoing river water m\u00b3/s Water Level Elevation of the river or reservoir surface m Volume Total stored water in the reservoir m\u00b3 Filling Percentage Calculated percentage of capacity (0.0 to 1.0) % Water Flow Rate of water discharge at a specific point m\u00b3/s Stream Flow Flow rate of the specific stream/tributary m\u00b3/s Sediment Flow Rate of sediment transport in the water kg/s Precipitation Amount of water in 1 hour mm/s <p>These variables are critical for hydrological modeling, water resource management, and integration with simulation tools such as TETIS. These data will be used for getting the initial conditions of the basins of interest. These initial conditions are part of the TETIS input data. The rest of the data needed to train the TETIS model are the predictions that will come from another data sources such as Open-Meteo. To understand better the predictions environment created to extract different forecasts from a lot of climate models check the SmartFlow Environment.</p>"},{"location":"SmartDataModels/StreamFlow/#model-definition","title":"Model Definition","text":"<p>The proposed StreamFlow entity introduces the necessary attributes and metadata fields to harmonize CHJ hydrological data with the FIWARE ecosystem. It supports contextual information management through the Orion Context Broker, enabling real-time ingestion and storage into the TimeScaleDB which is a Postgres database designed for time series data that enables the historization of real time data for future uses such as dashboards or training new climate models. For more information about the real time data pipeline go to the DataFlow section.</p>"},{"location":"SmartDataModels/StreamFlow/#repository","title":"Repository","text":"<p>The model implementation in python can be found in the following Github repository:</p> <p>\ud83d\udd17 GitHub \u2013 StreamFlow Model Definition</p>"},{"location":"SmartDataModels/WeatherForecastSeries/","title":"WeatherForecastSeries","text":""},{"location":"SmartDataModels/WeatherForecastSeries/#weatherforecastseries-model","title":"WeatherForecastSeries Model","text":"<p>The WeatherForecastSeries Smart Data Model is an extension of the official WeatherForecast model from FIWARE\u2019s Smart Data Models initiative. Its main goal is to enable the representation of forecast time series (rather than a single prediction) within a single entity.</p>"},{"location":"SmartDataModels/WeatherForecastSeries/#key-modifications","title":"Key Modifications","text":"<ul> <li> <p>Array-based attributes   Most of the variable types defined in the original <code>WeatherForecast</code> model have been converted into arrays of their base type.   This structure allows the model to store multiple forecasted values (e.g., temperature, wind speed, precipitation) corresponding to different timestamps for a single point.</p> </li> <li> <p>Inlined definitions   Some variables originally referenced from Weather-Commons have now been defined directly inside the WeatherForecastSeries schema, improving self-containment and making the model easier to use independently.</p> </li> <li> <p>New <code>timestamps</code> attribute   A new property, <code>timestamps</code>, has been introduced to specify the exact prediction times corresponding to each value in the variable arrays.   This makes it possible to directly associate each forecasted parameter with its valid time, simplifying temporal analysis.</p> </li> </ul>"},{"location":"SmartDataModels/WeatherForecastSeries/#purpose-and-advantages","title":"Purpose and Advantages","text":"<p>This model enables the aggregation of complete forecast time series (e.g., hourly or daily predictions) in a single JSON object, reducing redundancy and improving data retrieval efficiency.</p> <p>In the PGTEC project, it is used to:</p> <ul> <li>Store weather predictions retrieved from different data sources such as AEMET, Open-Meteo, and Copernicus.  </li> <li>Provide these standardized forecasts as input for the TETIS hydrological model, ensuring that all variables share a common format and temporal alignment.  </li> <li>Facilitate interoperability between APIs, Airflow workflows, and visualization dashboards.</li> </ul>"},{"location":"SmartDataModels/WeatherForecastSeries/#example-entity","title":"Example Entity","text":"<p>Below is an example of a <code>WeatherForecastSeries</code> entity representing 3-hourly weather predictions for a given location:</p> <pre><code>{\n  \"id\": \"WeatherForecastSeries:Valencia:2025-03-01\",\n  \"type\": \"WeatherForecastSeries\",\n  \"location\": {\n    \"type\": \"Point\",\n    \"coordinates\": [-0.3763, 39.4699]\n  },\n  \"timestamps\": [\n    \"2025-03-01T00:00:00Z\",\n    \"2025-03-01T03:00:00Z\",\n    \"2025-03-01T06:00:00Z\"\n  ],\n  \"temperature\": [14.5, 13.8, 13.1],\n  \"precipitation\": [0.0, 0.2, 1.1],\n  \"windSpeed\": [3.4, 2.8, 4.1],\n  \"relativeHumidity\": [65, 70, 72],\n  \"source\": \"AEMET\",\n  \"address\": {\n    \"addressLocality\": \"Valencia\",\n    \"addressCountry\": \"ES\"\n  },\n  \"dateIssued\": \"2025-03-01T00:00:00Z\",\n  \"validFrom\": \"2025-03-01T00:00:00Z\",\n  \"validTo\": \"2025-03-01T06:00:00Z\"\n}\n</code></pre>"},{"location":"SmartDataModels/WeatherObserved/","title":"WeatherObserved","text":""},{"location":"SmartDataModels/WeatherObserved/#weatherobserved-model","title":"WeatherObserved Model","text":"<p>The WeatherObserved Smart Data Model is used to represent real-time meteorological observations within the PGTEC project. It follows the official FIWARE definition and does not require any structural modification, ensuring full compatibility and interoperability with other FIWARE-based systems.</p> <p>This model serves as the standard framework for integrating real-time data collected from multiple external sources, including:</p> <ul> <li>AEMET \u2014 Agencia Estatal de Meteorolog\u00eda</li> <li>CHJ \u2014 Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar </li> <li>AVAMET \u2014 Agencia Valenciana de Meteorolog\u00eda</li> <li>SiAR \u2014 Sistema de informaci\u00f3n agroclim\u00e1tica para el regad\u00edo</li> </ul> <p>By adopting the WeatherObserved schema, all incoming data are harmonized under a common structure, ensuring consistency in variable names, measurement units, and metadata representation across the entire data ecosystem.</p>"},{"location":"SmartDataModels/WeatherObserved/#main-variables-of-interest","title":"Main Variables of Interest","text":"<p>The following table describes the most important variables used in the PGTEC project:</p> Description FIWARE Attribute Unit Precipitation <code>precipitation</code> mm, l/m\u00b2 Air temperature <code>temperature</code> \u00b0C Relative humidity <code>relativeHumidity</code> % Snow depth <code>snowHeight</code> mm Wind speed <code>windSpeed</code> m/s, km/h"},{"location":"SmartDataModels/WeatherObserved/#role-within-pgtec","title":"Role within PGTEC","text":"<p>Within PGTEC, the WeatherObserved model is primarily used for the ingestion of real-time meteorological data. This approach ensures a unified and interoperable data layer, allowing seamless integration between the Airflow pipelines, FastAPI services, and the TETIS dashboard, where users can visualize or request the latest standardized weather observations.</p>"},{"location":"SmartFlow/","title":"Smart Flow","text":""},{"location":"SmartFlow/#about-the-project","title":"About The Project","text":"<p>This section is part of the tasks developed within the PGTEC project. Its main objective is to describe and provide the infrastructure required to deploy a data space using FIWARE technologies, offering a detailed and easy-to-follow guide adaptable to different environments. Specifically, it explains the logic used to download data from multiple sources, transform it into a common language using Smart Data Models, and make it available in two complementary ways:</p> <ul> <li> <p>Airflow Workflows: Python scripts that use Airflow logic to automate the execution of data retrieval and transformation tasks, ensuring that climate predictions are periodically processed and ready to use.</p> </li> <li> <p>FastAPI Services: Python scripts built with FastAPI that expose the processed data through RESTful APIs. These services feed a dashboard where users can select the data sources and points of interest required to run the TETIS hydrological model. Once the user makes a selection, TETIS automatically triggers the corresponding FastAPI Python scripts described in this section to retrieve and process the climate predictions, which are then used as input for the model\u2019s execution.</p> </li> </ul> <p>This section specifically describes the Python scripts used to:</p> <ul> <li> <p>Retrieve data from multiple climate data sources such as AEMET, CHJ, Open-Meteo, and Copernicus.</p> </li> <li> <p>Convert the raw data into FIWARE Smart Data Models to standardize the format.</p> </li> <li> <p>The creation of automated Airflow DAGs for pipeline execution.</p> </li> <li> <p>The creation of FastAPI scripts to provide data as a service neede to provide a dashboard where users select the data sources to run TETIS model (Hydrological model from IIAMA-UPV)</p> </li> </ul> <p>All the python files are in the SmartFlow Github Repository</p>"},{"location":"SmartFlow/#built-with","title":"Built With","text":"<p>The project is built using the following main components:</p>"},{"location":"SmartFlow/#getting-started","title":"Getting Started","text":"<p>To get a local copy up and running follow these simple steps in ubuntu command line:</p> <ol> <li>Clone the repo and navigate to the project folder</li> </ol> <pre><code>git clone https://github.com/PGTEC-VRAIN/SmartFlow\ncd SmartFlow\n</code></pre> <ol> <li>Initialize docker:</li> </ol> <pre><code>sudo systemctl start docker\n</code></pre> <ol> <li>Initialize docker containers    <pre><code>docker compose up --build -d\n</code></pre></li> </ol>"},{"location":"SmartFlow/#airflow-data-sources-overview","title":"Airflow Data Sources Overview","text":"<p>This section contains links to detailed explanations of the Python scripts that programmatically download forecasts from different models using Airflow. Specifically, each script description includes:</p> <ul> <li> <p>The name of the Python file in the SmartFlow Github Repository</p> </li> <li> <p>The data source accessed and the variables of interest</p> </li> <li> <p>The Smart Data Model used to standardize the data</p> </li> <li> <p>The required API key (if applicable)</p> </li> <li> <p>The suggested execution frequency to keep the data up to date</p> </li> </ul> <ul> <li> <p> HARMONIE/AROME- AEMET</p> </li> <li> <p> ARPEGE \u2013 OpenMeteo</p> </li> <li> <p> DWD_ICON \u2013 OpenMeteo</p> </li> <li> <p> AIFS_ECMWF \u2013 OpenMeteo</p> </li> <li> <p> GEPS_ENS_CNC \u2013 OpenMeteo</p> </li> <li> <p> GFS_NOAA \u2013 OpenMeteo</p> </li> <li> <p> IFS9km_ECMWF \u2013 OpenMeteo</p> </li> <li> <p> Seas5_ECMWF \u2013 OpenMeteo</p> </li> <li> <p> EFAS \u2013 Copernicus</p> </li> <li> <p> EFFIS \u2013 Copernicus</p> </li> </ul>"},{"location":"SmartFlow/#usage","title":"Usage","text":"<p>This section describes how to run and test the main components of the PGTEC platform \u2014 the Airflow workflows for data processing and the FastAPI services for data delivery and dashboard integration. Both components can be deployed together using the provided Docker Compose environment.</p> <p>It is supposed that the enviroment has been cloned following the instructions of Getting Started section.</p>"},{"location":"SmartFlow/#1-running-airflow-workflows","title":"1. Running Airflow Workflows","text":"<p>The Airflow DAGs automate the process of retrieving and transforming climate and hydrological data from different sources (AEMET, CHJ, Open-Meteo, Copernicus...).</p>"},{"location":"SmartFlow/#steps","title":"Steps","text":"<p>1.1 Start the containers:</p> <pre><code>docker-compose up -d\n</code></pre> <p>The -d option runs the containers in detached mode, hiding Airflow logs and keeping the terminal clean.</p>"},{"location":"SmartFlow/#11-access-the-airflow-web-interface","title":"1.1. Access the Airflow web interface:","text":"<p>http://localhost:8080</p>"},{"location":"SmartFlow/#12-enable-the-dags","title":"1.2. Enable the DAGs:","text":"<p>Inside the Airflow UI, activate the desired workflows (e.g., AEMET_HARMONIE_AROME, AIFS_ECMWF, etc.).</p>"},{"location":"SmartFlow/#13-monitor-execution","title":"1.3. Monitor execution:","text":"<p>You can visualize the data extraction and transformation progress directly in the DAG view.</p> <p>Below is an example of a Python script running in the Airflow user interface, showing the logs of its execution:</p> <p></p> <p>In the screenshot, the DWD_ICON.py workflow is being executed. In just 4.04 seconds, it retrieves several points of interest from the Valencian Community and stores them using the WeatherForecastSeries Smart Data Model format.</p>"},{"location":"SmartFlow/#2-running-fastapi-scripts","title":"2. Running FastAPI scripts.","text":"<p>The FastAPI services expose the processed data as REST APIs, allowing other applications \u2014 such as the TETIS dashboard \u2014 to access the latest standardized data stored in the FIWARE Context Broker.</p> <p>Each service corresponds to a specific data source or Smart Data Model and can be easily extended to include new ones.</p>"},{"location":"SmartFlow/#steps_1","title":"Steps","text":""},{"location":"SmartFlow/#21-make-sure-the-docker-environment-is-running","title":"2.1. Make sure the Docker environment is running:","text":"<pre><code>docker-compose up -d\n</code></pre>"},{"location":"SmartFlow/#22-access-the-fastapi-documentation","title":"2.2. Access the FastAPI documentation:","text":"<p>http://localhost:8000/docs</p> <p>This interface allows you to explore and test all available endpoints interactively.</p>"},{"location":"SmartFlow/#23-test-an-endpoint","title":"2.3. Test an endpoint:","text":"<p>For example, you can retrieve the latest weather forecast data by calling:</p> <pre><code>GET /weather?source=AEMET&amp;variable=temperature\n</code></pre>"},{"location":"SmartFlow/#24-integration-with-the-tetis-dashboard","title":"2.4 Integration with the TETIS dashboard:","text":"<p>The FastAPI services feed the TETIS dashboard, allowing users to select:</p> <ul> <li> <p>The data sources (e.g., AEMET, DWD, ECMWF)</p> </li> <li> <p>The points of interest (catchments, stations, or coordinates)</p> </li> </ul> <p>Once the user selects these options, TETIS automatically triggers the corresponding API calls \u2014 which execute the same Python scripts described in the SmartFlow section \u2014 to retrieve and process the forecast data used as inputs for hydrological simulations.</p>"},{"location":"SmartFlow/#example-folder-structure","title":"Example folder structure","text":"<pre><code>SmartFlow/FastAPI/\n\u251c\u2500\u2500 main.py           # FastAPI entry point\n\u251c\u2500\u2500 routes/\n\u2502   \u251c\u2500\u2500 AEMET.py      # Endpoints for WeatherObserved / Forecast data\n\u2502   \u251c\u2500\u2500 DWD_ICON.py   # Endpoints for CHJ flow data\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 models/\n    \u251c\u2500\u2500 AEMET.py\n    \u2514\u2500\u2500 DWD_ICON.py\n</code></pre> <p>New endpoints</p> <p>You can add new endpoints easily by creating a new Python file in the routes/ folder and registering it in main.py.</p>"},{"location":"SmartFlow/#license","title":"License","text":"<p>Distributed under the AGPL-3.0 License. See <code>LICENSE</code> for more information.</p>"},{"location":"SmartFlow/#contact","title":"Contact","text":"<p>Project Link: https://github.com/PGTEC-VRAIN</p>"},{"location":"SmartFlow/#references","title":"References","text":"<ul> <li>Readme Template</li> <li>Smart Data Models Weather Smart Data Model - Fiware</li> </ul>"},{"location":"SmartFlow/metadata_scripts/AIFS_ECMWF/","title":"Model AIFS - Europe Centre Medium Weather Forecasts","text":"<p>This section describes the AIFS data integration process.</p> <ul> <li> <p><code>AIFS_ECMWF.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/ARPEGE/","title":"Model ARPEGE - MeteoFrance","text":"<p>This section describes the ARPEGE data integration process.</p> <ul> <li> <p><code>ARPEGE.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Relative Humidity</li> <li>Solar Radiation</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/DWD_ICON/","title":"Model ICON_EU - DWD","text":"<p>This section describes the ICON_EU data integration process.</p> <ul> <li> <p><code>DWD_ICON_EU.py</code>: Python script that retrieves weather data from the Open-Meteo API programmatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul> <p>\u2b05\ufe0f Back to overview</p>"},{"location":"SmartFlow/metadata_scripts/EFAS/","title":"Model EFAS  - Copernicus","text":"<p>This section describes the data integration process of EFAS model from Copernicus Early Warning Data Store (EWDS).</p> <ul> <li> <p><code>EFAS.py</code>: Python script that retrieves weather data from the EWDS dataset (https://ewds.climate.copernicus.eu/datasets/efas-forecast?tab=overview) and download data programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are NetCDF (.nc) files containing the following weather variable:</p> <ul> <li>River discharge last 6 hours</li> </ul> </li> <li> <p>API Key: An API key is required for execution. To get an API key, you need to register on the Copernicus Climate Data Store website: https://cds.climate.copernicus.eu</p> </li> <li> <p>Run script: Run this script daily because Copernicus updates the data once a day. It has been configured to automatically detect the latest available forecast.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/EFFIS/","title":"Model EFFIS  - Copernicus","text":"<p>This section describes the data integration process of EFFIS model from Copernicus Climate Data Store (CDS).</p> <ul> <li> <p><code>EFFIS.py</code>: Python script that retrieves weather data from the dataset (https://cds.climate.copernicus.eu/datasets/sis-tourism-fire-danger-indicators?tab=overview) and download data programamatically using airflow sintaxis. </p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are NetCDF (.nc) files containing various weather variables:</p> <ul> <li>Daily fire weather index</li> </ul> </li> <li> <p>API Key: An API key is required for execution. To get an API key, you need to register on the Copernicus Climate Data Store website: https://cds.climate.copernicus.eu</p> </li> <li> <p>Run script: Run this script daily because Copernicus updates the data once a day. It has been configured to automatically detect the latest available forecast.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/GEPS_ENS_CNC/","title":"Model GEPS Ensemble - Canadian Meteorological Centre","text":"<p>This section describes the data integration process of GEPS Ensemble model.</p> <ul> <li> <p><code>GEPS_ENS_CNC.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/GFS_NOAA/","title":"Model GFS  - National Oceanic and Atmospheric Administration (NOAA)","text":"<p>This section describes the data integration process of GFS model.</p> <ul> <li> <p><code>GFS_NOAA.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed</li> <li>Solar Radiation</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/Harmonie_AEMET/","title":"Model HARMONIE/AROME - AEMET","text":"<p>This section describes the AEMET data integration process.</p> <ul> <li> <p><code>AEMET_HARMONIE_AROME.py</code>: Python script that retrieves weather forecast data from the AEMET website and processes it programatically using airflow sintaxis</p> </li> <li> <p>Data: All the data is processed into a standardized format defined by the <code>WeatherForecastSeries.py</code> Smart Data Model. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed </li> </ul> </li> <li> <p>Raw Data: The input data are GeoTIFF (.tif) files containing weather variables encoded as color values. Using a color scale provided by AEMET, the script converts these color codes (RGBA) into real physical values such as temperature, wind speed, and precipitation.</p> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from AEMET website.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul> <p>\u2b05\ufe0f Back to overview</p>"},{"location":"SmartFlow/metadata_scripts/IFS9km_ECMWF/","title":"Model IFS  - Europe Centre Medium Weather Forecasts (ECMWF)","text":"<p>This section describes the data integration process of IFS model.</p> <ul> <li> <p><code>IFS9km_ECMWF.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul>"},{"location":"SmartFlow/metadata_scripts/Seas5_ECWMF_copernicus/","title":"Model Seas5  - Europe Centre Medium Weather Forecasts (ECMWF)","text":"<p>This section describes the data integration process of Seas5 model from Copernicus.</p> <ul> <li> <p><code>Seas5_ECWMF_copernicus.py</code>: Python script that retrieves weather data from the Open-Meteo API programamatically using airflow sintaxis.</p> </li> <li> <p>Data: processes it into a standardized format using <code>WeatherForecastSeries.py</code> Smart Data Model. </p> </li> <li> <p>Raw Data: The input data are JSON files. The variables we are currently using are:</p> <ul> <li>Temperature </li> <li>Precipitation</li> <li>Wind Speed</li> <li>Solar Radiation</li> </ul> </li> <li> <p>API Key: No API key is required for execution as we are downloading public data from Open-Meteo API.</p> </li> <li> <p>Run script: Run hourly to ensure the latest forecasts are always retrieved and processed.</p> </li> </ul> <p>The last two scrips retrieve data from Copernicus Climate Data Store (CDS) API and Early Warning Data Store (EWDS):</p>"},{"location":"data_sources/","title":"Data Sources","text":""},{"location":"data_sources/#data-sources","title":"Data Sources","text":"<p>This section will detail the different data sources available to feed the prediction models and algorithms that will enable us to anticipate extreme weather events, thereby contributing to a more effective response to meteorological emergencies. Their usefulness, quality and availability will be evaluated.</p> <ul> <li> <p> Conferencia Hidrogr\u00e1fica del J\u00facar \u2013 SAIH</p> <p>Public body responsible for water management in the J\u00facar River Basin District.</p> <p> Learn more</p> </li> <li> <p> Sistema de Informaci\u00f3n Agroclim\u00e1tica para el Regad\u00edo (SiAR)</p> <p>Network of over 520 weather stations that captures, records, and disseminates agroclimatic data necessary to determine the water demand of irrigated farms.</p> <p> Learn more</p> </li> <li> <p> Agencia Estatal de Meteorolog\u00eda (AEMET)</p> <p>Official Spanish government agency responsible for observing, forecasting and studying meteorological phenomena.  </p> <p> Learn more</p> </li> <li> <p> AEMET (ROCIO)</p> <p>AEMET Observational Grid with Optimal Interpolation. </p> <p> Learn more</p> </li> <li> <p> AEMET (Spain02)</p> <p>Institutional collaboration between AEMET and the Santander Meteorology Group (SMG) at the University of Cantabria-CSIC.  </p> <p> Learn more</p> </li> <li> <p> Copernicus</p> <p>Project coordinated and managed by the European Commission.</p> <p> Learn more</p> </li> <li> <p> MSWEP</p> <p>Multi-Source Weighted-Ensemble Precipitation (MSWEP) is a high-resolution global precipitation dataset.  </p> <p> Learn more</p> </li> <li> <p> CEDEX</p> <p>It is the centre for studies and experimentation in public works.</p> <p> Learn more</p> </li> <li> <p> AVSRE</p> <p>The Valencian Agency for Security and Emergency Response (AVSRE) manages civil protection, emergency management, firefighting and public safety in the Valencian Community.</p> <p> Learn more</p> </li> </ul>"},{"location":"data_sources/metadata_datasources/aemet_datasource/","title":"Aemet datasource","text":"<p> Back to Data Sources</p>"},{"location":"data_sources/metadata_datasources/aemet_datasource/#agencia-estatal-de-meteorologia-aemet","title":"Agencia Estatal de Meteorolog\u00eda (AEMET)","text":"<p>The State Meteorological Agency (AEMET) is the official Spanish government body responsible for observing, predicting and studying meteorological phenomena. As the main authority, AEMET provides reliable, consistent and standardised data, following the guidelines of the World Meteorological Organisation (WMO). </p> <p>Its AEMET OpenData service is the designated platform for disseminating this information, offering access to a wide range of meteorological and climatological products. The quality of its data is backed by rigorous validation processes and by the National Plan for the Prediction and Monitoring of Adverse Meteorological Phenomena (Meteoalerta), which establishes the risk thresholds for alerts. </p> <p>In the Valencian Community, there are approximately 40 meteorological stations that transmit data on a regular basis and are therefore used for live weather monitoring. </p> <p>It should be noted that, although real-time data is subject to automatic checks, it may contain occasional errors. Even so, the data collected by AEMET is considered to be of high quality due to its official nature. Consolidated climate data undergoes more exhaustive validation processes before its final publication. </p>"},{"location":"data_sources/metadata_datasources/aemet_datasource/#api","title":"API","text":"<p>The AEMET API is based on REST architecture and returns data mainly in JSON format, using international standards such as CAP (Common Alerting Protocol) for warnings and GeoJSON to represent affected areas.</p> <p>Data is available on:</p> <ul> <li>Meteorological observations</li> <li>Weather forecasts</li> <li>Radar and lightning</li> <li>Alerts</li> </ul> <p>With regard to historical data, there is no fixed start year for the data in the \u2018climatological values\u2019 section of the API. For daily, monthly and annual climatology and extreme values, the start date of the records depends on each individual weather station. For normal climatology, it returns average values calculated over the period from 1991 to 2020. </p> <p>The API is public, but registration is required. An API Key must be requested on the official AEMET OpenData. It is valid for three months and is linked to an email address. </p>"},{"location":"data_sources/metadata_datasources/aemet_rocio_datasource/","title":"Aemet rocio datasource","text":"<p> Back to Data Sources</p>"},{"location":"data_sources/metadata_datasources/aemet_rocio_datasource/#aemet-rocio","title":"AEMET (ROCIO)","text":"<p>The abbreviation ROCIO stands for Rejilla Observacional Con Interpolaci\u00f3n \u00d3ptima (Optimal Interpolation Observation Grid). AEMET has two types of grids to cover all island areas. The first high-resolution grid, approximately 5 km, covering mainland Spain and the Balearic Islands is called ROCIO_IBEB, and the second 2.5 km grid covering the Canary Islands is ROCIO_CAN. In both, the daily data on accumulated precipitation in 24 hours, maximum temperature and minimum temperature from a large number of stations of the State Meteorological Agency have been interpolated.   </p> <p>Therefore, for each grid there are two types of data sets available:  </p> <ul> <li> <p>Precipitation. </p> </li> <li> <p>Maximum and minimum temperatures. </p> </li> </ul> <p>With regard to the ROCIO_IBEB grid, the version 2 precipitation dataset has been generated using all the stations available in the AEMET National Climate Data Bank, i.e. 3,236 rainfall stations. This grid covers the period from 1951 to 2022 and is updated periodically.  </p> <p>The extreme temperature (maximum and minimum) datasets, version 1, have been generated using all the stations available in AEMET's National Climate Data Bank, 1,800 thermometric stations. These grids begin in 1951 and are updated until December 2022. Unlike the precipitation grid, they use climatology based on historical analyses of the HIRLAM numerical prediction model operated by AEMET as initial information, or first estimate, which is corrected by observations.  </p> <p>With regard to the ROCIO_CAN grid, the precipitation and temperature datasets have been compiled in the same way as those for ROCIO_IBEB, but with projections using regular coordinates (latitude and longitude) thanks to the use of the HARMONIE model. In addition, the data cover the period from 1990 to 2022 but are updated periodically.  </p>"},{"location":"data_sources/metadata_datasources/aemet_rocio_datasource/#api","title":"API","text":"<p>The ROCIO grid data is available on the AEMET website and also from the THREDDS server at the University of Cantabria, displayed using the OpeNDAP protocol. This protocol allows access to variables such as precipitation and temperatures (maximum and minimum) on a 20 km grid, covering mainland Spain and the Balearic Islands. The dataset includes historical series since 1950 and is not updated in real time, as it consists of interpolated observational data. Therefore, there is no real-time data from the ROCIO grid. As a result, update times are unknown. </p>"},{"location":"data_sources/metadata_datasources/aemet_spain_datasource/","title":"Aemet spain datasource","text":"<p> Back to Data Sources</p>"},{"location":"data_sources/metadata_datasources/aemet_spain_datasource/#aemet-spain02","title":"AEMET (Spain02)","text":"<p>The Spain02 dataset is the result of institutional collaboration between the State Meteorological Agency (AEMET) and the Santander Meteorology Group (SMG) at the University of Cantabria-CSIC.  </p> <p>The dataset consists of a 20 km resolution grid covering mainland Spain and the Balearic Islands, generated using interpolation techniques based on daily observations at meteorological stations. The aim is to provide a homogeneous and continuous basis of climate information, suitable for regional analyses and climate change impact studies. </p> <p>Similar to the ROCIO grid, Spain02 includes precipitation and maximum and minimum temperature series, offering a homogeneous and continuous product over time, suitable for regional analyses, hydrological studies, climate change impact assessments and climate modelling. The availability of multiple interpolation versions and resolutions allows the data to be adapted to different scales of analysis and modelling methodologies, ensuring the robustness of the scientific studies that use them. </p>"},{"location":"data_sources/metadata_datasources/aemet_spain_datasource/#api","title":"API","text":"<p>The only way to access the SPAIN02 grid data is through the THREDDS data centre, which stores a replica of the data files. This data centre is developed and maintained by the University of Cantabria. This server allows you to browse the catalogue of available datasets and, using the OPeNDAP (Open-source Project for a Network Data Access Protocol) protocol, remotely query and extract only the necessary variables or subsets, without having to download the entire files. </p> <p>Update times are unknown. The data files only cover up to the year 2022, so there is no real-time data available. With regard to historical data, the data files contain data from 1951 onwards, so there are large volumes of historical data available. </p>"},{"location":"data_sources/metadata_datasources/avsre_datasource/","title":"Avsre datasource","text":"<p> Back to Data Sources</p>"},{"location":"data_sources/metadata_datasources/avsre_datasource/#avsre","title":"AVSRE","text":"<p>The Valencian Agency for Security and Emergency Response (AVSRE) is the body of the Valencian Regional Government responsible for directing and managing civil protection, emergency management, firefighting and public safety in the Valencian Community. Its main function is to coordinate the various emergency services and ensure a rapid and effective response to any type of incident, including those of climatic origin. </p> <p>As a coordinating body, the AVSRE centralises a large amount of critical information for emergency management. This information comes from the following sources:</p> <ul> <li> <p>Calls to the 112 emergency telephone number: records of incidents reported by the public.</p> </li> <li> <p>First responders: police, fire brigade, health services, etc. </p> </li> <li> <p>Meteorological agencies: data and alerts from the State Meteorological Agency (AEMET) and the Valencian Cartographic Institute (ICV). </p> </li> <li> <p>Hydrographic confederations: information on the state of rivers and reservoirs. </p> </li> <li> <p>Firefighting consortia and civil protection units. </p> </li> </ul> <p>The AVSRE processes this information to generate alerts, coordinate the mobilisation of resources and provide up-to-date information to the public and the authorities.  </p> <p>The quality of the data managed by the AVSRE is considered high in terms of reliability and official status, as it is the body that centralises and verifies the information from all the agencies involved in an em</p>"},{"location":"data_sources/metadata_datasources/avsre_datasource/#api","title":"API","text":"<p>The AVSRE publishes institutional information, plans and notices, but the operational data needed for flood management usually comes from various connected providers (AEMET, Hydrographic Confederations, local councils, local sensor networks). </p> <p>The Regional Government maintains an open data catalogue (GVA Oberta / data portal) where data sets and REST APIs are published for many regional and municipal services; the AVSRE consumes and coordinates these flows in its operation.</p> <ul> <li>Open data portal 'Dades Obertes GVA'</li> <li>Real-time information (Portal 112cv.gva.es and GVA Oberta)</li> </ul>"},{"location":"data_sources/metadata_datasources/cedex_datasource/","title":"Cedex datasource","text":"<p> Back to Data Sources</p>"},{"location":"data_sources/metadata_datasources/cedex_datasource/#cedex","title":"CEDEX","text":"<p>CEDEX is the centre for public works studies and experimentation. It is a cutting-edge public body in the field of public works, mobility, inland and marine waters, the environment and climate change. It is attached to the Ministry of Transport and Sustainable Mobility, which also reports to the Ministry for Ecological Transition and Demographic Challenge.  </p> <p>CEDEX is divided into eight specialised technical units called Centres and Laboratories. From each centre, CEDEX provides high-level technical assistance, carries out applied research and technological development, and transfers knowledge through courses, conferences and seminars. Of the eight centres, the Centre for Hydrographic Studies stands out, providing the climate data necessary for this project. </p> <p>Its functions include the evaluation and certification of materials and techniques, the development of standards and regulations, and the promotion of R&amp;D&amp;I projects in line with national and European plans. In addition, it provides specialised technical assistance to administrations and private entities, collaborates in the conservation of infrastructure heritage, and promotes knowledge transfer through publications, courses and technical meetings. It also promotes national and international cooperation in the scientific-technical field and, in certain cases, acts as an arbitrator in disputes related to its area of competence. </p>"},{"location":"data_sources/metadata_datasources/cedex_datasource/#api","title":"API","text":"<p>CEDEX has a large amount of data available on its website. As such, it does not have an API for accessing the data. The data is fully accessible from the website itself.  </p> <p>Among the available data, CEDEX presents eight different databases, each covering a different area of those described above. The databases are as follows: </p> <ul> <li> <p>Environmental Restoration Portal </p> </li> <li> <p>Capacity Yearbook </p> </li> <li> <p>Noise Mapping System </p> </li> <li> <p>Coast and Sea Information System (INFOMAR) </p> </li> <li> <p>Spanish Precipitation Isotope Monitoring Network (REVIP) </p> </li> <li> <p>Spanish Water Information System (Hispagua) </p> </li> <li> <p>Catalogue of waste materials that can be used in construction </p> </li> </ul> <p>The database of interest for the project is the Flow Measurement Yearbook. This database is an official publication that compiles daily, monthly and annual data on flow rates measured at river and reservoir gauging stations. </p> <p>It includes hydrological records such as: </p> <ul> <li> <p>Average daily, monthly and annual flow rates. </p> </li> <li> <p>Precipitation and inflows in the basins.</p> </li> <li> <p>Information on the measuring stations (location, technical characteristics, etc.).</p> </li> <li> <p>Evapotranspiration data such as temperature and precipitation. </p> </li> </ul> <p>The data files are divided by each basin organisation. Among them is the J\u00facar Hydrographic Conference (CHJ), which has all the data on the reservoirs and the J\u00facar River, which is the basin of interest for the project. </p> <p>The update times are not defined on the website. The time range covered by the data is from the last century to a few years ago. Depending on the dataset downloaded, the time span is different. For example, for the CHJ data, specifically the daily data files on reservoirs, canals, rivers and evaporation, the time span is different. </p>"},{"location":"data_sources/metadata_datasources/copernicus_datasource/","title":"Copernicus datasource","text":"<p> Back to Data Sources</p>"},{"location":"data_sources/metadata_datasources/copernicus_datasource/#copernicus","title":"Copernicus","text":"<p>The Copernicus programme is a project coordinated and managed by the European Commission, which aims to achieve a comprehensive, continuous and autonomous high-quality Earth observation capability, the results of which are freely accessible to the scientific community, authorities and individuals.  </p> <p>The overall objective is to provide accurate, reliable and continuous information in order to, among other things, improve environmental management and conservation, understand and mitigate the effects of climate change, and ensure civil security. </p> <p>It aims to bring together different sources of information from environmental satellites and ground-based stations to provide a global view of the Earth's \u2018state of health\u2019. </p> <p>To this end, Copernicus has two sites where the different data it collects can be accessed:</p> <ul> <li> <p>Climate Data Store (CDS): A tool for accessing more than 140 data sets with information on the state of our climate around the world over the last 100 years. </p> </li> <li> <p>Atmosphere Data Store (ADS): A tool for accessing more than 100 data sets with satellite data from around the world and over the last 100 years. </p> </li> </ul> <p>The data sets presented are as follows:</p> <ul> <li> <p>ERA5-Land hourly data from 1950 to the present. </p> </li> <li> <p>RA5 hourly data from 1940 to the present. </p> </li> </ul>"},{"location":"data_sources/metadata_datasources/copernicus_datasource/#api","title":"API","text":"<p>The CDS, developed within the framework of the European Commission's Copernicus programme, makes a wide range of climate and atmospheric data available to the public. To access this information, the CDS API, called cdsapi, is mainly used, which is designed for use in Python. </p> <p>The API usage flow is simple and combines both the web interface and script automation: </p> <ul> <li> <p>Data selection on the website: From the Climate Data Store web portal, users can browse more than 140 available datasets, as well as more than 100 offered by the Atmosphere Data Store. Each dataset has filters to specify variables, time intervals, spatial resolution, and output format. </p> </li> <li> <p>Automatic script generation: Once the desired parameters have been selected, the platform automatically generates a Python script that uses the cdsapi library. This script can be run locally to download data, facilitating reproducibility and automation of the process. </p> </li> <li> <p>Download via API: The cdsapi library manages the connection to the CDS servers, authenticates the user's request, and downloads the selected files in the specified format (e.g., NetCDF or GRIB). </p> </li> </ul> <p>The API has a large amount of data available. For example, for the ERA5 hourly data set from 1940 to the present available from the Climate Data Store, we can download a large amount of data: temperature at different levels and soil types, wind, radiation, heat, clouds, lakes, water evaporation, precipitation and rain, snow, soil, vertical integrals, vegetation, and waves. </p> <p>Regarding real-time data, it should be noted that for the data sets analysed, the most recent data is from 6 days prior to the current day. The update time is daily, so each day that passes, data from day t-6 is added, where t is the current day.  </p> <p>With regard to historical data, ERA 5 and ERA5-Land have records dating back to 1940, providing data from almost 85 years ago. Figure 12 shows the years of data available for the ERA5 data set. </p>"},{"location":"data_sources/metadata_datasources/mswep_datasource/","title":"Mswep datasource","text":"<p> Back to Data Sources</p>"},{"location":"data_sources/metadata_datasources/mswep_datasource/#mswep","title":"MSWEP","text":"<p>Multi-Source Weighted-Ensemble Precipitation (MSWEP) is a high-resolution global precipitation dataset. It does not include variables such as temperature, humidity, etc. Its main feature is that it combines or merges different types of data sources to obtain the most accurate and reliable rainfall estimate possible. The amount of precipitation is expressed in mm/3h. It is not an instantaneous precipitation rate. To obtain daily values, the eight 3-hour intervals that make up a day must be added together. To obtain average hourly rates, the 3-hour value would be divided by three.  </p> <p>The data is obtained through: </p> <ul> <li> <p>Data from land-based weather stations. </p> </li> <li> <p>Satellite estimates (including oceans and remote areas). </p> </li> <li> <p>Climate reanalysis data (atmospheric models that reconstruct past climate). </p> </li> </ul> <p>By merging each source, MSWEP is able to correct for the weaknesses of each source individually.  </p>"},{"location":"data_sources/metadata_datasources/mswep_datasource/#api","title":"API","text":"<p>MSWEP does not offer an API to which requests are made in real time. Instead, access to data is managed through standard protocols and formats for handling large scientific data sets, mainly through files. </p> <p>As mentioned above, the only data it contains is precipitation. It is presented as the amount of rainfall over a 3-hour period, measured in millimetres (mm).  </p> <p>There are two versions of the data: </p> <ul> <li> <p>Near-Real-Time (NRT): updated with a delay of approximately 3 to 12 hours.  </p> </li> <li> <p>Research version: this is a corrected and more accurate version that is updated with a delay of several months. </p> </li> </ul> <p>One of its features is full access to the historical record. Data is available from 1979 to the present, allowing for the analysis of trends, past extreme events, and the training of models with a very solid database. </p> <p>Access to MSWEP data is public for research and educational purposes. To access it, please contact MSWEP:</p> <ul> <li> <p>If you are affiliated with a commercial entity and would like to try MSWEP, please fill out the application form.  </p> </li> <li> <p>If you do not have a commercial affiliation and intend to use the product for non-commercial purposes, please use the Apply here form  in the Data licence section. Once you have completed the form, you will receive a link to the shared Google Drive folder once your request has been approved. </p> </li> </ul>"},{"location":"data_sources/metadata_datasources/siah_datasource/","title":"Siah datasource","text":"<p> Back to Data Sources</p>"},{"location":"data_sources/metadata_datasources/siah_datasource/#confederacion-hidrografica-del-jucar-saih","title":"Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar \u2013 SAIH","text":"<p>The J\u00facar Hydrographic Confederation (CHJ) is the public body responsible for water management in the J\u00facar River Basin District, which covers a large part of the Valencian Community and areas of Castilla La Mancha, Aragon and Catalu\u00f1a. Its mission is to ensure sustainable water use, prevent risks associated with droughts and floods, and protect river ecosystems. </p> <p></p> <p>The J\u00facar Automatic Hydrological Information System (SAIH) is a technological network managed by the CHJ that collects real-time hydrological, hydraulic and meteorological data from across the entire basin. It consists of automatic sensors, measuring stations and a control centre that processes the information. Through the SAIH J\u00facar, the CHJ offers a comprehensive information system that connects institutional management with real-time monitoring technology. </p> <p>The SAIH collects data from three types of stations: </p> <ul> <li> <p>Gauges: measure the flow rate in rivers, tributaries and watercourses. They provide essential data for forecasting floods, assessing droughts and determining the availability of water resources. </p> </li> <li> <p>Reservoirs: record the volume of water stored, total capacity and daily variations. Key information for resource management, flood control and agricultural and urban planning.</p> </li> <li> <p>Rain gauges: collect accumulated precipitation at different time intervals. This allows for analysis of rainfall distribution and anticipation of extreme events. </p> </li> </ul>"},{"location":"data_sources/metadata_datasources/siah_datasource/#api","title":"API","text":"<p>The CHJ does not have a public API, although it does have a public dashboard, allowing access to all information without any verification via SAIH.</p> <p>To access SAIH data via its private API, you must contact the CHJ and make the relevant request.</p> More information <p>To access SAIH data through its private API, you must contact the CHJ and make the relevant request.  After this, you will obtain a username and password with which we will generate an authorisation token  that will be used to authenticate future requests.  </p> <p>In addition, you must be able to access their private network, specifically port 8000, either through  the IP whitelist or via a VPN connection.  </p> <p>The API address is: https://api-saih.chj.es:8000.  </p> <p>A graphical interface for the API is also available through the open source swagger tool. It can be accessed at https://api-saih.chj.es:8000/docs.</p> <p>Through the API, we can access real-time data from different sensors, updated every five minutes, which for the purposes of the project are classified into three types: </p> <ul> <li> <p>Rain gauges: these record accumulated rainfall in 1h, 4h, 12h and 24h and their operational status. </p> </li> <li> <p>Gauges: measure instantaneous flow with reference thresholds (low, medium and high) along with the date of the last measurement.  </p> </li> <li> <p>Reservoirs: report the elevation (metres above sea level (m a.s.l.)), the stored volume (hm\u00b3) and the inflow and outflow rates (m\u00b3/s).  </p> </li> </ul> Reservoirs Gauges Rain gauges 25 180 78 <p>In addition to real-time data, the J\u00facar Hydrographic Confederation also has historical data, although this is not accessible via API as it is not stored in a unified database but distributed across different time series. However, it is possible to perform an initial dump of this information into the platform's database, which would allow for its integration and subsequent exploitation. </p>"},{"location":"data_sources/metadata_datasources/siar_datasource/","title":"Siar datasource","text":"<p> Back to Data Sources</p>"},{"location":"data_sources/metadata_datasources/siar_datasource/#sistema-de-informacion-agroclimatica-para-el-regadio-siar","title":"Sistema de Informaci\u00f3n Agroclim\u00e1tica para el Regad\u00edo (SiAR)","text":"<p>The Agroclimatic Information System for Irrigation (SiAR) of the Ministry of Agriculture, Fisheries and Food is a network of more than 520 weather stations that captures, records and disseminates agroclimatic data necessary to determine the water demand of irrigated farms. </p> <p>The Sub-Directorate General for Irrigation, Natural Roads and Rural Infrastructure of the Ministry of Agriculture has been developing, maintaining and updating SiAR for more than 20 years. SiAR is currently the largest source of agroclimatic information provided by both the Ministry and the autonomous communities. </p> <p>The SiAR network places special emphasis on maintaining the high quality and accuracy of the data it provides. </p> <p>There are more than 360 stations managed by the Ministry and more than 160 managed by the autonomous communities. The SiAR network in the Valencian Community has 55 stations located in irrigated areas. </p>"},{"location":"data_sources/metadata_datasources/siar_datasource/#api","title":"API","text":"<p>SiAR allows access to all your data via REST API, provided that the maximum number of requests and the maximum amount of data queried per day and per minute are respected. Each user has different restrictions regarding the maximum number of requests. </p> More information <p>The technical manual for using the SIAR API specifies: </p> <p>\"Each web client is restricted in the number of requests it can make throughout a day and within a one-minute interval. Similarly, and for identical reasons, the number of agroclimatic data that can be consulted in a day and in a minute will also be limited\"  (Ministry of Agriculture, Fisheries and Food [MAPA], 2023, p. 12). </p> <p>To find out about the restrictions, each user can check their limitations via the following URL: https://servicio.mapama.gob.es/apisiar/API/v1/Info/Accesos?ClaveAPI={PUT_API_KEY}. When using the API key provided, the SIAR returns a response in JSON format indicating the limitations of API use to the user. The results are:</p> <ul> <li> <p>Maximum number of accesses per minute: 15</p> </li> <li> <p>Maximum number of accesses per day: 240</p> </li> <li> <p>Maximum number of records per minute: 100</p> </li> <li> <p>Maximum number of records per day: 10,000 </p> </li> </ul> <p>With regard to the amount of data available, SIAR allows data to be downloaded at the autonomous community, province and station level for the whole of Spain. For example, there are 41 weather stations in the province of Valencia. </p> <p>The SIAR has been storing data since 1999, so the time span is 25 years. This means that historical data is accessible. </p> <p>With regard to real-time data, attempts have been made to download the most recent data for several stations, and the most recent data returned by the API is from two hours ago.</p> <p>The data update times are not known exactly. The update time is estimated to be 30 minutes, given that the finest temporal granularity is every 30 minutes. </p> <p>Regarding temporal resolution, the technical manual explains the different granularities for accessing data according to the required time span (hourly, daily, weekly, monthly, etc.). </p>"},{"location":"getting_started/","title":"Getting started","text":""},{"location":"getting_started/#getting-started","title":"Getting Started","text":"<p>This section is the entry point to PGTEC for anyone who has never heard about the project before.</p> <p>The goal of this guide is to answer three basic questions:</p> <ol> <li>What is PGTEC?</li> <li>What does PGTEC offer?</li> <li>How can I join the PGTEC Data Space?</li> </ol>"},{"location":"getting_started/#who-is-this-documentation-for","title":"Who is this documentation for?","text":"<p>PGTEC documentation is meant for different types of users:</p> <ul> <li>Public authorities and emergency management teams interested in accessing harmonised climate and hydrological information.</li> <li>Researchers and technical partners who want to reuse PGTEC data for modelling, analytics or decision-support systems.</li> <li>Developers and integrators who want to deploy connectors, data pipelines or services using PGTEC building blocks.</li> </ul> <p>If you are in at least one of these groups, the following pages will guide you step by step.</p>"},{"location":"getting_started/#organization-of-the-guide","title":"Organization of the guide","text":"<p>The guide follows this points:</p> <ol> <li> <p>What is PGTEC?    An overview of PGTEC as a climate and weather data space, its objectives and main components.</p> </li> <li> <p>Data and Services    A guided tour of the data sources and services that PGTEC exposes, and how they are prepared using the PGTEC data pipeline and Smart Data Models.</p> </li> <li> <p>Join the Data Space    A high-level description of how organisations will be able to connect to PGTEC, request access and deploy a data space connector.</p> </li> </ol> <p>The following sections have more detailed information:</p> <ul> <li>Data Sources explains the different climate and hydrological sources used in the project and how they are integrated.</li> <li>Data Flow describes the end-to-end pipeline from raw data to context information stored in a FIWARE Context Broker.</li> <li>Smart Data Models introduces the harmonised data models used to represent climate, hydrological and risk entities.</li> <li>SmartFlow explains how the Airflow and FastAPI components orchestrate data retrieval, transformation and exposure as services.</li> <li>Data Space describes the PGTEC data space architecture, its main components, and how identity, access and governance are handled.</li> </ul> <p>More details</p> <p>If you just want a high-level understanding, reading this Getting Started section is enough.  </p> <p>If you plan to deploy infrastructure or contribute data, you will probably want to follow all the links to the detailed sections.</p>"},{"location":"getting_started/data_and_services/","title":"Data and Services","text":""},{"location":"getting_started/data_and_services/#data-and-services-in-pgtec","title":"Data and Services in PGTEC","text":"<p>This page answers the question:</p> <p> What does PGTEC offer in practice?</p> <p>PGTEC brings together climate, hydrological and risk-related information, processes it through a common pipeline and exposes it as interoperable services that can be reused by different applications.</p>"},{"location":"getting_started/data_and_services/#data-sources","title":"Data Sources","text":"<p>The detailed description of each source is available in the Data Sources section. Here we summarise the main categories:</p> <ul> <li> <p>Numerical weather prediction models    Forecasts from several models are retrieved programmatically, including configurations such as HARMONIE/AROME, ARPEGE, ICON, AIFS and GFS.</p> </li> <li> <p>Hydrological and flood-related services    Products from Copernicus services such as EFAS or EFFIS are integrated when relevant for early flood or fire risk assessment.</p> </li> <li> <p>National and regional agencies    Data from organisations such as AEMET and the Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar contributes local accuracy and observations needed to validate or complement modelled information.</p> </li> </ul> <p>Each source is mapped to one or more Smart Data Models so that variables like temperature, precipitation or discharge share the same structure regardless of the original provider.</p>"},{"location":"getting_started/data_and_services/#from-raw-data-to-context-entities","title":"From raw data to context entities","text":"<p>The transformation from raw files or external APIs into PGTEC entities follows these steps:</p> <ol> <li> <p>Retrieval     Airflow DAGs periodically call the external APIs or download files from each data provider.</p> </li> <li> <p>Transformation     Python scripts convert the raw input into entities that follow Smart Data Models. This includes handling coordinates, time grids, units and metadata.</p> </li> <li> <p>Loading into the Context Broker     The transformed entities are pushed into a FIWARE Context Broker, where they can be queried using a common NGSI-LD interface.</p> </li> <li> <p>Historical storage     Historical extensions keep track of time series so that models and dashboards can access both the latest situation and past evolution.</p> </li> </ol> <p>All these steps are orchestrated by the SmartFlow component, which provides Docker-based deployment, Airflow configuration and FastAPI services.</p>"},{"location":"getting_started/data_and_services/#services","title":"Services","text":"<p>Once data is harmonised and stored in the Context Broker, PGTEC enables several types of services:</p> <ul> <li> <p>Data-as-a-Service APIs    FastAPI endpoints expose subsets of data tailored to specific use cases, for example climate predictions for a given point of interest or catchment. These services can be called from external applications or dashboards. </p> </li> <li> <p>Support for hydrological modelling    The PGTEC pipeline feeds the configuration of the TETIS hydrological model, allowing users to select sources and locations from a dashboard, which then triggers the underlying FastAPI logic.</p> </li> <li> <p>Early warning and risk indicators    By combining meteorological and hydrological information, PGTEC can compute derived indicators for flood or drought risk that support early warning workflows.</p> </li> <li> <p>Experimentation and research    Since data is exposed in a standardised way, it can be reused for machine learning, downscaling or impact modelling without having to reimplement the ingestion logic for each project.</p> </li> </ul> <p>More details</p> <p>If you are mainly interested in the content that PGTEC provides:</p> <ul> <li>Start with Data Sources to see each provider and variable in detail.</li> <li>Continue with Smart Data Models to understand how entities are structured.</li> <li>If you want to run the pipelines or services yourself, go to SmartFlow, which explains cloning the repository and starting Docker containers.</li> </ul>"},{"location":"getting_started/join_data_space/","title":"Join Data Space","text":""},{"location":"getting_started/join_data_space/#join-the-pgtec-data-space","title":"Join the PGTEC Data Space","text":"<p>This page explains, at a high level, how organisations will be able to participate in the PGTEC data space once the operational environment is in place.</p> <p>The approach is aligned with other European Data Space initiatives, where access is based on trusted digital identities and the deployment of interoperable data space connectors.</p>"},{"location":"getting_started/join_data_space/#participation-roles","title":"Participation roles","text":"<p>There are several ways to interact with PGTEC:</p> <ul> <li> <p>Data Consumer    An organisation that wants to reuse PGTEC data (for example in dashboards, decision-support tools or research workflows).</p> </li> <li> <p>Data Provider    An organisation that wants to publish its own climate, hydrological or risk-related data into PGTEC using the agreed Smart Data Models.</p> </li> <li> <p>Service Provider   A partner that builds applications on top of PGTEC, such as early warning services, modelling platforms or analytics tools.</p> </li> </ul> <p>The technical and governance requirements will depend on the chosen role.</p>"},{"location":"getting_started/join_data_space/#what-will-you-need","title":"What will you need?","text":"<p>To participate in the data space, we required two main elements:</p> <ol> <li> <p>An organisational digital identity     This is often realised as a verifiable credential or digital certificate that uniquely identifies your organisation and its roles inside the data space. Components like VC issuers and Trust Anchors are used to manage these credentials.</p> </li> <li> <p>A Data Space connector     A software component deployed in your own infrastructure that handles authentication, authorisation and data exchange with the rest of the ecosystem. In FIWARE-based Data Spaces, reference implementations of connectors are available as open source.</p> </li> </ol> <p>PGTEC plans to follow this pattern so that participants can connect using standard, reusable components instead of custom one-off integrations.</p>"},{"location":"getting_started/join_data_space/#join","title":"Join","text":"<p>As we said we have different roles to participate into the Data Space, this are the detailed process to join it.</p>"},{"location":"getting_started/join_data_space/#joining-as-a-data-consumer","title":"Joining as a Data Consumer","text":"<p>The detailed access procedure will be defined as the operational environment stabilises, but the expected steps are:</p> <ol> <li> <p>Request access     Contact the PGTEC operators and explain your intended use case. In future iterations this will likely be handled through a dedicated \u201cRequest access\u201d page or portal.</p> </li> <li> <p>Obtain credentials     After validation, your organisation will receive the necessary credentials or configuration to authenticate against the PGTEC Trust Anchor or identity management system.</p> </li> <li> <p>Deploy or configure a connector     You can either deploy a full data space connector or, for simple read-only scenarios, configure existing components to call PGTEC APIs exposed by the Context Broker and FastAPI services.</p> </li> <li> <p>Integrate data into your applications     Once access is granted, you can start using PGTEC data for dashboards, models or other tools, following the Smart Data Models described in the documentation.</p> </li> </ol>"},{"location":"getting_started/join_data_space/#joining-as-a-data-provider","title":"Joining as a Data Provider","text":"<p>If you want to contribute data to PGTEC, the process will extend the consumer steps with additional tasks:</p> <ol> <li> <p>Align with Smart Data Models     Map your internal data structures to the Smart Data Models used in PGTEC so that your information can be interpreted consistently.</p> </li> <li> <p>Implement ingestion logic     Reuse or adapt SmartFlow pipelines or equivalent scripts to pull or receive your data and transform it into PGTEC entities.</p> </li> <li> <p>Configure your connector as a provider    Set up your data space connector so that it can publish your entities or expose your own Context Broker in a way that is compatible with the PGTEC governance rules.</p> </li> <li> <p>Agree on policies and terms of use    Define under which conditions your data can be accessed, including licensing, retention and usage constraints.</p> </li> </ol>"},{"location":"getting_started/join_data_space/#current-status","title":"Current status","text":"<p>At the current stage of the project, PGTEC is still in an experimental and pre-production phase. This means that:</p> <ul> <li>Some access procedures may still be manual and based on direct contact with the project team.</li> <li>Endpoints, policies and governance details are subject to change as the data space evolves.</li> <li>The documentation will be updated as soon as stable connectors, trust services and self-service registration mechanisms are available.</li> </ul> <p>In the meantime, you can already:</p> <ul> <li>Explore all public documentation in this site.</li> <li>Clone and test the SmartFlow repository and its Docker environment.</li> <li>Review the Data Sources and Smart Data Models sections to prepare your systems for future integration.</li> </ul> <p>If you are interested in participating, the recommended first step is to reach out to the PGTEC coordinators through the contact information provided in the main project website.</p>"},{"location":"getting_started/what_is_pgtec/","title":"What is PGTEC?","text":""},{"location":"getting_started/what_is_pgtec/#what-is-pgtec_1","title":"What is PGTEC?","text":"<p>PGTEC is a climate and weather data space that supports early warning and risk management for climate emergencies. It connects data providers, data users and digital services through shared standards, governance rules and interoperable components.</p> <p>Instead of being a single data platform, PGTEC is designed as a shared space where multiple organisations can publish, access and reuse data under common technical standards and governance rules.</p> <p>The project combines:</p> <ul> <li>Climate and weather predictions from several numerical models and providers.</li> <li>Hydrological and risk-related information used to feed early warning systems.</li> <li>An interoperable infrastructure based on FIWARE technologies, Smart Data Models and Data Space components.</li> </ul>"},{"location":"getting_started/what_is_pgtec/#why-a-data-space-and-not-just-a-data-platform","title":"Why a data space and not just a data platform?","text":"<p>Traditional data platforms usually belong to a single organisation and expose their own APIs and formats. A data space introduces additional concepts:</p> <ul> <li>Shared governance instead of a single owner.</li> <li>Common data models and APIs to reduce integration costs.</li> <li>Trust and identity mechanisms to control who can access what.</li> <li>The possibility to connect multiple platforms rather than replacing them.</li> </ul> <p>In the European context, data spaces are promoted as a way to enable secure and sovereign data sharing between organisations, particularly in domains such as smart cities, mobility and climate services.</p> <p>PGTEC applies these ideas to the specific domain of climate emergency management.</p>"},{"location":"getting_started/what_is_pgtec/#pgtec-high-level-architecture","title":"PGTEC high-level architecture","text":"<p>At a very high level, PGTEC can be seen as four layers:</p> <ol> <li> <p>Data Sources    Climate and hydrological data are periodically retrieved from several providers such as AEMET, Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar, Open-Meteo and Copernicus services.</p> </li> <li> <p>Data ingestion and processing     The SmartFlow component orchestrates Python scripts and Airflow workflows that download forecasts, transform them using Smart Data Models and prepare them for storage and service exposure.</p> </li> <li> <p>Context management and storage    Harmonised entities are stored in a FIWARE Context Broker with historical persistence, enabling both real-time queries and time-series analysis for machine learning and hydrological models.</p> </li> <li> <p>Services and Applications    FastAPI services expose processed data through RESTful APIs and feed dashboards, such as the one used to configure inputs for the TETIS hydrological model.</p> </li> </ol>"},{"location":"getting_started/what_is_pgtec/#technologies","title":"Technologies","text":"<p>PGTEC relies on several key technologies that appear across the documentation:</p> <ul> <li> <p>FIWARE Context Broker    Manages context information and exposes NGSI-LD APIs, acting as the core registry of entities in the data space.</p> </li> <li> <p>Smart Data Models    Provide a common schema for climate variables, hydrological indicators and risk entities so that data from different sources can be compared and combined consistently.</p> </li> <li> <p>SmartFlow    A collection of Airflow DAGs and FastAPI services, with Docker-based deployment, that automates data retrieval, transformation and publication. </p> </li> <li> <p>Decentralized Identifiers (DIDs)     Used to uniquely identify organisations, services or components in a self-sovereign way, without relying on a single central authority. PGTEC demonstrates the use of did:web and did:key methods together with Docker-based tooling to generate DID documents.</p> </li> </ul> <p>More details</p> <p>If you want to dive into the technical details after this overview:</p> <ul> <li>See Data Flow for an end-to-end description of the ingestion pipeline.</li> <li>See Smart Data Models for the entity schemas used across the project.</li> <li>See SmartFlow if you are planning to run the pipelines or services locally.</li> <li>See Data Space for the architecture, governance and trust model of the PGTEC data space.</li> </ul>"},{"location":"pipeline/","title":"Data Flow","text":""},{"location":"pipeline/#definition","title":"Definition","text":"<p>The purpose of this section is to explain all the data flows that take place within the PGTEC ecosystem. Since the objective of the project is to create an interoperable data space in which climate data and services can be securely shared, the associated data flows play a key role.</p> <p>For the development of the project, data flows have been classified according to the temporal nature of the climate data. Based on this criterion, two main groups have been defined:</p> <ul> <li> <p>Meteorological prediction data flow: Includes all data flows from climate agencies that provide future or forecast data to the ecosystem.</p> </li> <li> <p>Real-time data flow: Includes climate agencies that provide real-time or near real-time climate data to the ecosystem.</p> </li> </ul> <p>This classification improves the clarity and understanding of the different data flows and facilitates a more detailed analysis of each one.</p>"},{"location":"pipeline/#types","title":"Types","text":"<p>Each data flow comprises a set of climate agencies or institutions that provide climate data of particular relevance for the early prevention of climate-related emergencies. All climate agencies are therefore equally important, as all climate data, regardless of the data flow to which they belong, contribute to anticipating adverse meteorological phenomena.</p> <p>The following subsections describe in detail the data flows introduced above. Defining structured and traceable data flows is essential to ensure timely access to data and to enable secure data sharing within the data space, preserving data integrity and supporting informed decision-making for the prevention of adverse events.</p> <p>To explore the different data flows, navigate to the following sections:</p> <ul> <li>Dive into Real Time Data Flow to access the explanation of real-time data flows.</li> <li>Go to Predictions Data Flow to access the explanation of meteorological prediction data flows.</li> </ul> <p>Next steps</p> <p>Click on Real Time Data in the bottom navigation bar located at the upper-left to advance to the next section.</p>"},{"location":"pipeline/example_copernicus/","title":"Example","text":"<p>This is an example to use the environment using the Copernicus data as an example with the DAG:</p> <p>flujo_coperniocus_orion_quantumleap.py</p>"},{"location":"pipeline/example_copernicus/#instructions","title":"Instructions","text":"<ol> <li>Get a free Copernicus Climate Data Store API Key at Copernicus Climate Data Store</li> </ol> <p> 2. Clone the repository  <pre><code>git clone https://github.com/PGTEC-VRAIN/Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap.git\n</code></pre> 3. Run Docker service <pre><code>sudo service docker start\n</code></pre> 4. Access to the correct repository with the environment Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap <pre><code>cd Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap\n</code></pre> 5. Change API Key at the script flujo_coperniocus_orion_quantumleap.py <pre><code> c = cdsapi.Client(url=\"https://cds.climate.copernicus.eu/api\",\n                   key=\"PUT_YOUR_API_KEY_HERE\")\n</code></pre> 6. Run the docker compose yaml <pre><code>docker compose up --build\n</code></pre> 7. Check the status of the containers in a new terminal <pre><code>docker ps\n</code></pre> 8. Access to Airflow to see the DAG running (You may create an account first)  9. Check CreateDB to get historical data You may create an account first  10. Shut down the docker <pre><code>docker compose down\n</code></pre></p>"},{"location":"pipeline/requirements/","title":"Requirements","text":""},{"location":"pipeline/requirements/#what-do-i-need","title":"What do I need?","text":"<p>To correctly deploy and operate the data pipeline, the following components and requirements must be available:</p> <ul> <li> <p>Python: Required for implementing auxiliary scripts, data preprocessing tasks, and integration logic with external services.</p> </li> <li> <p>Docker Compose: Used to orchestrate and deploy the FIWARE stack.</p> </li> <li> <p>Airflow: Provides workflow orchestration, ensuring the automated and scheduled execution of data collection, transformation, and loading tasks.</p> </li> <li> <p>Access credentials: An API key or endpoint URL to connect to the external data sources supplying the raw information.</p> </li> <li> <p>Networking and FIWARE configuration: Proper network setup and FIWARE component configuration to allow seamless communication between all services in the stack.</p> </li> </ul>"},{"location":"pipeline/requirements/#core-components","title":"Core Components","text":"<ul> <li>IoT Agent: Interface for receiving IoT device data and forwarding it to the appropriate FIWARE context entities.</li> <li>Orion-LD Context Broker: Core NGSI-LD\u2013based context management engine responsible for storing, updating, and distributing contextual information.</li> <li>QuantumLeap: Time-series database service that persists historical events and temporal data, supporting advanced analytics and AI model training.</li> <li>CreateDB: Serves as the historical storage backend within the FIWARE stack.</li> <li>MongoDB: Provides context data persistence for the Orion-LD Context Broker.</li> <li>Apache Airflow: Workflow orchestration through Directed Acyclic Graphs (DAGs), enabling automation and scheduling of data-processing pipelines.</li> </ul> <p>This project was developed and tested on: </p> <ul> <li>Ubuntu 24.04.1 LTS</li> </ul> <p>These are the necessary requirements to be able to execute the project:</p> Software Version / Note Docker 27.2.0 Docker Compose 27.2.0 Python Docker container Airflow Docker container IoT Agent-UL Docker container Orion-LD Context Broker Docker container QuantumLeap Docker container / CreateDB backend"},{"location":"pipeline/setup/","title":"Set up","text":"<p>This repository provides a complete environment for orchestration and management of IoT data, integrating a set of FIWARE-based components and workflow automation tools.</p>"},{"location":"pipeline/setup/#repository-contents","title":"Repository Contents","text":"<ul> <li> <p>docker-compose.yml: Defines and configures all services and containers within the stack.</p> </li> <li> <p>dags/: Directory containing Airflow DAGs for automated workflows.</p> </li> <li> <p>Scripts:</p> <ul> <li>AEMET.py: Fetches meteorological data from the AEMET API.</li> <li>Copernicus.py: Retrieves Copernicus climate datasets for the province of Valencia using Python\u2019s cdsapi library.</li> <li>Flujo_Copernicus_orion.py: DAG that collects Copernicus data, registers it in the IoT Agent, and stores it in Orion-LD.</li> <li>Flujo_copernicus_orion_quantumleap.py: Enhanced DAG that extends the previous workflow by integrating QuantumLeap for historical data persistence in CrateDB. (Recommended DAG for production use.)</li> </ul> </li> </ul>"},{"location":"pipeline/setup/#install-the-software","title":"Install the software","text":"<p>The following commands can be used to install some of the necessary software:</p>"},{"location":"pipeline/setup/#docker","title":"Docker","text":"<pre><code>sudo apt update\nsudo apt install -y apt-transport-https ca-certificates curl software-properties-common\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt update\nsudo apt install -y docker-ce docker-ce-cli containerd.io\n</code></pre>"},{"location":"pipeline/setup/#docker-compose","title":"Docker Compose","text":"<pre><code>sudo curl -L \"https://github.com/docker/compose/releases/download/v2.20.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n</code></pre>"},{"location":"pipeline/setup/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/PGTEC-VRAIN/Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap.git\ncd Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap\n</code></pre>"},{"location":"pipeline/setup/#python-environment-from-anaconda-miniconda","title":"Python (Environment from anaconda / miniconda)","text":"<pre><code># Download Miniconda installer (Linux x86_64)\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh\n\n# Run the installer\nbash ~/miniconda.sh\n\n# Follow the prompts (accept license, choose install path, initialize conda)\n\n# Initialize conda for bash\nsource ~/.bashrc\n\n# Create a project environment (Python 3.12.3)\nconda create -n pgtec_env python=3.12.3 -y\nconda activate pgtec_env\n</code></pre>"},{"location":"pipeline/setup/#setup-instructions","title":"Setup instructions","text":""},{"location":"pipeline/setup/#clone-the-repository_1","title":"Clone the repository","text":"<pre><code>git clone https://github.com/PGTEC-VRAIN/Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap.git\ncd Entorno-AirFlow_IotAgent_OrionLD_QuantumLeap\n</code></pre>"},{"location":"pipeline/setup/#start-the-containers","title":"Start the containers","text":"<pre><code>docker-compose up -d --build\n</code></pre> <ul> <li> <p>Use --build only if changes were made to the docker-compose.yml.</p> </li> <li> <p>The -d option runs services in the background (detached mode).</p> </li> </ul>"},{"location":"pipeline/setup/#verify-running-services","title":"Verify running services","text":"<pre><code>docker ps\n</code></pre>"},{"location":"pipeline/setup/#shut-down-the-environment","title":"Shut down the environment","text":"<ul> <li>From a new terminal: <pre><code>docker-compose down\n</code></pre></li> <li>Or interrupt the current process with CTRL + C.</li> </ul>"},{"location":"pipeline/predictions_flow/","title":"Predictions","text":""},{"location":"pipeline/predictions_flow/#overview","title":"Overview","text":"<p>Different climate agencies around the world are used to provide weather forecast data from their climate models. In this case, the Open-Meteo wrapper is used, as it provides direct access to the forecasts of several climate models via an API, eliminating the need to navigate to the official websites of each climate agency individually. It also offers the data in a standardised format, facilitating conversion to the JSON-LD format used by Smart Data Models. The climate agencies that hold relevant PGTEC project predictions data, given their national scope, are:</p> <ul> <li> <p>CNC (Canadian National Committee):  </p> <ul> <li>Model GEPS (Global Ensemble Prediction System). </li> </ul> </li> <li> <p>NOAA (National Oceanic and Atmospheric Administration):  </p> <ul> <li>Model GFS (Global Forecasting System). </li> </ul> </li> <li> <p>DWD (Deutscher Wetterdienst):  </p> <ul> <li>Model ICON (Icosahedral Con-hidrost\u00e1tico). </li> </ul> </li> <li> <p>Meteofrance:</p> <ul> <li>Model Arpege-Arome. </li> </ul> </li> <li> <p>ECMWF (Europe Centre for Medium Weather Forecasts): </p> <ul> <li> <p>Model IFS (Integrated Forecasting System). </p> </li> <li> <p>Model AIFS (Artificial Intelligence Forecasting System). </p> </li> <li> <p>Model Seas5 (Seasonal Forecasting System 5). </p> </li> </ul> </li> <li> <p>AEMET (Agencia Estatal de Meteorolog\u00eda):  </p> <ul> <li>Model Harmonie-Arome. </li> </ul> </li> </ul> <p>The data flow for weather forecasts differs considerably from the real-time data flow. Since weather forecasts provided by climate models cover time spans ranging from several days to weeks, storing all of these forecasts in Orion-LD and persisting them in TimeScaleDB would incur very high computational costs. Therefore, we have chosen to store the forecast files directly in Amazon S3 buckets and only process files that are requested by users or participants in the data space. To facilitate this, we have created a REST API to handle the process. Once it receives a user's request for predictions, the API processes the raw files and converts them to the Smart Data Model Weather Forecast Series format, thus providing standardised predictions for ease of use.</p>"},{"location":"pipeline/predictions_flow/#components-of-the-data-flow","title":"Components of the data flow","text":"<p>The components of the data flow used to collect predictions are as follows:</p> <ul> <li> <p>Airflow: Como se explica en el flujo de datos de tiempo real Airflow is responsible for orchestrating and scheduling the periodic acquisition of prediction data. It is the only component shared between both data flows.</p> </li> <li> <p>Bucket S3: Used to store raw prediction files obtained from Open-Meteo. These files contain forecasts for different models, variables and time horizons and are kept unprocessed until requested.</p> </li> <li> <p>SmartFlow API: REST API designed to manage access to prediction data. It retrieves raw files from the S3 bucket, processes them on demand and transforms the data into Smart Data Models, ensuring consistency and interoperability across the data space.</p> </li> <li> <p>APISIX: An API Gateway that acts as the single entry point to the data space services. It is responsible for handling authentication, authorization and request routing, ensuring that only authorized users or services can access internal components. This component is already in the Fiware Data Space Connector.</p> </li> </ul>"},{"location":"pipeline/predictions_flow/#stages-of-data-flow","title":"Stages of data flow","text":"<p>As explained above, the prediction data flow follows a demand-driven lifecycle that optimises storage and processing resources:</p> <ul> <li> <p>Data acquisition: Prediction data from multiple climate models is periodically retrieved through Open-Meteo or using Airflow and stored as raw files in the S3 bucket.</p> </li> <li> <p>On-demand processing: When a user or service requests prediction data, the SmartFlow API identifies and loads the relevant raw files from S3.</p> </li> <li> <p>Standardisation: The requested data is transformed into the Weather Forecast Series Smart Data Model, converting variables, units and structures into a common representation.</p> </li> <li> <p>Delivery: The standardised prediction data is returned to the requester through the API, ready for direct use in simulations, dashboards or decision-support tools.</p> </li> </ul> <p>To visually understand the data flow, a flow chart has been created that captures the passage of data between the different components of the flow and the interactions between them.</p> <p></p> <p>As shown in the diagram, access to prediction data is always mediated by the SmartFlow API, which acts as the entry point to the raw prediction files stored in Amazon S3 and ensures that all outputs conform to the Smart Data Models used within the PGTEC data space. </p> <p>All user and service requests are first routed through APISIX, which acts as the API Gateway and security entry point of the FIWARE connector. APISIX is responsible for authentication and authorization, ensuring that only authenticated dashboards or services can access the available endpoints. Once the credentials are validated, APISIX routes each request to the appropriate internal service, in this case, the SmartFlow API.</p> <p>This architecture enforces a single, secure entry point to the prediction services while maintaining a clear separation between access control, routing and data processing within the PGTEC data space.</p>"},{"location":"pipeline/real_time_data_flow/","title":"Real Time Data","text":""},{"location":"pipeline/real_time_data_flow/#overview","title":"Overview","text":"<p>This data flow brings together climate data from multiple agencies and institutions that provide information in real time or near real time. Specifically, the climate agencies that hold real-time data relevant to the PGTEC project, given their national scope, are:</p> <ul> <li>AEMET (Agencia Estatal de Meteorolog\u00eda)</li> <li>SIAR (Sistema de Informaci\u00f3n Agroclim\u00e1tica para el regad\u00edo) </li> <li>CHJ Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar</li> <li>AVAMET Agencia Valenciana de Meteorolog\u00eda</li> </ul> <p>To enable effective data sharing within the data space, it is not enough to simply collect information from different sources. The data must be continuously updated, expressed in a standardised format and made easily accessible to different organisations and applications.</p> <p>For this purpose, a structured data flow is defined, covering several consecutive stages from data acquisition at the source to its storage and distribution within the data space. This flow ensures that each dataset follows a consistent process of collection, translation, standardisation, management and historical persistence, allowing it to be reliably integrated into the overall data ecosystem.</p>"},{"location":"pipeline/real_time_data_flow/#components-of-the-data-flow","title":"Components of the data flow","text":"<p>The different components or services used to implement the real-time data flow are presented below.</p> <ul> <li> <p>Airflow: An orchestration tool that allows you to schedule and automate data collection tasks. It is responsible for periodically executing scripts that query APIs or download files from different sources. This service makes it possible to obtain near real-time data at regular intervals, continuously updating climate information and enabling early response to adverse phenomena.</p> </li> <li> <p>Orion-LD (Context Broker): A Context Broker that stores real-time data generated by Airflow in JSON-LD format. Each time new data is received from data platforms, it is written to Orion-LD, updating the entity values with the most recent information. It is the central component of the data flow, acting as the main data manager.</p> </li> <li> <p>TimeScaleDB: A solution designed by TigerData to store temporal and spatial data, as historical information is also required in order to generate maps or train predictive models. It is an extension of a Postgres SQL database ready to store time-series related data. Orion-LD is responsible for inserting the data into the database, preserving real-time values before they are overwritten by newer updates. This is achieved through the use of TROE (Temporal Representation of Entities), which is natively supported by Orion-LD.</p> </li> <li> <p>Mintaka: A component that enables efficient querying of historical data stored in TimeScaleDB. It acts as a query service that retrieves data from the TimeScale database in an optimized manner and returns it in NGSI-LD format.</p> </li> <li> <p>APISIX: An API Gateway that acts as the single entry point to the data space services. It is responsible for handling authentication, authorization and request routing, ensuring that only authorized users or services can access internal components. This component is already in the Fiware Data Space Connector.</p> </li> </ul>"},{"location":"pipeline/real_time_data_flow/#stages-of-data-flow","title":"Stages of data flow","text":"<p>The data lifecycle comprises several stages, from the initial distribution of data across different sources to its incorporation into the ecosystem after passing through an ETL (Extract-Transform-Load) processing stage that cleans and standardises it. The stages are as follows:</p> <ul> <li> <p>Data collection: Climate data is collected periodically from different sources, mostly through REST APIs, although some comes from direct downloads from web pages. This process is automated with Apache Airflow, which, thanks to the use of DAGs (Directed Acyclic Graphs) in Python, allows dependencies to be defined, execution to be scheduled (e.g., hourly) and regular, reliable data updates to be ensured.</p> </li> <li> <p>Standardisation and translation: Each source uses different variables and formats, so it is necessary to unify names, units, and structures. This task is performed at the end of the data collection phase. When the data is already downloaded, the scripts translate the data into common models defined by FIWARE's Smart Data Models. For further details, see the section below Smart Data Models.</p> </li> <li> <p>Contextual management and distribution: Standardised data is managed by the Orion-LD Context Broker, which stores and distributes it in JSON-LD format. This makes it easier for multiple applications or services to consume it consistently.</p> </li> <li> <p>Historical and analytical persistence: Finally, real-time data is stored in a time-series database through Orion-LD using TROE component. In this way, complete records are built that allow different data space participants such as IIAMA to train machine learning and deep learning models, which are key to the prediction and early detection of adverse climate phenomena.</p> </li> </ul> <p>To visually understand the data flow, a flow chart has been created that captures the passage of data between the different components of the flow and the interactions between them.</p> <p></p> <p>As can be seen in the image, all historical data is stored in TimeScaleDB. To access this data, Mintaka must be used, as it is designed to perform optimised queries on the database. Requests for real-time data can be made directly to Orion-LD. Therefore, depending on the required data, requests must be made to either component.</p>"},{"location":"welcome/","title":"Welcome","text":"<p>This site serves as a comprehensive documentation hub for the PGTEC project.  </p>"},{"location":"welcome/#about-pgtec","title":"About PGTEC","text":"<p>Following the terrible consequences of the 2024 DANA floods in the province of Valencia, PGTEC aims to develop a platform for climate emergency prevention and management, providing advanced tools for the collection, analysis and modelling of environmental and climate data. This solution is integrated into interoperable data spaces, allowing public administrations, emergency management agencies and companies in the climate sector to access real-time information for decision-making.</p> <p>This project is part of the call for sectoral data spaces from the Data Space Reference Centre (CRED), under the Ministry for Digital Transformation and Public Administration, within the products and services programme.</p>"},{"location":"welcome/#platform","title":"Platform","text":"<p>The platform will contribute to the digitalization of the climate resilience and emergency management sector, fostering the creation of data-driven products and services aligned with the principles of the European Data Strategy and the Recovery, Transformation and Resilience Plan.</p>"},{"location":"welcome/#beneficiary-sectors","title":"Beneficiary sectors","text":"<ul> <li>Emergency management and civil protection</li> <li>Environment and climate change  </li> <li>Public administration and smart cities </li> <li>Mobility and logistics</li> </ul>"},{"location":"welcome/#technologies","title":"Technologies","text":"<p>To ensure interoperability, trust, and security across the data space, the project integrates several core technologies. The main ones are:</p> <ul> <li> <p>APIs: Various types of public and private APIs are used to collect data from different climate agencies, as well as proprietary REST APIs designed specifically for the project.</p> </li> <li> <p>Digital Twins: Different participants in the data space contribute hydrological, hydraulic and fire models to be fed with data and provide vital predictions for the prevention of climate emergencies. Other participants contribute dashboards or interactive maps to simulate real and future weather scenarios, such as IIAMA's Water4Cast application.</p> </li> <li> <p>Smart Data Models: Data schemas used to create a common ontology to standardise all data in the data space following the JSON-LD format. For more information go to the  Smart Data Models section.</p> </li> <li> <p>Data Spaces: Following the developments of Fiware for data spaces, the aim of the project is to implement a trusted, interoperable and federated data space that adds value to citizens and the government when making data-based decisions in the event of a climate emergency. Notable developments include the Trust Anchor component, which acts as a police force within the data space to ensure the trust and security of participants.</p> </li> <li> <p>Verifiable Credentials: Verifiable credentials that comply with European standards and the EUDI European wallet are used for storage to ensure total security in the data space. Each participant will have credentials with which to interact with the other participants. This ensures a reliable data space. </p> </li> <li> <p>Cloud Services: To ensure that the platform can scale efficiently, the data space has been designed to operate in the cloud, leveraging Amazon Web Services (AWS). The project has the collaboration of the companies Kanzo-Tech and Think-IT to carry out the migration to the cloud.</p> </li> </ul>"},{"location":"welcome/#data-sources","title":"Data Sources","text":"<p>The project is designed to prevent national climate emergencies, so the climate agencies of interest will be those that have national information. In addition, following the consequences of the DANA explained above, considerable emphasis has been placed on climate agencies in the Valencian Community. Therefore, the national climate agencies that provide data for the project are:</p> <ul> <li>Agencia Estatal de Meteorolog\u00eda (AEMET)</li> <li>Sistema de Informaci\u00f3n Agroclim\u00e1tica para el Regad\u00edo (SiAR)</li> <li>Confederaci\u00f3n Hidrogr\u00e1fica del J\u00facar (CHJ)</li> <li>Agencia Valenciana de Meteorologia (AVAMET)</li> </ul> <p>On the other hand, the international climate agencies that have been used for the development of the project are as follows:</p> <ul> <li>Copernicus Climate Change &amp; Emergency</li> <li>Canadian Meteorological Centre (MSC)</li> <li>German Meteorological Centre (Deutscher Wetterdienst - DWD)</li> <li>MeteoFrance</li> <li>Europe Centre for Medium-Range Weather Forecasts (ECMWF)</li> <li>National Oceanic and Atmospheric Administration (NOAA)</li> </ul>"},{"location":"welcome/#services-offered","title":"Services offeredImage of L'Horta Sud (Valencia) Flooded After the DANA of October 29, 2024","text":"<p>Currently, PGTEC offers the following data related services in the data space:</p> <ul> <li> <p>Hydrological Model (TETIS): A spatially distributed hydrological and sediment cycle simulation model that divides the basin into regular cells and uses physically based parameters. As a comprehensive model, it can address flooding and erosion processes (with minute- or hour-level temporal resolution) as well as water-resources problems (daily resolution). It also includes a robust automatic calibration algorithm for both effective parameters and initial state variables, greatly simplifying practical implementation.</p> </li> <li> <p>Real Time Dashboard: A dynamic, user-configurable map developed by VRAIN-UPV that visualizes different meteorological variables\u2014such as temperature and precipitation\u2014in real time across various locations throughout the national territory.</p> </li> <li> <p>Tetis predictions Dashboard: An interactive map developed by VRAIN-UPV to display model outputs after executing the TETIS hydrological model. It allows users to select multiple input datasets to run TETIS simulations and visualize these simulations simultaneously, enabling comparative analysis of inputs and supporting decision-making processes.</p> </li> <li> <p>Water4Cast APP: An application developed by IIAMA designed to provide a visualization and decision-support system to predict variables and indicators relevant to the efficient management of water resource systems and river basins. The case study focuses on the Cuenca del J\u00facar, the main basin in the Valencian Community.</p> </li> </ul> <p>In the future, it is expected that the University of Le\u00f3n will share the wildfire prediction model they have developed. Any additional participant joining the data space as a service provider will be incorporated accordingly.</p> <p>Next steps</p> <p>Click on Getting started in the bottom navigation bar to advance to the next section.</p> <p>You can use the buttons in the bottom navigation bar to navigate between the previous and next pages or jump to a section with the side navigation bars.</p>"}]}