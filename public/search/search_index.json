{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"welcome/","title":"Welcome","text":"<p>This site serves as a comprehensive documentation hub for the CitCom.ai project, focusing on offering the Minimal Package for Testing and Experimentation Facilities (TEF) nodes. The Minimal Package contains comprehensive guides, toolboxes, and deployment frameworks to ensure interoperability across the CitCom.ai wide range of TEF nodes by defining a common reference architecture. Based on this architecture, it also provides support and examples for developing AI services.</p>"},{"location":"welcome/#about-citcomai","title":"About CitCom.ai","text":"<p>The CitCom.ai project is an ambitious European Union initiative aimed at establishing a comprehensive Testing and Experimentation Facilities (TEFs) for AI and robotics, tailored explicitly to smart and sustainable cities and communities. This project is designed to accelerate the adoption of AI technologies across Europe by providing real-world testing environments where companies can develop, test, and validate their AI-driven products and services. CitCom ensures that these technologies meet the latest EU regulations, including those related to data privacy and cybersecurity while supporting the creation of interoperable and ethically sound AI solutions. The project spans multiple countries, with over 30 participating organizations, and it emphasizes a coordinated approach to fostering innovation that aligns with the EU\u2019s goals for a greener and more digital future.</p> <p></p> <p></p> <p>Next steps</p> <p>Click on Getting started in the bottom navigation bar to advance to the next section.</p> <p>You can use the buttons in the bottom navigation bar to navigate between the previous and next pages or jump to a section with the side navigation bars.</p>"},{"location":"data_catalog/","title":"Data Catalog","text":"<p>The data catalog is a centralized hub to keep track of available datasets. It is regularly updated to include new data as it becomes available in any TEF node. If you want access to any dataset, please click \"Contact\" to reach the owners.</p> <p>How to add new datasets?</p> <p>:simple-github: Add New Datasets </p> Dataset Super Node TEF Node Site Data Model Sampling Time Historical Owner Get Access Waste Container South Spain Valencia gitlab_vlci RealTime From 2000 Val\u00e8ncia City Council Contact Weather Forecast South Spain Valencia gitlab_vlci Daily From 2010 Val\u00e8ncia City Council Contact Bikeparking stands in Aarhus City Nordic Denmark GTM no specific value Ongoing No Aarhus Municipality City of Aarhus Bike terminals in Aarhus, air and tools Nordic Denmark GTM no specific value Ongoing No Aarhus Municipality Cykelterminal - Dataset Citybike locations in Aarhus Nordic Denmark GTM no specific value no specific value No Aarhus Municipality Aarhus Bycykel - Dataset Fast track bikeroutes in Aarhus Nordic Denmark GTM no specific value Ongoing No Aarhus Municipality Supercykelsti i Aarhus Kommune - Dataset recreative bikeroutes in Aarhus Nordic Denmark GTM no specific value Ongoing No Aarhus Municipality Rekreative cykelruter - Dataset AirQuality South Italy UpTown no specific value 30s Yes Politecnico di Milano neslab.it Biodiversity South Italy UpTown no specific value 15m Yes Politecnico di Milano neslab.it Energy distribution South Italy UpTown no specific value Hourly Yes A2A neslab.it Archaeological Site South Italy Mithraeum of Circus Maximus no specific value Real-time Yes Politecnico di Milano neslab.it District Heating Data North Center Denmark Center Denmark Smart Data Models Daily Yes, 2 years Center Denmark portal.centerdenmark.com Water Data North Center Denmark Center Denmark Smart Data Models Daily Yes, 2 years Center Denmark portal.centerdenmark.com Electricity Data North Center Denmark Center Denmark Smart Data Models Daily Yes, 2 years Center Denmark portal.centerdenmark.com"},{"location":"data_catalog/instructions/","title":"How to add new datasets?","text":"<p>To add new datasets to the data catalog, an automated request system has been enabled through GitHub Issues and csv's files. </p> <p>Each of the datasets that you want to add to the data catalog must be attached as a different csv file. The template for this file can be found at the following link: Dataset Template.</p> <p></p>"},{"location":"data_catalog/instructions/#the-dataset-template","title":"The Dataset Template","text":"<p>The dataset template is a file in csv format that can be divided into two parts depending on its content. </p> <p>The first part collects general information about the dataset:</p> Description DATASET-NAME Dataset name. SUPER-NODE Name of the super node to which it belongs. TEF-NODE Name of the TEF node to which it belongs. SITE Name of the site to which it belongs. DATA-MODEL Link to the data model that complies. For example, the smart data model of firmware https://github.com/smart-data-models. SAMPLING-TIME Sampling time of the dataset (Realtime, X h/min/seg). HISTORICAL Historical data availability (From XXX/No). OWNER Owner of the dataset. GET-ACCESS Contact information to access the dataset. <p>The second part collects information about the fields (metadata) of the dataset:</p> Description ATTRIBUTE Variable name. TYPE Type of variable (Integer, Float, String, Boolean, etc.). UNITS (SI) Units of the variable in the International System. DESCRIPTION/COMMENTS Description/Explanation of the variable. <p>Warning</p> <ul> <li> <p>If you do not have information to complete a field, DO NOT delete it, LEAVE IT EMPTY.</p> </li> <li> <p>If a field is a web link, copy it in plain text, not as a hyperlink.</p> </li> </ul>"},{"location":"data_catalog/instructions/#example-a-filled-template","title":"Example: A filled template","text":"<p>The following image shows an example of a filled template:</p> <p></p>"},{"location":"data_catalog/instructions/#making-the-request-github-issues","title":"Making the request (GitHub Issues)","text":"<p>A GitHub account is required.</p> <p>To make the request to add new datasets to the data catalog, you must create an issue using the New dataset request template and follow the instructions.</p> <p>From the data catalog page and clicking on the <code>Add New Datasets</code> button you will directly access to adding a new issue.</p> <p></p> <p>In the Github Issues tab, click <code>Get started</code> in the New dataset request issue.</p> <p></p> <p>This will be the template that you must fill out.</p> <p></p>"},{"location":"data_catalog/instructions/#filling-the-issue","title":"Filling the issue","text":"<ol> <li>Add a Title: Only replace the <code>&lt;site_name&gt;</code> with the name of the site to which the dataset belongs (e.g. <code>Valencia</code>).</li> <li> <p>Attach files: Select the <code>&gt; Add files here.</code> line and insert the csv by clicking on the paper clip in the top bar. All the files you attach must appear one under the other within this Attached csv files section.</p> <p></p> </li> <li> <p>Comments: If you have any comments you can add them in the corresponding section.</p> </li> <li> <p>Submit new issue: Once everything is complete, publish the request.</p> <p></p> </li> </ol>"},{"location":"data_catalog/instructions/#reviewing-the-request","title":"Reviewing the request","text":"<p>Submitted the request, it will be processed automatically. Notifying in the same issue the start of the process and ending with the closing of the issue and the creation of a pull-request with the requested changes. </p> <p></p> <p>This pull request will be reviewed by the site administrators and, if everything is correct, it will be accepted and the information will be added to the data catalogue. If not, the user will be notified to make the necessary corrections.</p>"},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/","title":"Super Node: Nordic | TEF Node: CitCom","text":""},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/#site-gtm-bikeparking-stands-in-aarhus-city","title":"Site: GTM | Bikeparking stands in Aarhus City","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS ID number numerical identicator for each stand element txt the type of geographical element elementnavn txt name of the stand bemaerkninger txt comments beliggenhed txt adress og location oprettet_af internal ID person/system that created the item oprettet_dato date date of creation rettet_dato date person/system that corrected the item SP_geometry coordinates geograpical coordinates"},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/#site-gtm-bike-terminals-in-aarhus-air-and-tools","title":"Site: GTM | Bike terminals in Aarhus, air and tools","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS fid txt identicator for each route type txt type of terminal beskrivelse txt description adress txt adress by txt city komnr number municipality number oprettet_af internal ID person/system that created the item oprettet_dato date date of creation rettet_af internal ID person/system that corrected the item rettet_dato date date of correction MI_STYLE MI_PRINX number SP_geometry coordinates geograpical coordinates"},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/#site-gtm-citybike-locations-in-aarhus","title":"Site: GTM | Citybike locations in Aarhus","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS _id number identicator bid number place txt name og location alttext txt comments lat number degrees of latitude ing number degrees of longitude date date"},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/#site-gtm-fast-track-bikeroutes-in-aarhus","title":"Site: GTM | Fast track bikeroutes in Aarhus","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS fid txt identicator for each route navn_cykelrute txt Name of route komnr number municipality number navn_sted txt name of place bemaerkninger txt oprettet_af internal ID person/system that created the item oprettet_dato date date of creation rettet_af internal ID person/system that corrected the item rettet_dato date date of correction MI_STYLE MI_PRINX number SP_geometry coordinates geograpical coordinates"},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/#site-gtm-recreative-bikeroutes-in-aarhus","title":"Site: GTM | recreative bikeroutes in Aarhus","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS fid txt identicator for each route straekning number nummerical identicator for each route oprettet_af internal ID person/system that created the item oprettet_dato date date of creation rettet_dato date person/system that corrected the item MI_STYLE MI_PRINX SP_geometry coordinates geograpical coordinates"},{"location":"data_catalog/metadata_datasets/north_center%20denmark_center%20denmark/","title":"Super Node: North | TEF Node: Center Denmark","text":""},{"location":"data_catalog/metadata_datasets/north_center%20denmark_center%20denmark/#site-center-denmark-district-heating-data","title":"Site: Center Denmark | District Heating Data","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS id INTEGER Identifier timestamp DATETIME Date and time are shown in UTC in ISO 8601 format. Timestamps are represented in full hours, i.e., minutes and seconds are 00. location STRING Address \u2013 following DAR (Danish Address Register) energy DOUBLE MWh The heating consumption of the district heating meter water supply. energy_computed DOUBLE MWh Computed heat energy consumption based on the flow and temperature difference. Coefficients are determined for the water at 60 Degrees Celsius. flow DOUBLE m\u00b3/h The flow rate of the district heating meter water supply. forward_temperature DOUBLE \u00b0C The forward temperature of the district heating meter water supply. return_temperature DOUBLE \u00b0C Return temperature"},{"location":"data_catalog/metadata_datasets/north_center%20denmark_center%20denmark/#site-center-denmark-water-data","title":"Site: Center Denmark | Water Data","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS id INTEGER Identifier timestamp DATETIME Date and time are shown in UTC in ISO 8601 format. Timestamps are represented in full hours, i.e., minutes and seconds are 00. location STRING Adress - following DAR (Danish Adress Register) volumen DOUBLE M3 Unit for Volumen flow DOUBLE m\u00b3/h The flow rate of the water supply."},{"location":"data_catalog/metadata_datasets/north_center%20denmark_center%20denmark/#site-center-denmark-electricity-data","title":"Site: Center Denmark | Electricity Data","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS id INTEGER Identifier timestamp DATETIME Date and time are shown in UTC in ISO 8601 format. Timestamps are represented in full hours, i.e., minutes and seconds are 00. location STRING Adress - following DAR (Danish Adress Register) value DOUBLE kWh The actual value of the reading."},{"location":"data_catalog/metadata_datasets/south_italy_mithraeum-of-circus-maximus/","title":"Super Node: South | TEF Node: Italy","text":""},{"location":"data_catalog/metadata_datasets/south_italy_mithraeum-of-circus-maximus/#site-mithraeum-of-circus-maximus-archaeological-site","title":"Site: Mithraeum of Circus Maximus | Archaeological Site","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS id Text Sensor identifier temperature Number K Ambient temperature humidity Number % Relative humidity pm10 Number \u00b5g/m\u00b3 Particulate matter &lt; 10 \u00b5m concentration pm25 Number \u00b5g/m\u00b3 Particulate matter &lt; 2.5 \u00b5m concentration spectralPower Number mW/Hz Spectral power density of vibrations timestamp Number Unix timestamp of the measurement location geo:json Geographical coordinates of the sensor"},{"location":"data_catalog/metadata_datasets/south_italy_uptown/","title":"Super Node: South | TEF Node: Italy","text":""},{"location":"data_catalog/metadata_datasets/south_italy_uptown/#site-uptown-airquality","title":"Site: UpTown | AirQuality","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS id Text Sensor identifier temperature Number K Ambient temperature humidity Number % Relative humidity co Number mg/m\u00b3 Carbon monoxide concentration nh3 Number \u00b5g/m\u00b3 Ammonia concentration no Number \u00b5g/m\u00b3 Nitric oxide concentration no2 Number \u00b5g/m\u00b3 Nitrogen dioxide concentration o3 Number \u00b5g/m\u00b3 Ozone concentration pm25 Number \u00b5g/m\u00b3 Particulate matter &lt; 2.5 \u00b5m concentration pm10 Number \u00b5g/m\u00b3 Particulate matter &lt; 10 \u00b5m concentration so2 Number \u00b5g/m\u00b3 Sulfur dioxide concentration indoor Boolean True if the sensor is located indoors manufacturer Text Sensor manufacturer name sensorName Text Human-readable sensor name location geo:json Geographical coordinates of the sensor timestamp Number Unix timestamp of the measurement"},{"location":"data_catalog/metadata_datasets/south_italy_uptown/#site-uptown-biodiversity","title":"Site: UpTown | Biodiversity","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS id Text Sensor identifier timestamp Number Unix timestamp of the measurement device_hostname Text Device hostname firmware_version Text Current firmware version state Text Reported high-level state of the device activity_state Text Current activity state (e.g., active, idle) device_temperature Number \u00b0C Internal device temperature battery_voltage Number V Battery voltage battery_level Number % Battery charge level battery_power_input Text Raw battery input power data last_ping Number Timestamp of last device ping last_audio_record Number Timestamp of last audio recording last_detection_ts Number Timestamp of last detection event last_detection_scientific_name Text Scientific name of last detected species last_detection_common_name_en Text Common name (EN) of last detected species last_detection_common_name_it Text Common name (IT) of last detected species last_detection_confidence Number Confidence score of last detection"},{"location":"data_catalog/metadata_datasets/south_italy_uptown/#site-uptown-energy-distribution","title":"Site: UpTown | Energy distribution","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS id Text Sensor identifier timestamp Number Unix timestamp of the measurement user_identifier Text Anonymized unique identifier for the user pod_identifier Number Anonymized identifier of the heating POD unit pod_coordinates geo:json Geographical coordinates of the POD user_type Text Category or role of the user (e.g., residential, commercial) external_temperature Number \u00b0C Outdoor air temperature heating_temperature_delivery Number \u00b0C Temperature of the fluid delivered to the heating system heating_temperature_return Number \u00b0C Temperature of the fluid returning from the heating system heating_volume Number m\u00b3/h Volume flow rate of heating fluid heating_power Number kW Thermal power delivered for heating energy_consumption Number kWh Cumulative heating energy consumed"},{"location":"data_catalog/metadata_datasets/south_spain_valencia/","title":"Super Node: South | TEF Node: Spain","text":""},{"location":"data_catalog/metadata_datasets/south_spain_valencia/#site-valencia-waste-container","title":"Site: Valencia | Waste Container","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS location geo:json Geojson reference to the element. Can be Point, LineString, Polygon, MultiPoint, MultiLineString or MultiPolygon address string Civil address where the container is located. project string Project to which this entity belongs. category list Container category(s). storedWasteKind string Type of waste stored. depth number m Container depth height number m Container height. fillingLevel number Percentage of container filling in parts by 1. temeprature number \u00baC Temperature inside the container. c_factor number Multiplicative factor between [0 - 2] to adapt the measurement taken by the sensor with the visual % of filling that the client has of it."},{"location":"data_catalog/metadata_datasets/south_spain_valencia/#site-valencia-weather-forecast","title":"Site: Valencia | Weather Forecast","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS name string The name of where the weather forecast is located. location geo:json Geojson reference to the element. It will be a Point. dateIssued date-time ISO8601 UTC Date and time of issuance of the forecast in ISO8601 UTC format. validFrom date-time ISO8601 UTC Date and time of start of the validity period in ISO8601 UTC format validTo date-time ISO8601 UTC Date and time of end of the validity period in ISO8601 UTC format. temperatureSurface number \u00baC Surface temperature. windSpeed number m/s Wind speed at 10 meters high dewPoint2m number \u00baC Dew point at 2 meters high."},{"location":"documentation/","title":"Guides","text":"<p>Reports and deployment guides of different components.</p>"},{"location":"documentation/#data-space-components","title":"Data Space components","text":"<ul> <li> <p>:material-security:{ .lg .middle } Trust Frameworks</p> <p>A trust framework is a set of policies, principles, and mechanisms that establish and maintain trust among participants in a data space ecosystem. </p> <p>:octicons-arrow-right-24: Learn more</p> </li> <li> <p>:material-account-credit-card-outline:{ .lg .middle } Verifiable Credentials</p> <p>A Verifiable Credential is a digital document that proves something about its holder. It's like a digital passport, diploma, or any other credential, but with the added benefit of being tamper-evident and verifiable using cryptography. </p> <p>:octicons-arrow-right-24: Learn more</p> </li> <li> <p>:material-power-plug-outline:{ .lg .middle } Data Space Connector</p> <p>The Data Space Connectors section groups the guides of data space connector technology.</p> <p>:octicons-arrow-right-24: Learn more</p> </li> </ul>"},{"location":"documentation/#how-can-i-connect-different-data-platforms-or-data-spaces","title":"How can I connect different data platforms or data spaces?","text":"<ul> <li> <p>:material-graph-outline:{ .lg .middle } Data Federation</p> <p>The data federation section groups the guides to be able to communicate different brokers based on their technology.</p> <p>:octicons-arrow-right-24: Learn more</p> </li> </ul>"},{"location":"documentation/data_federation/","title":"Data Federation","text":"<p>A key aspect of data spaces is their interoperability, which essentially refers to the union of several independent data spaces under a common set of rules and protocols to enable data exchange. This federation allows data to be shared and accessed securely and efficiently among different organizations, while at the same time maintaining the sovereignty and control of the data by the entities that own them.</p> <p>The federation arises in data spaces due to the need for collaboration and large-scale data sharing in today's digital world. The challenges associated with managing large volumes of data, protecting privacy and security, and the need to extract value from data have led to the creation of federated data spaces. These spaces allow organizations to work together to leverage data more effectively, while respecting regulations and the rights of data owners.</p> Brokers communication NGSI-LD NGSI-v2 Custom NGSI-LD not tested Lepus | IoT Agent not tested NGSI-v2 Lepus | IoT Agent not in data spaces not tested Custom not tested not tested not tested"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/","title":"FIWARE IoT-Agent","text":"<p>Repository :simple-github:</p> <p>This section explains how to connect an NGSI-V2 broker with an NGSI-LD broker (Fiware Orion) through subscriptions to a FIWARE IoT-Agent. This connection between brokers arises as an alternative to the use of FIWARE Lepus due to the lack of testing and the experimental nature of this service.</p> <p></p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#simulation-environment","title":"Simulation Environment","text":"<p>An environment has been created with docker compose that deploys in an isolated way (on two independent networks) the architectures FIWARE NGSI-V2 and FIWARE NGSI-LD.</p> <p></p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#services","title":"Services","text":"<p>Grouping the services according to the network or networks they belong to, we have:</p> <ul> <li>Network NGSI-V2 <code>ngsiv2_network</code><ul> <li>IoT-Agent <code>ngsiv2.iotagent</code></li> <li>Mongo DB <code>ngsiv2.mongodb</code></li> <li>Orion-V2 <code>ngsiv2.orionv2</code></li> </ul> </li> <li>Network NGSI-LD <code>ngsild_network</code><ul> <li>IoT-Agent <code>ngsild.iotagent</code></li> <li>Mongo DB <code>ngsild.mongodb</code></li> <li>LD Context <code>ngsild.context</code></li> <li>Orion-LD <code>ngsild.orionld</code></li> <li>Quantum Leap <code>ngsild.quantumleap</code>: Fiware data persistence service</li> <li>CreateDB <code>ngsild.cratedb</code>: DB for Quantum Leap.</li> </ul> </li> <li>Both networks <code>ngsiv2_network</code> y <code>ngsild_network</code><ul> <li>Jupyter Server</li> <li>NGINX</li> </ul> </li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#before-continuing","title":"Before continuing...","text":""},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#services-checks","title":"Services checks","text":""},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#mongodb-connection","title":"MongoDB connection","text":"<p>A quick way to check that the environment is correctly deployed and the different services communicate with each other is to verify the different databases and collections through MongoDB Compass:</p> MongoDB Compass <ul> <li> <p>MongoDB-V2: localhost, port <code>27028</code></p> <ul> <li>IoT-Agent<ul> <li>[x] DB: <code>iotagent-json</code><ul> <li>[x] Colection: <code>groups</code></li> </ul> </li> </ul> </li> <li>Context Broker: The database will appear when it has some entity.</li> </ul> </li> <li> <p>MongoDB-LD: localhost, port <code>27027</code></p> <ul> <li>IoT-Agent<ul> <li>[x] DB: <code>iotagent-json</code><ul> <li>[x] Colection: <code>groups</code></li> </ul> </li> </ul> </li> <li>Context Broker<ul> <li>[x] DB: <code>orionld</code><ul> <li>[x] Colection: <code>contexts</code></li> </ul> </li> <li>[x] DB: <code>myOrion</code><ul> <li>[x] Colection <code>entities</code></li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#communication-with-the-iot-agents","title":"Communication with the IoT-Agents","text":"<p>By requesting the version from the IoT-Agents, it can be easily verified that they are correctly deployed and that they are visible among the different networks of the environment.</p> From <code>jupyter-server</code> <p>IoT-Agent NGSI-V2:</p> <pre><code>curl -X GET 'http://ngsiv2.iotagent:4041/iot/about' | jq .\n</code></pre> <p>IoT-Agent NGSI-LD:</p> <pre><code>curl -X GET 'http://ngsild.iotagent:4041/iot/about' | jq .\n</code></pre> From <code>fiware-orionv2</code> to the IoT-Agent LD <pre><code>curl -X GET 'http://ngsiv2.to-ngsild:4041/iot/about' | jq .\n</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#terminal-access-to-the-containers","title":"Terminal access to the containers","text":"<p>To access the containers from the terminal, the command <code>docker exec</code> can be used.</p> <pre><code>docker exec -it -u &lt;usuario&gt; &lt;nombre_contenedor&gt; &lt;sh/bash&gt;\n</code></pre> <p>Example</p> <p>=== \"<code>jupyter-server</code>\"</p> <pre><code>```bash\ndocker exec -it orion-jupyter bash\n```\n\n- Access (by default): `root`\n- Package installer: `apk`\n</code></pre> <p>=== \"<code>fiware-orionv2</code>\"</p> <pre><code>```bash\ndocker exec -it -u root fiware-orionv2 bash\n```\n\n- Access (by default): `nobody`\n- Package installer: `apt`\n</code></pre> <p>=== \"<code>fiware-orionld</code>\"</p> <pre><code>```bash\ndocker exec -it fiware-orionld bash\n```\n\n- Access (by default): `root`\n- Package installer: `yum`\n</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#steps-for-the-connection-between-brokers","title":"Steps for the connection between brokers","text":"<p>In theory, the procedure to connect two brokers through the IoT-Agent will be as follows:</p> <ol> <li> <p>Provisioning of a service group (IoT-Agent)</p> </li> <li> <p>Devices creation (IoT-Agent)</p> </li> <li> <p>Subscription (Context Broker)</p> </li> <li> <p>Modification of devices values (IoT-Agent)</p> </li> </ol> <pre><code>graph LR\n    fir(1\u00ba - Provisioning\\n of a service group) -.-&gt; A;\n    sec(2\u00ba - Provisioning\\n of a service group) -.-&gt; C;\n    thi(3\u00ba - Custom\\n subscription V2 to LD) -.-&gt; B;\n    four(4\u00ba - Modification\\n of devices values) -.-&gt; A;\n    subgraph NGSI-V2\n        A[IoT-Agent];\n        A --- B[Orion Broker];\n    end\n    subgraph NGSI-LD\n        B --&gt; C[IoT-Agent\\n];\n        C --- D[Orion Broker];\n    end\n</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#simulations","title":"Simulations","text":""},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#non-normalized-data-according-to-data-models","title":"Non-normalized data according to data-models","text":"<p>The objective of this example is to show the procedure to connect two brokers, setting aside the use of data-models as a normalizing element.</p> Property Value Type <code>example-type</code> fiware-service <code>vrainIoTA</code> <p>All the code shown is designed to be run from the <code>jupyter-server container</code></p> <pre><code>docker exec -it orion-jupyter bash\n</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#provisioning-of-a-group-of-services-iot-agent","title":"Provisioning of a group of services (IoT-Agent)","text":"<p>Creation of a service in the IoT-Agent with the attributes that all devices belonging to this service will have.</p> <p>Through services, sets of devices can be grouped together and the creation of these can be automated.</p> Editable fields (in both versions) <ul> <li>URL: http://ngsiv2.iotagent:4041/iot/services</li> <li>Headers:<ul> <li>fiware-service: vrainIoTA</li> <li>fiware-servicepath: /</li> </ul> </li> <li>apikey: vrain2gpepnvsb2uv4s40d59ov (value random, in principle the main requirement is to maintain the length and characters).</li> <li>cbroker: http://ngsiv2.orion:1026</li> <li>entity_type: example-type</li> <li>attributes: Modify according to the attributes of the devices. The structure of: <code>object_id</code>, <code>name</code> and <code>type</code> must be maintained..</li> </ul> <p>=== \"cURL-V2\"</p> <pre><code>```bash\ncurl -iX POST \\\n'http://ngsiv2.iotagent:4041/iot/services' \\\n-H 'Content-Type: application/json' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' \\\n-d '{\n    \"services\": [\n        {\n            \"apikey\":      \"vrain2gpepnvsb2uv4s40d59ov\",\n            \"cbroker\":     \"http://ngsiv2.orion:1026\",\n            \"entity_type\": \"example-type\",\n            \"resource\":    \"/iot/json\",\n            \"attributes\": [\n                {\n                    \"object_id\": \"t\",\n                    \"name\": \"temperature\",\n                    \"type\": \"Number\"\n                },\n                {\n                    \"object_id\": \"h\",\n                    \"name\": \"humidity\",\n                    \"type\": \"Number\"\n                },\n                {\n                    \"object_id\": \"f\",\n                    \"name\": \"fillingLevel\",\n                    \"type\": \"Number\"\n                }\n            ]\n        }\n    ]\n}'\n```\n</code></pre> <p>=== \"cURL-LD\"</p> <pre><code>```bash\ncurl -iX POST \\\n'http://ngsild.iotagent:4041/iot/services' \\\n-H 'Content-Type: application/json' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' \\\n-d '{\n    \"services\": [\n        {\n            \"apikey\":      \"vrainld2gpnvsb2uv4s40d59ov\",\n            \"cbroker\":     \"http://ngsild.orion:1026\",\n            \"entity_type\": \"example-type\",\n            \"resource\":    \"/iot/json\",\n            \"attributes\": [\n                {\n                    \"object_id\": \"t\",\n                    \"name\": \"temperature\",\n                    \"type\": \"Number\"\n                },\n                {\n                    \"object_id\": \"h\",\n                    \"name\": \"humidity\",\n                    \"type\": \"Number\"\n                },\n                {\n                    \"object_id\": \"f\",\n                    \"name\": \"fillingLevel\",\n                    \"type\": \"Number\"\n                }\n            ]\n        }\n    ]\n}'\n```\n</code></pre> If everything goes well, we will receive a response like this: <pre><code>HTTP/1.1 201 Created\nX-Powered-By: Express\nFiware-Correlator: c3e07e1c-62be-4ae4-8ee9-f112f585c077\nContent-Type: application/json; charset=utf-8\nContent-Length: 2\nETag: W/\"2-vyGp6PvFo4RvsFtPoIWeCReyIC8\"\nDate: Wed, 25 Oct 2023 11:39:01 GMT\nConnection: keep-alive\nKeep-Alive: timeout=5\n</code></pre> <p>Result in MongoDB</p> <p>It should appear within the <code>iotagent-json</code> database, a collection <code>groups</code> and within this as many documents as services have been created.</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#checks","title":"Checks","text":"<p>The service groups can be obtained with the following request:</p> <p>=== \"cURL-V2\"</p> <pre><code>```bash\ncurl -X GET 'http://ngsiv2.iotagent:4041/iot/services' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' | jq .\n```\n\n??? note \"If everything goes well, we will receive a response like this:\"\n\n    ```bash\n    {\n    \"count\": 1,\n    \"services\": [\n        {\n        \"commands\": [],\n        \"lazy\": [],\n        \"attributes\": [\n            {\n            \"object_id\": \"t\",\n            \"name\": \"temperature\",\n            \"type\": \"Number\"\n            },\n            {\n            \"object_id\": \"h\",\n            \"name\": \"humidity\",\n            \"type\": \"Number\"\n            },\n            {\n            \"object_id\": \"f\",\n            \"name\": \"fillingLevel\",\n            \"type\": \"Number\"\n            }\n        ],\n        \"_id\": \"653900b475ac594b1cc9e607\",\n        \"resource\": \"/iot/json\",\n        \"apikey\": \"vrain2gpepnvsb2uv4s40d59ov\",\n        \"service\": \"vrainiota\",\n        \"subservice\": \"/\",\n        \"__v\": 0,\n        \"static_attributes\": [],\n        \"internal_attributes\": [],\n        \"entity_type\": \"example-type\"\n        }\n    ]\n    }\n    ```\n</code></pre> <p>=== \"cURL-LD\"</p> <pre><code>```bash\ncurl -X GET 'http://ngsild.iotagent:4041/iot/services' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' | jq .\n```\n\n??? note \"If everything goes well, we will receive a response like this:\"\n\n    ```bash\n    {\n    \"count\": 1,\n    \"services\": [\n        {\n        \"commands\": [],\n        \"lazy\": [],\n        \"attributes\": [\n            {\n            \"object_id\": \"t\",\n            \"name\": \"temperature\",\n            \"type\": \"Number\"\n            },\n            {\n            \"object_id\": \"h\",\n            \"name\": \"humidity\",\n            \"type\": \"Number\"\n            },\n            {\n            \"object_id\": \"f\",\n            \"name\": \"fillingLevel\",\n            \"type\": \"Number\"\n            }\n        ],\n        \"_id\": \"6540cb26101b6eba30d4d653\",\n        \"resource\": \"/iot/json\",\n        \"apikey\": \"vrainld2gpnvsb2uv4s40d59ov\",\n        \"service\": \"vrainiota\",\n        \"subservice\": \"/\",\n        \"__v\": 0,\n        \"static_attributes\": [],\n        \"internal_attributes\": [],\n        \"entity_type\": \"example-type\"\n        }\n    ]\n    }\n    ```\n</code></pre> <p>Checklist - Before continuing...</p> <p>To continue correctly with the example, you must have:</p> <ul> <li>[x] IoT-Agent V2: Having a service created.<ul> <li>Take note (to use later) of the service's:<ul> <li>Attributes.</li> <li><code>apikey</code></li> </ul> </li> </ul> </li> <li>[x] IoT-Agent LD: Having a service created.<ul> <li>Take note (to use later) of the service's:<ul> <li>Attributes.</li> <li>`apikey</li> </ul> </li> </ul> </li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#aprovisionamiento-de-dispositivos-iot-agent","title":"Aprovisionamiento de dispositivos (IoT-Agent)","text":"<p>The creation of devices can be done in two ways:</p> <ul> <li>Manual: By explicitly creating a device.</li> <li>Automated: By using a service (previously created) and modifying the values of a device from the service.</li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#manual-method","title":"Manual Method","text":"<p>Info</p> <p>This section is not necessary for the correct operation of the example. It will only be necessary to know the automated method to use it after creating the subscription and verifying that it works.</p> <p>A specific device is added to the created service:</p> <p>=== \"cURL-V2\"</p> <pre><code>```bash\ncurl -iX POST \\\n'http://ngsiv2.iotagent:4041/iot/devices' \\\n-H 'Content-Type: application/json' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' \\\n-d '{\n    \"devices\": [\n        {\n            \"device_id\":   \"example001\",\n            \"entity_name\": \"example:example001\",\n            \"entity_type\": \"example-type\",\n            \"timezone\":    \"Europe/Madrid\"\n        }\n    ]\n}'\n```\n??? note \"If everything goes well, we will receive a response like this:\"\n\n    ```bash\n    HTTP/1.1 201 Created\n    X-Powered-By: Express\n    Fiware-Correlator: 421f4048-f56d-423c-8868-088ae96965bf\n    Content-Type: application/json; charset=utf-8\n    Content-Length: 2\n    ETag: W/\"2-vyGp6PvFo4RvsFtPoIWeCReyIC8\"\n    Date: Wed, 25 Oct 2023 12:10:10 GMT\n    Connection: keep-alive\n    Keep-Alive: timeout=5\n    ```\n</code></pre> <p>This process only adds a device in the IoT-Agent but does not modify the value of the attributes. To modify the device values, you must use the request of the automated method.</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#automated-method-through-services","title":"Automated method (through services)","text":"<p>This method fulfills two functions:</p> <ul> <li>Create a device (if it does not exist).</li> <li>Modify the values of its attributes.</li> </ul> URL Structure <p>Replace the values of: <code>&lt;iotagent_adress&gt;</code>, <code>&lt;apikey&gt;</code> y <code>&lt;device_id&gt;</code> for the corresponding values.</p> <pre><code>http://&lt;iotagent_adress&gt;:7896/iot/json?k=&lt;apikey&gt;&amp;i=&lt;device_id&gt;\n</code></pre> <p>Warning</p> <p>The port for this request changes from <code>4041</code> to <code>7896</code>.</p> <p>=== \"cURL-V2\"</p> <pre><code>```bash\ncurl -iX POST \\\n'http://ngsiv2.iotagent:7896/iot/json?k=vrain2gpepnvsb2uv4s40d59ov&amp;i=example001' \\\n-H 'Content-Type: application/json' \\\n-d '{\"t\": 1, \"h\":20, \"f\": 30}'\n```\n\n??? note \"If everything goes well, we will receive a response like this:\"\n\n    ```bash\n    HTTP/1.1 200 OK\n    X-Powered-By: Express\n    Content-Type: application/json; charset=utf-8\n    Content-Length: 2\n    ETag: W/\"2-vyGp6PvFo4RvsFtPoIWeCReyIC8\"\n    Date: Wed, 25 Oct 2023 12:15:01 GMT\n    Connection: keep-alive\n    Keep-Alive: timeout=5\n    ```\n</code></pre> <p>=== \"cURL-LD\"</p> <pre><code>!!! bug \"It does not work with versions higher than `2.3.0` of the IoT-Agent.\"\n\n```bash\ncurl -iX POST \\\n'http://ngsiv2.to-ngsild:7896/iot/json?k=vrainld2gpnvsb2uv4s40d59ov&amp;i=example001' \\\n-H 'Content-Type: application/json' \\\n-d '{\"t\": 1, \"h\":20, \"f\": 30}'\n```\n\n??? note \"If everything goes well, we will receive a response like this:\"\n\n    ```bash\n    HTTP/1.1 200 OK\n    Server: nginx/1.25.2\n    Date: Thu, 02 Nov 2023 11:35:07 GMT\n    Content-Type: application/json; charset=utf-8\n    Content-Length: 2\n    Connection: keep-alive\n    X-Powered-By: Express\n    ETag: W/\"2-vyGp6PvFo4RvsFtPoIWeCReyIC8\"\n    ```\n</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#checks_1","title":"Checks","text":"<p>To check the creation and modification of the values of a device, it can be done directly through MongoDB.</p> <p>Results in MongoDB</p> <p>=== \"cURL-V2\"</p> <pre><code>As a result of this operation, a new database `orion-&lt;fiware-service&gt;` (orion-vrainiota) is created in mongodb with a `entities` collection and within this a document per device.\n</code></pre> <p>=== \"cURL-LD\"</p> <pre><code>As a result of this operation, a new database `myOrion-&lt;fiware-service&gt;` (myOrion-vrainiota) is created in mongodb with a `entities` collection and within this a document per device.\n</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#subscription-of-orion-context-broker-v2-to-the-iot-agent-ld","title":"Subscription of Orion Context Broker V2 to the IoT-Agent LD","text":"<p>Finally, a custom subscription is created in the V2 broker, in this way, for each modification of the values of a device, the broker will send a POST request (just like the previous one) in an automated way.</p> <p>=== \"cURL (BrokerV2 -&gt; IoT-Agent LD)\"</p> <pre><code>!!! bug \"In the httpCustom:url, you cannot put addresses with `-`\"\n\n```bash\ncurl -iX POST \\\n'http://ngsiv2.orion:1026/v2/subscriptions' \\\n-H 'Content-Type: application/json' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' \\\n-d '{\n    \"description\": \"Reenv\u00edo datos entre brokers.\",\n    \"status\": \"active\",\n    \"subject\": {\n        \"entities\": [\n            {\n                \"idPattern\": \".*\",\n                \"type\": \"example-type\"\n            }\n        ],\n        \"condition\": {\n            \"attrs\": [],\n            \"notifyOnMetadataChange\": true\n        }\n    },\n    \"notification\": {\n        \"attrs\": [],\n        \"onlyChangedAttrs\": false,\n        \"attrsFormat\": \"normalized\",\n        \"httpCustom\": {\n            \"url\": \"http://ngsiv2.to-ngsild:7896/iot/json\",\n            \"method\": \"POST\",\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            },\n            \"qs\": {\n                \"i\": \"${id}\",\n                \"k\": \"vrainld2gpnvsb2uv4s40d59ov\"\n            },\n            \"json\": {\n                \"t\": \"${temperature}\",\n                \"h\": \"${humidity}\",\n                \"f\": \"${fillingLevel}\"\n            }\n        }\n    }\n}'\n```\n\n??? note \"If everything goes well, we will receive a response like this:\"\n\n    ```bash\n    HTTP/1.1 201 Created\n    Date: Thu, 02 Nov 2023 11:56:13 GMT\n    Fiware-Correlator: d244b742-7976-11ee-8fe6-0242ac130005\n    Location: /v2/subscriptions/65438e5d0a524d42d10bffca\n    Content-Length: 0\n    ```\n</code></pre> <p>Analyzing in detail some elements of this request we have:</p> <ul> <li>URL: Points to the subscriptions of the V2 broker.</li> <li>fiware-service: Necessary for the subscription to work.</li> <li>fiware-servicepath: Necessary for the subscription to work.</li> <li>notification<ul> <li>attrs: There is no need to specify attributes as they are later selected in the body of the request.</li> <li>httpCustom<ul> <li>url: Points to the IoT-Agent LD. Since the V2 broker network does not have direct visibility to the LD network, this address points to the reverse proxy between both networks.</li> <li>qs: <ul> <li>k: The <code>k</code> parameter refers to the <code>apiKey</code> of the service in IoT-Agent of LD.</li> </ul> </li> <li>json: This is the body of the request. It relates the <code>id</code> of the attributes in the IoT-Agent LD (<code>t</code>, <code>h</code> and <code>f</code>) with the attributes in the V2 broker (<code>${temperature}</code>, <code>${humidity}</code> and <code>${fillingLevel}</code>).</li> </ul> </li> </ul> </li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#checks_2","title":"Checks","text":"<p>To verify that the subscription is working, you simply need to modify the value of a device using the IoT-Agent or directly modify an attribute of an entity in the V2 broker. In this case, the first option is chosen through the IoT-Agent.</p> <p>Note how the request points to the address of the IoT-Agent in V2.</p> <pre><code>curl -iX POST \\\n'http://ngsiv2.iotagent:7896/iot/json?k=vrain2gpepnvsb2uv4s40d59ov&amp;i=example001' \\\n-H 'Content-Type: application/json' \\\n-d '{\"t\": 656, \"h\":87, \"f\": \"40\"}'\n</code></pre> If everything goes well, we will receive a response like this: <pre><code>HTTP/1.1 200 OK\nX-Powered-By: Express\nContent-Type: application/json; charset=utf-8\nContent-Length: 2\nETag: W/\"2-vyGp6PvFo4RvsFtPoIWeCReyIC8\"\nDate: Thu, 02 Nov 2023 12:31:00 GMT\nConnection: keep-alive\nKeep-Alive: timeout=5\n</code></pre> <p>After executing this request, we will observe how in MongoDB (both V2 and LD) the values of the device in question (<code>example001</code>) appear or are modified.</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#references","title":"References","text":"<p>FIWARE IoT-Agent - Github Each version (V2 or LD) is located in a different branch.</p> <p>FIWARE Docu. - API Custom Notificacions</p> <p>FIWARE Docu. - API httpCustom</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/lepus/","title":"Lepus","text":"<p>Repository :simple-github:</p> <p>FIWARE-Lepus is an NGSI-LD wrapper for use with NGSI-v2 Context Brokers. It understands the NGSI-LD endpoints and inputs, converts them to NGSI-v2, makes a request to the NGSI-v2 broker behind it and transforms responses back to NGSI-LD using a fixed JSON-LD @context. It supports the NGSI-LD federationOps endpoints only and is designed to be used as a registered source with NGSI-LD Context Brokers in federation mode.</p> <pre><code>graph LR\n  A[NGSI-v2 Context Broker] --- B[Lepus];\n  B ---|NGSI-LD| C[Client];\n</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/lepus/#getting-started","title":"Getting started","text":"<p>Warning</p> <p>Lepus is a library still in development. It is considered untested and experimental. Errors may occur.</p> <p>Create a <code>docker-compose.yml</code> file with the following config. Please, replace <code>NGSI_V2_CONTEXT_BROKER</code> with the URL / IP address of your NGSI-v2 Context Broker. Also set <code>CONTEXT_URL</code> to the desired context (for instance WasteManagement context). </p> <pre><code>version: '3.9'\nservices:\n  lepus:\n    image: quay.io/fiware/lepus\n    build:\n      context: .\n      dockerfile: Dockerfile\n    hostname: adapter\n    container_name: lepus\n    networks:\n      - default\n    expose:\n      - \"3000\"\n    ports:\n      - \"3000:3000\"\n    environment:\n      - DEBUG=adapter:*\n      - NGSI_V2_CONTEXT_BROKER=http://orion2:1026/v2\n      - CONTEXT_URL=https://fiware.github.io/tutorials.Step-by-Step/tutorials-context.jsonld\n      - NOTIFICATION_RELAY_URL=http://adapter:3000/notify\n      - NODE_TLS_REJECT_UNAUTHORIZED=0\nnetworks:\n  default:\n</code></pre> <p>Create and start the container by using the following command:</p> <pre><code> docker compose up\n</code></pre> <p>Set-up your AI service / client so it points to <code>127.0.0.1.1:3000</code>.</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/lepus/#debugging","title":"Debugging","text":"<p>As mentioned above, Lepus is still in its early stages, so it lacks some features, and wild bugs may appear. In order to make it easier to detect and report errors, a developing / debugging repository was created. If you find any issues or want to request a feature, please, open a new issue in the official repository.</p> <p>Steps to run the development version (it is necessary to have Node.js installed):</p> <ol> <li> <p>Clone the repository and navigate to its root folder: <pre><code>git clone https://github.com/CitComAI-Hub/lepus-dev &amp;&amp; cd lepus-dev\n</code></pre></p> </li> <li> <p>Install dependencies <pre><code>npm install\n</code></pre></p> </li> <li> <p>Run <pre><code>npm start\n</code></pre></p> </li> </ol>"},{"location":"documentation/data_federation/ngsiv2_to_ld/lepus/#track-and-status-of-detected-errors","title":"Track and status of detected errors","text":"<ul> <li>[x] Missing <code>NGSILD-Tenant header</code> (equivalent to <code>Fiware-Service</code>)   </li> <li>[x] Missing <code>scopeQ parameter</code> (equivalent to <code>Fiware-ServicePath</code>)  </li> <li>[x] Default response timeout too low for the VLCi platform   <ul> <li>Use <code>NGSI_V2_TIMEOUT</code> environment variable to set a custom value (1000ms by default)</li> </ul> </li> <li>[x] Missing @context property when requesting entities </li> <li>[ ] HTTPS / SSL problems  <ul> <li>Lepus' protocol is http. It should be fine for working locally and testing purposes</li> <li><code>NODE_TLS_REJECT_UNAUTHORIZED=0</code> must be set to avoid SSL certificate checking problem</li> </ul> </li> <li>[ ] Static context</li> </ul>"},{"location":"documentation/data_space_connectors/","title":"Connectors","text":"<p>A data space connector is a technical component that enables the secure and controlled exchange of data between organisations within a data space. It functions as a gateway that manages communication, authentication, and data usage policies. Each participant deploys their own connector, ensuring sovereignty over their data. Furthermore, it promotes interoperability by adhering to standards such as those set by IDSA or Gaia-X.</p> <p>The most extended data space connector technology are:</p> <ul> <li> <p>:material-power-plug-outline:{ .lg .middle } Fiware</p> <p>The FIWARE Data Space Connector is an integrated suite of components implementing DSBA Technical Convergence recommendations.</p> <p>:octicons-arrow-right-24: Official Documentation</p> <p>:octicons-arrow-right-24: Learn more</p> </li> <li> <p>:material-power-plug-outline:{ .lg .middle } Eclipse</p> <p>Eclipse data space connector is a framework that provides a set of components and APIs for building data space connectors. It is designed to be modular and extensible, allowing developers to create custom connectors that meet their specific needs.</p> <p>:octicons-arrow-right-24: Official Documentation</p> <p>:octicons-arrow-right-24: Learn more</p> </li> <li> <p>:material-power-plug-outline:{ .lg .middle } Simpl</p> <p>Simpl programme is an initiative that aims to create a data space connector based on the Eclipse Data Space Components. It is still in its early stages of development, but it aims to leverage the foundational strengths of Eclipse to provide a final product solution within the data space connector landscape in the EU.</p> <p>:octicons-arrow-right-24: Official Site</p> Official References <ul> <li>Installation Guide</li> <li>Functional and Technical Architecture Specifications</li> <li>User Manual </li> </ul> </li> </ul>"},{"location":"documentation/data_space_connectors/eclipse/","title":"Eclipse Connector","text":""},{"location":"documentation/data_space_connectors/eclipse/#introduction","title":"Introduction","text":"<p>The Eclipse Dataspace Components (EDC) provide a framework for sovereign, inter-organizational data sharing. They implement the IDS Dataspace Protocol (DSP) as well as relevant protocols associated with GAIA-X. The EDC are designed in an extensible way in order to support alternative protocols and integrate in various ecosystems. </p> <p>It is important to note that EDC is not a prepackaged system or application and, as such, does not ship an installable distribution\u2014downstream projects, such as Simpl, take EDC as a foundational component and customize it to meet their specific requirements. Moreover, EDC does not supply the underlying infrastructure for storing, processing, or moving data; instead, it seamlessly integrates with third-party data planes that deliver these essential services.</p> <p>Therefore, EDC needs further downstream work to adapt and make it work in the CitCom.ai context. </p>"},{"location":"documentation/data_space_connectors/eclipse/#key-points","title":"Key points","text":"<ul> <li>The working group formed by IDSA, Gaia-X, and Eclipse is quite solid (+\u00a0Simpl). It seems to be the EU's direction.\u200b</li> <li>Agnostic </li> <li>Push and pull data transfer mechanism. It also supports streaming data.</li> <li>EDC is a framework.\u00a0It needs further work to deploy it. EDC development is very active.</li> </ul>"},{"location":"documentation/data_space_connectors/eclipse/#getting-started","title":"Getting started","text":"<p>If you\u2019re eager to put EDC to the test and see how it functions in real-world scenarios, the next step is to dive into the practical examples available. We recommend starting with the comprehensive samples, which are designed to give you immediate insights and hands-on experience.</p> <p>Go to samples repository and take a high-level look at the available sample scopes. The repository is organized by topics such as basic setup, data transfer, advanced integrations, and policy enforcement. If you\u2019re new to EDC, start with the \u201cbasic\u201d samples to learn how to set up and run a connector. Once you have a handle on the basics, consider exploring other scopes (e.g., \u201ctransfer\u201d or \u201cpolicy\u201d) to delve deeper into specific aspects of the framework.</p>"},{"location":"documentation/data_space_connectors/eclipse/#experiments","title":"Experiments","text":"<p>Once you successfully run samples, consider modifying configuration files or sample code to see how changes affect the behavior. This hands-on approach will help cement your understanding of how EDC can be extended to meet your organization\u2019s needs. For example, by modifying the transfer-03 example it is possible to perform a quick and dirty experiment in order to connect your current TEF data platform to the connector. Take a look at the Valencia TEF setup.</p> <p></p> <p>Using a middleware makes it possible to connect the Eclipse connector to the current data platform. The middleware, an API rest server built with Flask, handles authentication and redirects all data requests to the data platform (via Lepus to translate NGSIv2 to NGSI-LD).</p> <p>=== \"middleware.py\"     <pre><code>import os\nfrom lib.ngsildclient.Client import Client\nfrom lib.ngsildclient.Authv2 import Authv2\n\n# Ngsi-ld broker client\nclient = Client()\n\n# Init flask server\nfrom flask import Flask, request\nfrom flask_cors import CORS\n\napp = Flask(__name__)\nCORS(app)\n\n\n# API REST\n@app.route(\"/wastecontainers/\")\ndef waste_containers():\n    containers = get_all_WasteContainers()\n    return containers\n\n\n# Data functions\ndef get_all_WasteContainers():\n    # Define service &amp; subservice\n    service = \"tef_vlci\"\n    subservice = \"/residuos_contenedores_vlc\"\n\n    # Authenticate\n    auth = Authv2()\n    token = auth.get_auth_token_subservice(service, subservice)\n\n    # Fetch all WasteContainer entities\n    context = os.environ.get(\"WASTECONTAINERS_CONTEXT\")\n    containers = client.get_all_entities_by_type(\"WasteContainer\", context, 100, 0, service, subservice, token).json()\n    return containers\n</code></pre></p> <p>=== \"create-asset.json\"     <pre><code>{\n    \"@context\": {\n        \"@vocab\": \"https://w3id.org/edc/v0.0.1/ns/\"\n    },\n    \"@id\": \"assetId\",\n    \"properties\": {\n        \"name\": \"product description\",\n        \"contenttype\": \"application/json\"\n    },\n    \"dataAddress\": {\n        \"type\": \"HttpData\",\n        \"name\": \"WasteContainers\",\n        \"baseUrl\": \"http://127.0.0.1:5000/wastecontainers\",\n        \"proxyPath\": \"true\"\n    }\n}\n</code></pre></p>"},{"location":"documentation/data_space_connectors/fiware/","title":"FIWARE Data Space Connector","text":""},{"location":"documentation/data_space_connectors/fiware/#overview","title":"Overview","text":"<p>The FIWARE Data Space Connector (FDSC) is an integrated suite of components every organization participating in a data space should deploy to connect to a data space. Following the DSBA recommendations, it allows to:</p> <ul> <li>Interface with Trust Services aligned with EBSI specifications.</li> <li>Implement authentication based on W3C DID with VC/VP standards and SIOPv2/OIDC4VP protocols.</li> <li>Implement authorization based on attribute-based access control (ABAC) following an XACML P*P architecture using Open Digital Rights Language (ODRL) and the Open Policy Agent (OPA).</li> <li>Provide compatibility with ETSI NGSI-LD as data exchange API.</li> <li>Supports the TMForum APIs for contract negotiation.</li> </ul> <p>Note</p> <p>Although the FIWARE Data Space Connector provides compatibility with NGSI-LD as the data exchange API, it could also be used for any other RESTful API by replacing or extending the PDP component of the connector.</p> Key points <ul> <li>Final and ready-to-use software (versus the framework approach of Eclipse).</li> <li>(Partial support for) IDS Dataspace Protocol (DSP).</li> <li>Not as agnostic as Eclipse, although its modular approach makes it possible (in theory) to extend its capabilities.</li> <li>It is not very tested; expect bugs and error reporting work.</li> <li>Development is relatively slow.</li> </ul>"},{"location":"documentation/data_space_connectors/fiware/#getting-started","title":"Getting started","text":"<p>A good way to start working with the connector is to deploy a Minimum Viable Data Space (MVDS) using FIWARE's minimum infrastructure. This infrastructure provides a minimal implementation of a data space using Fiware technology, which allows test the FIWARE Data Space Connector and its components in a local environment.</p> <p></p> <p>This MVDS is composed of the following blocks:</p> Component Description Fiware Data Space Operator or Trust Anchor The entity responsible for managing the issuers and credentials within the data space. It ensures the trustworthiness of the data space by managing the identities and credentials of participants. FDS Connector A (Provider) An entity that provides data from the data space. It acts as a data provider, allowing for data exchange within the data space. FDS Connector B (Consumer) An entity that consumes data from the data space. It acts as a data consumer, retrieving data from the data space without providing any data in return. <p>Example</p> <ul> <li>FIWARE MVDS local example: Code repository.</li> <li>CitcomAI MVDS local example: Code repository.</li> </ul>"},{"location":"documentation/data_space_connectors/fiware/#technical-details-deployments","title":"Technical Details &amp; Deployments","text":"<p>The FIWARE Data Space Connector repository provides a Helm chart for deploying the connector in a Kubernetes cluster. The chart includes all the necessary components to set up a data space connector in both consumer and provider modes. The chart is designed to be flexible and can be customized to fit the specific needs of the data space.</p>"},{"location":"documentation/data_space_connectors/fiware/#consumer","title":"Consumer","text":"<p>The consumer mode of the FIWARE Data Space Connector is composed of the following components:</p> <p></p> <p>Deployments</p> <ul> <li>Minimum AWS deployment example: Code</li> </ul> Component Functionality Description DID (did-helper) Config Services A component that provides support for W3C Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs). It helps in managing DIDs and VCs within the data space. Keycloak Authentication An identity and access management solution that provides authentication and authorization services. It is used to manage user identities and access to resources within the data space. Rainbow IDSA Data Space Protocol Rainbow or also known as Dataspace Rainbow is an implementation of Dataspace Protocol 2024-1 promoted by IDSA (International Data Spaces Association). PostgreSQL Database A relational database management system that stores data related to the data space."},{"location":"documentation/data_space_connectors/fiware/#provider","title":"Provider","text":"<p>The provider mode of the FIWARE Data Space Connector is composed of the following components:</p> <p></p> <p>Deployments</p> <ul> <li>Minimum AWS deployment example: Code</li> </ul> Component Functionality Description APISIX Authorization A component that provides API gateway functionality with a OPA plugin for traffic management. OPA Authorization An open-source policy engine that provides attribute-based access control (ABAC) for the data space. It evaluates policies and makes authorization decisions based on attributes and rules defined in the data space. ODRL-PAP Authorization A component that implements the ODRL (Open Digital Rights Language) Policy Administration Point (PAP) for managing data access policies within the data space. Scopio Data Broker A data broker, facilitating the exchange of data between different participants in the data space. It manages data discovery and retrieval processes. VCVerifier Authentication A component that verifies the authenticity of Verifiable Credentials (VCs) and exchanges them for tokens. It ensures that the credentials presented by participants are valid and trustworthy. Credential Config Service Authentication A service that manages the configuration of credentials. Holds the information which VCs are required for accessing a service. Trusted Issuers List Authentication A list of trusted issuers for the provider. Acts as Trusted Issuers List by providing an EBSI Trusted Issuers Registry API. TM Forum API Data Discovery A component that implements the TM Forum APIs for contract negotiation within the data space. It allows participants to negotiate and manage contracts related to data exchange. Contract Management Data Discovery Notification listener for contract management events out of TMForum. Rainbow IDSA Data Space Protocol Rainbow or also known as Dataspace Rainbow is an implementation of Dataspace Protocol 2024-1 promoted by IDSA (International Data Spaces Association). TPP IDSA Data Space Protocol Integration of checks for the transfer process protocol. PostgreSQL Database A relational database management system that stores data related to the data space. PostGIS Data Bases PostgreSQL Database with PostGIS extensions MySQL Data Bases An open-source relational database management system that uses SQL for data management. DID (did-helper) Config Services A component that provides support for W3C Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs). It helps in managing DIDs and VCs within the data space."},{"location":"documentation/mv_data_space/","title":"MV Data Space on AWS Cloud","text":""},{"location":"documentation/mv_data_space/#overview","title":"Overview","text":"<p>This guide provides step-by-step instructions for deploying a Minimum Viable Data Space (MVDS) on AWS with three distinct roles using Fiware's Technology (Trust Framework, FDS Connector). Each role is completely independent and can be deployed separately:</p> <ul> <li>Trust Anchor: It manages the identities and credentials of participants in the data space, ensuring trustworthiness and security. Only one instance of this role is needed in the data space.</li> <li>Consumer: Requests and consumes data/services from providers. Each participant needs its own consumer instance.</li> <li>Provider: Offers data/services to consumers. Each participant needs its own provider instance.</li> </ul> <p>Prerequisites</p> <p>Before starting any deployment, ensure you have:</p> <ul> <li>[x] AWS CLI installed and configured with appropriate credentials.</li> <li>[x] <code>kubectl</code> installed on your system (Installation Guide).</li> <li>[x] <code>helm</code> installed on your system (Installation Guide).</li> <li>[x] Basic understanding of Kubernetes and AWS EC2.</li> </ul>"},{"location":"documentation/mv_data_space/#common-setup-steps","title":"Common Setup Steps","text":""},{"location":"documentation/mv_data_space/#get-your-public-ip","title":"Get Your Public IP","text":"<p>First, determine your current public IP address for security group configuration:</p> <pre><code>curl -s https://checkip.amazonaws.com\n</code></pre> <p>Note this IP address - you'll need it for security group configuration in each role.</p>"},{"location":"documentation/mv_data_space/#create-ssh-key-pair","title":"Create SSH Key Pair","text":"<p>In addition, we will also need an SSH key to access EC2 instances. Create an SSH key pair that will be used across all deployments:</p> <pre><code>aws ec2 create-key-pair \\\n  --key-name dataspace-key \\\n  --query 'KeyMaterial' \\\n  --output text &gt; dataspace-key.pem\n\nchmod 400 dataspace-key.pem\n</code></pre>"},{"location":"documentation/mv_data_space/#clone-deployment-repository","title":"Clone deployment repository","text":"<pre><code># Clone\ngit clone https://github.com/wistefan/deployment-demo.git\ncd deployment-demo\n\n# Open with your preferred editor. For intance:\ncode .\n</code></pre>"},{"location":"documentation/mv_data_space/#components-deployment","title":"Components Deployment","text":"<p>Below you'll find deployment instructions for each component in our Minimum Viable Data Space. Choose the component you need to deploy based on your role in the data space. Remember that a complete data space requires at least one Trust Anchor and at least one pair of Provider and Consumer. Follow the links for detailed deployment instructions specific to each component.</p> <ul> <li> <p>:material-account:{ .lg .middle } Consumer</p> <p>Fiware Data Space Connector (Consumer role) that is configured to access and retrieve data/services from the data space.</p> <p>:octicons-arrow-right-24: AWS</p> <p>:octicons-arrow-right-24: Technical Details</p> </li> <li> <p>:material-factory:{ .lg .middle } Provider</p> <p>Fiware Data Space Connector (Provider role) that is configured to share data/services with the data space.</p> <p>:octicons-arrow-right-24: AWS</p> <p>:octicons-arrow-right-24: Technical Details</p> </li> <li> <p>:material-security:{ .lg .middle } Trust Anchor</p> <p>Warning</p> <p>It is not necessary to deploy this if you want to connect to an existing Data Space.</p> <p>It serves as a trusted entity that issues and manages digital certificates (Verifiable Credentials) for organizations and individuals participating in the data space.</p> <p>:octicons-arrow-right-24: AWS</p> <p>:octicons-arrow-right-24: Technical Details</p> </li> </ul>"},{"location":"documentation/mv_data_space/#per-role-cleanup","title":"Per-Role Cleanup","text":"Trust Anchor <pre><code>export TRUST_ANCHOR_INSTANCE_ID=\"i-xxxxxxxxx\"\nexport TRUST_ANCHOR_ALLOCATION_ID=\"eipalloc-xxxxxxxxx\"\n\nhelm uninstall trust-anchor\naws ec2 terminate-instances --instance-ids $TRUST_ANCHOR_INSTANCE_ID --region $AWS_REGION\naws ec2 release-address --allocation-id $TRUST_ANCHOR_ALLOCATION_ID --region $AWS_REGION\naws ec2 delete-security-group --group-name trust-anchor-sg --region $AWS_REGION\n</code></pre> Consumer <pre><code>export CONSUMER_INSTANCE_ID=\"i-xxxxxxxxx\"\nexport CONSUMER_ALLOCATION_ID=\"eipalloc-xxxxxxxxx\"\n\nhelm uninstall consumer-dsc -n consumer\nkubectl delete namespace consumer\naws ec2 terminate-instances --instance-ids $CONSUMER_INSTANCE_ID --region $AWS_REGION\naws ec2 release-address --allocation-id $CONSUMER_ALLOCATION_ID --region $AWS_REGION\naws ec2 delete-security-group --group-name consumer-sg --region $AWS_REGION\n</code></pre> Provider <pre><code>export PROVIDER_INSTANCE_ID=\"i-xxxxxxxxx\"\nexport PROVIDER_ALLOCATION_ID=\"eipalloc-xxxxxxxxx\"\n\nhelm uninstall provider-dsc -n provider\nkubectl delete namespace provider\naws ec2 terminate-instances --instance-ids $PROVIDER_INSTANCE_ID --region $AWS_REGION\naws ec2 release-address --allocation-id $PROVIDER_ALLOCATION_ID --region $AWS_REGION\naws ec2 delete-security-group --group-name provider-sg --region $AWS_REGION\n</code></pre>"},{"location":"documentation/mv_data_space/#background-information","title":"Background Information","text":""},{"location":"documentation/mv_data_space/#nipio-service","title":"nip.io Service","text":"<p>nip.io is a free wildcard DNS service that converts subdomains like <code>service.1.2.3.4.nip.io</code> into A records pointing to <code>1.2.3.4</code>. This eliminates the need for custom DNS configuration during development.</p>"},{"location":"documentation/mv_data_space/#architecture","title":"Architecture","text":"<p>Each role runs on its own EC2 instance with a dedicated k3s Kubernetes cluster. The Ingress Controller in each cluster routes traffic based on hostnames to the appropriate internal services.</p>"},{"location":"documentation/mv_data_space/fiware/consumer/","title":"FDSC Consumer","text":"<p>Warning</p> <p>Check the prerequisites section before proceeding with the deployment.</p>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-by-step-aws-deployment","title":"Step by Step AWS deployment","text":"<p>The Consumer role allows you to request and consume data/services from providers in the data space.</p>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-1-create-security-group","title":"Step 1: Create Security Group","text":"<p>Create a dedicated security group for the Consumer:</p> <pre><code># Set your configuration\nexport YOUR_PUBLIC_IP=\"YOUR_IP_HERE\"  # Replace with your public IP\nexport AWS_REGION=\"eu-west-1\"         # Replace with your preferred region\n\n# Create security group\naws ec2 create-security-group \\\n  --group-name consumer-sg \\\n  --description \"Security group for Consumer\" \\\n  --region $AWS_REGION\n\n# Add SSH access from your IP\naws ec2 authorize-security-group-ingress \\\n  --group-name consumer-sg \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr ${YOUR_PUBLIC_IP}/32 \\\n  --region $AWS_REGION\n\n# Add Kubernetes API access from your IP\naws ec2 authorize-security-group-ingress \\\n  --group-name consumer-sg \\\n  --protocol tcp \\\n  --port 6443 \\\n  --cidr ${YOUR_PUBLIC_IP}/32 \\\n  --region $AWS_REGION\n\n# Add HTTP/HTTPS access (public)\naws ec2 authorize-security-group-ingress \\\n  --group-name consumer-sg \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0 \\\n  --region $AWS_REGION\n</code></pre> <p>Important</p> <p>Note the security group ID returned by the create command.</p>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-2-launch-consumer-instance","title":"Step 2: Launch Consumer Instance","text":"<p>For the Consumer instance we use Ubuntu 22.04 LTS image (<code>ami-0694d931cee176e7d</code>) and <code>t3.large</code> instance type. Feel free to change these parameters, especially if you see that the load to be supported is greater than the capacity of the virtual machine.</p> <pre><code># Replace with your security group ID\nexport CONSUMER_SG_ID=\"sg-xxxxxxxxx\"\n\n# Launch Consumer instance\naws ec2 run-instances \\\n  --image-id ami-0694d931cee176e7d \\\n  --instance-type t3.large \\\n  --key-name dataspace-key \\\n  --security-group-ids $CONSUMER_SG_ID \\\n  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=consumer}]' \\\n  --region $AWS_REGION\n</code></pre> <p>Important</p> <p>Note the instance ID returned by this command.</p>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-3-assign-elastic-ip","title":"Step 3: Assign Elastic IP","text":"<pre><code># Replace with your Consumer instance ID\nexport CONSUMER_INSTANCE_ID=\"i-xxxxxxxxx\"\n\n# Allocate Elastic IP\naws ec2 allocate-address \\\n  --domain vpc \\\n  --tag-specifications 'ResourceType=elastic-ip,Tags=[{Key=Name,Value=consumer-ip}]' \\\n  --region $AWS_REGION\n\n# Associate IP to instance (replace ALLOCATION_ID with the one returned above)\naws ec2 associate-address \\\n  --instance-id $CONSUMER_INSTANCE_ID \\\n  --allocation-id ALLOCATION_ID_FROM_ABOVE \\\n  --region $AWS_REGION\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-4-verify-instance-status","title":"Step 4: Verify Instance Status","text":"<pre><code>aws ec2 describe-instances \\\n  --instance-ids $CONSUMER_INSTANCE_ID \\\n  --query 'Reservations[*].Instances[*].[Tags[?Key==`Name`].Value | [0], PublicIpAddress, State.Name]' \\\n  --output table \\\n  --region $AWS_REGION\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-5-install-k3s","title":"Step 5: Install k3s","text":"<pre><code># Replace with your Consumer public IP\nexport CONSUMER_IP=\"YOUR_CONSUMER_IP\"\n\n# Connect to the instance\nssh -i \"dataspace-key.pem\" ubuntu@$CONSUMER_IP\n\n# Install k3s\ncurl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--tls-san $CONSUMER_IP\" sh -\n\n# Get the kubeconfig\nsudo cat /etc/rancher/k3s/k3s.yaml\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-6-configure-local-access","title":"Step 6: Configure Local Access","text":"<p>On your local machine, create a kubeconfig file for the Consumer:</p> <pre><code># Create k3s-consumer.yaml with the content from the previous step\n# Replace 127.0.0.1 with your Consumer IP in the server field\n# The file should contain:\n# server: https://YOUR_CONSUMER_IP:6443\n\n# Test the connection\nexport KUBECONFIG=k3s-consumer.yaml\nkubectl get nodes\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-7-configure-storage","title":"Step 7: Configure Storage","text":"<pre><code># Enable storage provisioner\nkubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml\n\n# Wait a few seconds for it to start. You can check its status with\nkubectl get pods -n local-path-storage\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-8-create-namespace","title":"Step 8: Create namespace","text":"<pre><code>kubectl create namespace consumer\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-9-create-consumer-identity","title":"Step 9: Create Consumer Identity","text":"<pre><code># Create directory for identity files\nmkdir consumer-identity\n\n# Generate the private key - dont get confused about the curve, openssl uses the name `prime256v1` for `secp256r1`(as defined by P-256)\nopenssl ecparam -name prime256v1 -genkey -noout -out consumer-identity/private-key.pem\n\n# Generate corresponding public key\nopenssl ec -in consumer-identity/private-key.pem -pubout -out consumer-identity/public-key.pem\n\n# Create a (self-signed) certificate\nopenssl req -new -x509 -key consumer-identity/private-key.pem -out consumer-identity/cert.pem -days 360\n\n# Export the keystore\nopenssl pkcs12 -export -inkey consumer-identity/private-key.pem -in consumer-identity/cert.pem -out consumer-identity/cert.pfx -name didPrivateKey\n\n# Check the contents\nkeytool -v -keystore consumer-identity/cert.pfx -list -alias didPrivateKey\n\n# Generate did from the keystore\nwget https://github.com/wistefan/did-helper/releases/download/0.1.1/did-helper\nchmod +x did-helper\n./did-helper -keystorePath ./consumer-identity/cert.pfx -keystorePassword=test\n</code></pre> <p>Important</p> <p>Note the DID returned by the <code>did-helper</code>. It is the consumer DID.</p>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-10-deploy-identity-secret","title":"Step 10: Deploy Identity Secret","text":"<pre><code># Create secret with the identity\nkubectl create secret generic consumer-identity --from-file=consumer-identity/cert.pfx -n consumer\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-11-configure-values","title":"Step 11: Configure Values","text":"<p>Danger</p> <p>Before deploying, you must modify the Consumer's <code>values.yaml</code> file to use your actual IP address instead of <code>127.0.0.1.nip.io</code>. Modify <code>consumer/values.yaml</code>:</p> <pre><code># 1. Replace the localhost address for the Keycloak ingress hostname:\nkeycloak:\n  ingress:\n    enabled: true\n    hostname: keycloak-consumer.YOUR_CONSUMER_IP.nip.io\n\n\n# 2. Replace the localhost also for KC_HOSTNAME in extraVars:\n- name: KC_HOSTNAME\n    value: keycloak-consumer.YOUR_CONSUMER_IP.nip.io\n\n# 3. In realm, replace:\nrealm:\nfrontendUrl: http://keycloak-consumer.127.0.0.1.nip.io:8080\n\n# with\nrealm:\nfrontendUrl: http://keycloak-consumer.YOUR_CONSUMER_IP.nip.io\n\n# 4. Replace DID with you own, previously generated consumer DID.\n- name: DID\n    value: \"did:key:xxxxxxxxxx\"\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-12-add-helm-repository","title":"Step 12: Add Helm Repository","text":"<pre><code>helm repo add data-space-connector https://fiware.github.io/data-space-connector/\nhelm repo update\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/consumer/#step-13-deploy-consumer","title":"Step 13: Deploy Consumer","text":"<pre><code># Deploy using your modified values file\nhelm install consumer-dsc data-space-connector/data-space-connector --version 7.17.0 -f consumer/values.yaml --namespace=consumer\n\n# Monitor deployment\nwatch kubectl get pods -n consumer\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/","title":"FDSC Provider","text":"<p>Warning</p> <p>Check the prerequisites section before proceeding with the deployment.</p>"},{"location":"documentation/mv_data_space/fiware/provider/#step-by-step-aws-deployment","title":"Step by Step AWS deployment","text":"<p>The Provider role allows you to offer data/services to consumers in the data space.</p>"},{"location":"documentation/mv_data_space/fiware/provider/#step-1-create-security-group","title":"Step 1: Create Security Group","text":"<p>Create a dedicated security group for the Provider:</p> <pre><code># Set your configuration\nexport YOUR_PUBLIC_IP=\"YOUR_IP_HERE\"  # Replace with your public IP\nexport AWS_REGION=\"eu-west-1\"         # Replace with your preferred region\n\n# Create security group\naws ec2 create-security-group \\\n  --group-name provider-sg \\\n  --description \"Security group for Provider\" \\\n  --region $AWS_REGION\n\n# Add SSH access from your IP\naws ec2 authorize-security-group-ingress \\\n  --group-name provider-sg \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr ${YOUR_PUBLIC_IP}/32 \\\n  --region $AWS_REGION\n\n# Add Kubernetes API access from your IP\naws ec2 authorize-security-group-ingress \\\n  --group-name provider-sg \\\n  --protocol tcp \\\n  --port 6443 \\\n  --cidr ${YOUR_PUBLIC_IP}/32 \\\n  --region $AWS_REGION\n\n# Add HTTP/HTTPS access (public)\naws ec2 authorize-security-group-ingress \\\n  --group-name provider-sg \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0 \\\n  --region $AWS_REGION\n</code></pre> <p>Important</p> <p>Note the security group ID returned by the create command.</p>"},{"location":"documentation/mv_data_space/fiware/provider/#step-2-launch-provider-instance","title":"Step 2: Launch Provider Instance","text":"<p>For the Trust Anchor instance we use Ubuntu 22.04 LTS image (<code>ami-0694d931cee176e7d</code>) and <code>t3.xlarge</code> instance type.  Feel free to change these parameters, especially if you see that the load to be supported is greater than the capacity of the virtual machine.</p> <pre><code># Replace with your security group ID\nexport PROVIDER_SG_ID=\"sg-xxxxxxxxx\"\n\n# Launch Provider instance\naws ec2 run-instances \\\n  --image-id ami-0694d931cee176e7d \\\n  --instance-type t3.xlarge \\\n  --key-name dataspace-key \\\n  --security-group-ids $PROVIDER_SG_ID \\\n  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=provider}]' \\\n  --region $AWS_REGION\n</code></pre> <p>Important</p> <p>Note the instance ID returned by this command.</p>"},{"location":"documentation/mv_data_space/fiware/provider/#step-3-assign-elastic-ip","title":"Step 3: Assign Elastic IP","text":"<pre><code># Replace with your Provider instance ID\nexport PROVIDER_INSTANCE_ID=\"i-xxxxxxxxx\"\n\n# Allocate Elastic IP\naws ec2 allocate-address \\\n  --domain vpc \\\n  --tag-specifications 'ResourceType=elastic-ip,Tags=[{Key=Name,Value=provider-ip}]' \\\n  --region $AWS_REGION\n\n# Associate IP to instance (replace ALLOCATION_ID with the one returned above)\naws ec2 associate-address \\\n  --instance-id $PROVIDER_INSTANCE_ID \\\n  --allocation-id ALLOCATION_ID_FROM_ABOVE \\\n  --region $AWS_REGION\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/#step-4-verify-instance-status","title":"Step 4: Verify Instance Status","text":"<pre><code>aws ec2 describe-instances \\\n  --instance-ids $PROVIDER_INSTANCE_ID \\\n  --query 'Reservations[*].Instances[*].[Tags[?Key==`Name`].Value | [0], PublicIpAddress, State.Name]' \\\n  --output table \\\n  --region $AWS_REGION\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/#step-5-prepare-instance-storage","title":"Step 5: Prepare Instance Storage","text":"<p>Since the Provider handles more data, you may need to increase the EBS volume size:</p> <ol> <li>Go to AWS Console \u2192 EC2 \u2192 Volumes</li> <li>Find the volume associated with your Provider instance</li> <li>Select it and click \"Actions\" \u2192 \"Modify volume\"</li> <li>Increase the size to at least 16 GB</li> <li>Save the changes</li> </ol> <pre><code># Replace with your Provider public IP\nexport PROVIDER_IP=\"YOUR_PROVIDER_IP\"\n\n# Connect to the instance\nssh -i \"dataspace-key.pem\" ubuntu@$PROVIDER_IP\n\n# Update and install utilities\nsudo apt-get update &amp;&amp; sudo apt-get install -y cloud-guest-utils\n\n# Expand the partition and filesystem\nsudo growpart /dev/nvme0n1 1\nsudo resize2fs /dev/root\n\n# Verify the changes\ndf -h\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/#step-6-install-k3s","title":"Step 6: Install k3s","text":"<pre><code># Install k3s\ncurl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--tls-san $PROVIDER_IP\" sh -\n\n# Get the kubeconfig\nsudo cat /etc/rancher/k3s/k3s.yaml\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/#step-7-configure-local-access","title":"Step 7: Configure Local Access","text":"<p>On your local machine, create a kubeconfig file for the Provider:</p> <pre><code># Create k3s-provider.yaml with the content from the previous step\n# Replace 127.0.0.1 with your Provider IP in the server field\n# The file should contain:\n# server: https://YOUR_PROVIDER_IP:6443\n\n# Test the connection\nexport KUBECONFIG=k3s-provider.yaml\nkubectl get nodes\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/#step-8-configure-storage-and-namespace","title":"Step 8: Configure Storage and Namespace","text":"<pre><code># Enable storage provisioner\nkubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml\n\n# Wait a few seconds for it to start. You can check its status with\nkubectl get pods -n local-path-storage\n\n# Create namespace\nkubectl create namespace provider\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/#step-9-create-provider-identity","title":"Step 9: Create Provider Identity","text":"<pre><code># Create directory for identity files\n\n# generate the private key - dont get confused about the curve, openssl uses the name `prime256v1` for `secp256r1`(as defined by P-256)\nopenssl ecparam -name prime256v1 -genkey -noout -out provider-identity/private-key.pem\n\n# generate corresponding public key\nopenssl ec -in provider-identity/private-key.pem -pubout -out provider-identity/public-key.pem\n\n# create a (self-signed) certificate\nopenssl req -new -x509 -key provider-identity/private-key.pem -out provider-identity/cert.pem -days 360\n\n# export the keystore\nopenssl pkcs12 -export -inkey provider-identity/private-key.pem -in provider-identity/cert.pem -out provider-identity/cert.pfx -name didPrivateKey\n\n# check the contents\nkeytool -v -keystore provider-identity/cert.pfx -list -alias didPrivateKey\n\n# generate did from the keystore\nwget https://github.com/wistefan/did-helper/releases/download/0.1.1/did-helper\nchmod +x did-helper\n./did-helper -keystorePath ./provider-identity/cert.pfx -keystorePassword=test\n</code></pre> <p>Important</p> <p>Note the DID returned by the <code>did-helper</code>. It is the consumer DID.</p>"},{"location":"documentation/mv_data_space/fiware/provider/#step-10-deploy-identity-secret","title":"Step 10: Deploy Identity Secret","text":"<pre><code># Create secret with the identity\nkubectl create secret generic provider-identity --from-file=provider-identity/cert.pfx -n provider\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/#step-11-configure-values","title":"Step 11: Configure Values","text":"<p>Danger</p> <p>Before deploying, you must modify the Providers's <code>values.yaml</code> file to use your actual IP address instead of <code>127.0.0.1.nip.io</code>. Modify <code>provider/values.yaml</code> file to use the external IP address instead of localhost. Other variables such as the provider DID should also be modified. In your <code>provider/values.yaml</code> file, make these changes:</p> <pre><code># Summary of Changes in provider/values.yaml\n\n## 1. Hostnames updated from localhost (127.0.0.1.nip.io) to YOUR_PROVIDER_IP (YOUR_PROVIDER_IP.nip.io)\n- provider-verifier.127.0.0.1.nip.io            \u2192 provider-verifier.YOUR_PROVIDER_IP.nip.io\n# - til-provider.127.0.0.1.nip.io                 \u2192 til-provider.YOUR_PROVIDER_IP.nip.io\n- mp-data-service.127.0.0.1.nip.io              \u2192 mp-data-service.YOUR_PROVIDER_IP.nip.io\n# - pap-provider.127.0.0.1.nip.io                 \u2192 pap-provider.YOUR_PROVIDER_IP.nip.io\n- tm-forum-api.127.0.0.1.nip.io                 \u2192 tm-forum-api.YOUR_PROVIDER_IP.nip.io\n\n## 2. DID &amp; TIR configuration updated\n- tirAddress: http://tir.127.0.0.1.nip.io:8080  \u2192 tirAddress: http://trusted-issuers-list:8080\n- did: did:key:zDnaeQfjsx66YNYV86SDBB1e5kunWKJcWwk686dvjirEE7pqW  \u2192 did: did:key:provider_key\n\n## 3. Server host URLs updated\n- host: http://provider-verifier.127.0.0.1.nip.io:8080\n  \u2192 host: http://provider-verifier.YOUR_PROVIDER_IP.nip.io\n\n## 4. Added fullnameOverride to trusted-issuers-list\n+ fullnameOverride: trusted-issuers-list\n\n## 5. APISIX routes and upstream hostnames updated\n- hostname: mp-data-service.127.0.0.1.nip.io     \u2192 hostname: mp-data-service.YOUR_PROVIDER_IP.nip.io\n- host: mp-data-service.127.0.0.1.nip.io         \u2192 host: mp-data-service.YOUR_PROVIDER_IP.nip.io\n\n## 6. ODRL PAP organization DID updated\n- value: did:key:zDnaeQfjsx66YNYV86SDBB1e5kunWKJcWwk686dvjirEE7pqW\n  \u2192 value: did:key:provider_key\n\n## 7. Scorpio trustedParticipantsLists endpoints updated\n- http://tir.trust-anchor.svc.cluster.local:8080 \u2192 http://tir.TRUS_ANCHOR_IP.nip.io\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/#step-12-add-helm-repository","title":"Step 12: Add Helm Repository","text":"<pre><code>helm repo add data-space-connector https://fiware.github.io/data-space-connector/\nhelm repo update\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/#step-13-deploy-provider","title":"Step 13: Deploy Provider","text":"<pre><code># Deploy using your modified values file\nhelm install provider-dsc data-space-connector/data-space-connector \\\n  --version 7.17.0 \\\n  -f provider/values.yaml \\\n  --namespace=provider\n\n# Monitor deployment\nkubectl get pods -n provider -w\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/provider/#changes-and-updates","title":"Changes and updates","text":"<pre><code># Update\nhelm upgrade provider-dsc data-space-connector/data-space-connector -f provider/values.yaml --namespace provider\n\n# Monitor\nwatch kubectl get pods -n provider\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/","title":"Trust Anchor","text":"<p>Warning</p> <p>Check the prerequisites section before proceeding with the deployment.</p>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-by-step-aws-deployment","title":"Step by Step AWS deployment","text":"<p>Warning</p> <p>If you are joining an existing dataspace, this step should be skipped as you will use the trust anchor of the dataspace you want to join.</p> <p>The Trust Anchor provides the basic trust infrastructure for the data space. It is usually the first component to be deployed if you are setting up a data space from scratch. </p>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-1-create-security-group","title":"Step 1: Create Security Group","text":"<p>Create a dedicated security group for the Trust Anchor:</p> <pre><code># Set your configuration\nexport YOUR_PUBLIC_IP=\"YOUR_IP_HERE\"  # Replace with your public IP\nexport AWS_REGION=\"eu-west-1\"         # Replace with your preferred region\n\n# Create security group\naws ec2 create-security-group \\\n  --group-name trust-anchor-sg \\\n  --description \"Security group for Trust Anchor\" \\\n  --region $AWS_REGION\n\n# Add SSH access from your IP\naws ec2 authorize-security-group-ingress \\\n  --group-name trust-anchor-sg \\\n  --protocol tcp \\\n  --port 22 \\\n  --cidr ${YOUR_PUBLIC_IP}/32 \\\n  --region $AWS_REGION\n\n# Add Kubernetes API access from your IP\naws ec2 authorize-security-group-ingress \\\n  --group-name trust-anchor-sg \\\n  --protocol tcp \\\n  --port 6443 \\\n  --cidr ${YOUR_PUBLIC_IP}/32 \\\n  --region $AWS_REGION\n\n# Add HTTP/HTTPS access (public)\naws ec2 authorize-security-group-ingress \\\n  --group-name trust-anchor-sg \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0 \\\n  --region $AWS_REGION\n</code></pre> <p>Important</p> <p>Note the security group ID returned by the create command.</p>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-2-launch-trust-anchor-instance","title":"Step 2: Launch Trust Anchor Instance","text":"<p>For the Trust Anchor instance we use Ubuntu 22.04 LTS image (<code>ami-0694d931cee176e7d</code>) and <code>t3.medium</code> instance type.  Feel free to change these parameters, especially if you see that the load to be supported is greater than the capacity of the virtual machine.</p> <pre><code># Replace with your security group ID\nexport TRUST_ANCHOR_SG_ID=\"sg-xxxxxxxxx\"\n\n# Launch Trust Anchor instance\naws ec2 run-instances \\\n  --image-id ami-0694d931cee176e7d \\\n  --instance-type t3.medium \\\n  --key-name dataspace-key \\\n  --security-group-ids $TRUST_ANCHOR_SG_ID \\\n  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=trust-anchor}]' \\\n  --region $AWS_REGION\n</code></pre> <p>Important</p> <p>Note the instance ID returned by this command.</p>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-3-assign-elastic-ip","title":"Step 3: Assign Elastic IP","text":"<pre><code># Replace with your Trust Anchor instance ID\nexport TRUST_ANCHOR_INSTANCE_ID=\"i-xxxxxxxxx\"\n\n# Allocate Elastic IP\naws ec2 allocate-address \\\n  --domain vpc \\\n  --tag-specifications 'ResourceType=elastic-ip,Tags=[{Key=Name,Value=trust-anchor-ip}]' \\\n  --region $AWS_REGION\n\n# Associate IP to instance (replace ALLOCATION_ID with the one returned above)\naws ec2 associate-address \\\n  --instance-id $TRUST_ANCHOR_INSTANCE_ID \\\n  --allocation-id ALLOCATION_ID_FROM_ABOVE \\\n  --region $AWS_REGION\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-4-verify-instance-status","title":"Step 4: Verify Instance Status","text":"<pre><code>aws ec2 describe-instances \\\n  --instance-ids $TRUST_ANCHOR_INSTANCE_ID \\\n  --query 'Reservations[*].Instances[*].[Tags[?Key==`Name`].Value | [0], PublicIpAddress, State.Name]' \\\n  --output table \\\n  --region $AWS_REGION\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-5-install-k3s","title":"Step 5: Install k3s","text":"<pre><code># Replace with your Trust Anchor public IP\nexport TRUST_ANCHOR_IP=\"YOUR_TRUST_ANCHOR_IP\"\n\n# Connect to the instance\nssh -i \"dataspace-key.pem\" ubuntu@$TRUST_ANCHOR_IP\n\n# Install k3s\ncurl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--tls-san $TRUST_ANCHOR_IP\" sh -\n\n# Get the kubeconfig\nsudo cat /etc/rancher/k3s/k3s.yaml\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-6-configure-local-access","title":"Step 6: Configure Local Access","text":"<p>On your local machine, create a kubeconfig file for the Trust Anchor:</p> <pre><code># Create k3s-trust-anchor.yaml with the content from the previous step (cat command)\n# Replace 127.0.0.1 with your public Trust Anchor IP in the server field\n# The file should contain:\n# server: https://YOUR_TRUST_ANCHOR_IP:6443\n\n# Test the connection\nexport KUBECONFIG=k3s-trust-anchor.yaml\nkubectl get nodes\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-7-configure-storage","title":"Step 7: Configure Storage","text":"<pre><code># Enable storage provisioner\nkubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml\n\n# Wait a few seconds for it to start. You can check its status with\nkubectl get pods -n local-path-storage\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-8-add-helm-repository","title":"Step 8: Add Helm Repository","text":"<pre><code>helm repo add data-space-connector https://fiware.github.io/data-space-connector/\nhelm repo update\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-9-configure-values","title":"Step 9: Configure Values","text":"<p>Danger</p> <p>Before deploying, you must modify the Trust Anchor's <code>values.yaml</code> file to use your actual IP address instead of <code>127.0.0.1.nip.io</code>. Modify <code>trust-anchor/values.yaml</code> file to use the external IP address instead of localhost. Replace the <code>tir</code> host reference <code>127.0.0.1.nip.io</code> with <code>YOUR_TRUST_ANCHOR_IP.nip.io</code>. This change ensures that the Trusted Issuer Registry (TIR) is accessible outside the local environment.</p> <pre><code>trusted-issuers-list:\n  tir:\n    enabled: true\n    hosts:\n      - host: tir.YOUR_TRUST_ANCHOR_IP.nip.io\n  til:\n    enabled: true\n    hosts:\n      - host: til.127.0.0.1.nip.io # Do not modify\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-10-create-namespace","title":"Step 10: Create namespace","text":"<pre><code># Create namespace\nkubectl create namespace trust-anchor\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-11-deploy-trust-anchor","title":"Step 11: Deploy Trust Anchor","text":"<pre><code># Deploy using your modified values file\nhelm install trust-anchor data-space-connector/trust-anchor --version 0.2.0 -f trust-anchor/values.yaml --namespace=trust-anchor\n\n# Monitor deployment\nwatch kubectl get pods -n trust-anchor\n</code></pre>"},{"location":"documentation/mv_data_space/fiware/trust_anchor/#step-12-changes-and-updates","title":"Step 12: Changes and updates","text":"<pre><code># Upgrade\nhelm upgrade trust-anchor data-space-connector/trust-anchor -f trust-anchor/values.yaml --namespace trust-anchor\n\n# Monitor\nwatch kubectl get pods -n trust-anchor\n</code></pre>"},{"location":"documentation/trust_frameworks/","title":"Trust Frameworks","text":"<p>A trust framework is a set of policies, principles, and mechanisms that establish and maintain trust among participants in a data space ecosystem. It outlines the rules and agreements that govern how data is accessed, used, and shared, ensuring security, transparency, compliance with regulations, and ethical use. It often includes aspects like identity verification, access control, data protection, and mutual agreements between entities to facilitate trustworthy collaboration.</p> <p>Every Data Spaces requires a framework that ensures trust between the participants. Depending on the requirements of the concrete Data Space, this can become a rather complex topic. Various trust-providers exist( f.e. Gaia-X Digital Clearing Houses) and could be reused (see Gaia-X for more information). Source: Fiware data space connector</p> <ul> <li> <p>:material-rocket-outline:{ .lg .middle } Fiware Trusted Issuers List</p> <p>EBSI Trusted Issuers Registry implementation to act as the Trusted-List-Service in the DSBA Trust and IAM Framework.</p> <p>:octicons-arrow-right-24: Official Documentation</p> <p>:octicons-arrow-right-24: More Details</p> <p>:octicons-arrow-right-24: Deployment</p> </li> <li> <p>:material-rocket-outline:{ .lg .middle } Gaia-X Digital Clearing House (GXDCH)</p> <p>The Gaia-X Framework describes functional specifications, technical requirements, and software assets necessary to get Gaia-X Compliance.</p> <p>:octicons-arrow-right-24: Official Documentation</p> </li> <li> <p>:material-rocket-outline:{ .lg .middle } iShare Trust Framework</p> <p>The iSHARE Trust Framework is a collaborative effort to improve the exchange of data between organisations in and across data spaces. The Framework results in a set of agreements which improve circumstances for data exchange.</p> <p>:octicons-arrow-right-24: Official Documentation</p> </li> </ul>"},{"location":"documentation/trust_frameworks/fiware_trust_anchor/","title":"Technical Details","text":""},{"location":"documentation/trust_frameworks/fiware_trust_anchor/#technical-details","title":"Technical Details","text":"<p>The Trusted-Issuers-List Service provides an EBSI-Trusted Issuers Registry implementation to act as the Trusted-List-Service in the DSBA Trust and IAM Framework. In addition, a Trusted Issuers List API to manage the issuers is provided.</p> <p>Both APIs Trusted-Issuers-List API and Trusted-Issuers-Registry API, are found on the same port (by default 8080) but different contexts.</p> <ul> <li>Trusted-Issuers-List API: <code>/issuer</code></li> <li>Trusted-Issuers-Registry API: <code>/v4/issuers/</code></li> </ul> <p>Fiware Trust Anchor</p> <p>The Fiware Trust Anchor is based in: FIWARE Trusted Issuers List</p> <p></p> <p>The default setup of the connector requires an EBSI-Trusted Issuers Registry to provide the list of participants. The local Data Spaces comes with the FIWARE Trusted Issuers List as a rather simple implementation of that API, providing CRUD functionality for Issuers and storage in an MySQL Database. After deployment, the API is available at http://tir.127.0.0.1.nip.io:8080. Both participants are automatically registered as \"Trusted Issuers\" in the registry with their did's.</p>"},{"location":"documentation/trust_frameworks/fiware_trust_anchor/#api-details-version-002","title":"API details (version 0.0.2)","text":""},{"location":"documentation/trust_frameworks/fiware_trust_anchor/#trusted-issuers-list","title":"Trusted Issuers List","text":""},{"location":"documentation/trust_frameworks/fiware_trust_anchor/#trusted-issuers-registry","title":"Trusted Issuers Registry","text":""},{"location":"documentation/verifiable_credentials/","title":"Verifier Credentials","text":""},{"location":"documentation/verifiable_credentials/#verifier-credential-issuer","title":"Verifier Credential Issuer","text":"<p>Every participant in a data space must have a Decentralized Identifier (DID) that identifies them. Also, Keycloak needs this key to set the organization's DID as the issuer in the Verifiable Credential.</p> <p>To generate the DID in the format required by Keycloak, there is a Dockerized tool called did-helper that produces all the necessary files by configuring just a few parameters to identify your organization:</p> <pre><code>docker run -v $(pwd):/cert \\\n    -e OUTPUT_FORMAT=\"env\" \\\n    -e OUTPUT_FILE=\"/cert/did.env\" \\\n    -e KEY_ALIAS=\"didPrivateKey\" \\\n    -e STORE_PASS=\"fill_me\" \\\n    -e COUNTRY=\"BE\" \\\n    -e STATE=\"BRUSSELS\" \\\n    -e LOCALITY=\"Brussels\" \\\n    -e ORGANIZATION=\"Fancy Marketplace Co.\" \\\n    -e COMMON_NAME=\"www.fancy-marketplace.biz\" \\\n    quay.io/wi_stefan/did-helper:0.2.0\n</code></pre> <p>This will generate two files required by Keycloak to issue Verifiable Credentials: <code>did.env</code> and <code>cert.pfx</code>.</p> <p>Warning</p> <p>Although the process of generating the DID and its use in Keycloak is automated in local FIWARE deployments, this practice, as stated by FIWARE, is not acceptable in production environments. The generation and custody of the DID should be the responsibility of the organization.</p>"},{"location":"documentation/verifiable_credentials/#identity-management-keycloak","title":"Identity Management - Keycloak","text":"<p>Keycloak is a powerful open-source Identity and Access Management solution that can be used to issue Verifiable Credentials (VCs) for users or services in a Data Space. It provides a flexible and secure way to manage identities, roles, and access policies. Some of the key features of Keycloak include:</p> <ol> <li>Manage identities of users and services</li> <li>Issue Verifiable Credentials</li> <li>Control access to resources</li> <li>Establish trust through digital identity verification</li> </ol> <p>Info</p> <p>When configured with your organization's DID, Keycloak can issue trusted Verifiable Credentials that other participants in the data space can verify.</p>"},{"location":"documentation/verifiable_credentials/#concepts","title":"Concepts","text":"<p>Keycloak operates with the concept of realms, clients, and users:</p>"},{"location":"documentation/verifiable_credentials/#realms","title":"Realms","text":"<p>A realm is a isolated logical grouping of users, roles, and clients.</p> <p>Within a realm:</p> <ul> <li>Users and their credentials are managed.</li> <li>Clients are configured, which are applications that use Keycloak for authentication.</li> <li>Roles, groups, access policies, etc. are defined.</li> <li>Authentication and login flows can be customized (screens, MFA, etc.).</li> <li>Each realm is completely independent from others.</li> </ul>"},{"location":"documentation/verifiable_credentials/#clients","title":"Clients","text":"<p>Clients are applications that use Keycloak for authentication. For each provider that you want to connect to the Data Space, you need to create a new client. Each client can be configured with its own settings, such as:</p> <ul> <li>Client ID and secret.</li> <li>Redirect URIs.</li> <li>Authentication flows.</li> <li>Access policies.</li> </ul>"},{"location":"documentation/verifiable_credentials/#users","title":"Users","text":"<p>Users are the entities or members of your organization that will authenticate against Keycloak to access the data space. They can be individuals or services. Each user can have:</p> <ul> <li>A unique username and password.</li> <li>Roles assigned to them.</li> <li>Attributes and metadata.</li> </ul>"},{"location":"documentation/verifiable_credentials/#configuration","title":"Configuration","text":"<p>To configure Keycloak for issuing Verifiable Credentials, follow these steps: Hands-On Configuration.</p>"},{"location":"documentation/verifiable_credentials/keycloak/","title":"Hands-on Configuration","text":"<p>Configuring Keycloak is essential for managing Verifiable Credentials (VCs) in a Data Space. This document provides an overview of how to set up and use Keycloak for issuing VCs.</p>"},{"location":"documentation/verifiable_credentials/keycloak/#web-access","title":"Web access","text":"<p>Keycloak has a web interface that allows you to manage different aspects of the Keycloak server, such as creating realms, clients, and users, as well as issuing verifiable credentials (VCs). Access to the Keycloak web interface is through a web browser, using the URL provided during the deployment of the Keycloak server.</p> <p>If you deploy this service following the MVDS instructions in AWS, the access URL for the Keycloak web interface is: <code>http://keycloak-consumer.&lt;your_consumer_ip&gt;.nip.io</code>.</p> <p>By default, Keycloak is deployed with a preconfigured user <code>keycloak-admin</code>. The password for this user is generated during the deployment process. You can get the password by running the following command:</p> <pre><code>kubectl get secret -n consumer -o json issuance-secret | jq '.data.\"keycloak-admin\"' -r | base64 --decode\n</code></pre>"},{"location":"documentation/verifiable_credentials/keycloak/#configuration","title":"Configuration","text":"<p>From the Keycloak administration console, you can setup different parts of the Keycloak server:</p> <ul> <li>Realms: Isolated logical grouping of users, roles, and clients. You can create multiple realms to separate different environments or applications.</li> <li>Clients: Clients are applications or services that can authenticate users and request access to resources. In the context of VCs, clients can be providers or consumers that interact with the Data Space.</li> <li>Users: Users are the entities that can authenticate and interact with the Keycloak server. Users can be assigned roles and permissions to access specific resources or perform certain actions.</li> </ul>"},{"location":"documentation/verifiable_credentials/keycloak/#realms","title":"Realms","text":"<p>From the Keycloak administration console, you can create a new realm by clicking on the <code>Add Realm</code> button.</p> <p></p> <p>Bug</p> <p>Creating a realm from scratch using only the web interface is not currently possible. You need to import a realm configuration from a JSON file or use the Keycloak Admin REST API.</p>"},{"location":"documentation/verifiable_credentials/keycloak/#from-json-file","title":"From json file","text":"<p>You can import a realm configuration from a JSON file. This is useful for setting up predefined realms with specific configurations. The realm configuration is provided in the template.default_config_all-realm.json file. This file contains the necessary configuration for the Keycloak realm, including clients, roles, and users.</p> <p>Warning</p> <p>Before importing the realm, ensure that:</p> <ul> <li>you changed the did:key values in the <code>template.default_config_all-realm.json</code> file to match the did:key generated by the did-helper. You can find the did in the did.env file inside the Keycloak container.</li> <li>you changed the pkcs12 password in the <code>template.default_config_all-realm.json</code> file to match the one used in cert.pfx generation. You can find the password executing the command in the 'Realm Settings - Key'.</li> </ul>"},{"location":"documentation/verifiable_credentials/keycloak/#initialize-json-realm-file","title":"Initialize json realm file","text":"<pre><code>cp template.default_config_all-realm.json default_config_all-realm.json\nsed -i \"s/{% raw %}{{ISSUER-DID}}{% endraw %}/$(kubectl exec keycloak-0 -c keycloak -n consumer-raw -- cat /did-material/did.env | cut -d= -f2 | xargs)/g\" default_config_all-realm.json\nsed -i \"s/{% raw %}{{STORE-PASS}}{% endraw %}/$(kubectl get secret -n consumer-raw -o json issuance-secret | jq '.data.\"store-pass\"' -r | base64 --decode)/g\" default_config_all-realm.json\n</code></pre>"},{"location":"documentation/verifiable_credentials/keycloak/#values-to-change","title":"Values to change","text":"Replace with the correct consumer did-key generated by the did-helper: <ul> <li>attributes &gt; <code>issuerDid</code>:     <pre><code>...\n\"attributes\": {\n        \"frontendUrl\": \"http://keycloak.consumer-raw.local\",\n        \"issuerDid\": \"did:key:zDnaeSwnupmLVfbuU6vwpiUBSwgjDuVon1f7u6i31f6v7VTfE\"\n    }\n...\n</code></pre></li> <li>components &gt; <code>org.keycloak.keys.KeyProvider</code> &gt; <code>config</code> &gt;<code>kid</code>:     <pre><code>...\n\"components\": {\n    \"org.keycloak.keys.KeyProvider\": [\n        {\n            ...\n            \"config\": {\n                ...\n                \"kid\": [\n                    \"did:key:zDnaeSwnupmLVfbuU6vwpiUBSwgjDuVon1f7u6i31f6v7VTfE\"\n                ],\n                ...\n            }\n        }\n    ]\n    ...\n}\n...\n</code></pre></li> <li>components &gt; <code>org.keycloak.protocol.oid4vc.issuance.signing.VerifiableCredentialsSigningService</code> &gt; <code>config</code> &gt; <code>keyId</code> and <code>issuerDid</code>:     <pre><code>...\n\"components\": {\n    ...\n    \"org.keycloak.protocol.oid4vc.issuance.signing.VerifiableCredentialsSigningService\": [\n        {\n            ...\n            \"config\": {\n                \"keyId\": [\n                    \"did:key:zDnaeSwnupmLVfbuU6vwpiUBSwgjDuVon1f7u6i31f6v7VTfE\"\n                ],\n                ...\n                \"issuerDid\": [\n                    \"did:key:zDnaeSwnupmLVfbuU6vwpiUBSwgjDuVon1f7u6i31f6v7VTfE\"\n                ],\n                ...\n            }\n        }\n    ]\n    ...\n}\n...\n</code></pre></li> </ul> Replace with the correct provider did-key generated by the did-helper: <ul> <li>clients &gt; <code>clientId</code>:     <pre><code>...\n\"clients\": [\n    {   \n        \"clientId\": \"did:key:zDnaeud7zQitUYp1MxZyRGSuweXfDXGdu6hbYpKvrM6Ws1Eva\",\n        ...\n    }\n]\n...\n</code></pre></li> <li>clients &gt; <code>protocolMappers</code> &gt; <code>config</code> &gt; <code>clientId</code>:     <pre><code>...\n\"clients\": [\n    {   \n        \"protocolMappers\": [\n            {\n                \"config\": {\n                    ...\n                    \"clientId\": \"did:key:zDnaeud7zQitUYp1MxZyRGSuweXfDXGdu6hbYpKvrM6Ws1Eva\",\n                    ...\n                }\n            }\n            ...\n        ],\n        ...\n    }\n]\n...\n</code></pre></li> <li>roles &gt; <code>client</code>:     <pre><code>...\n\"roles\": {\n    ...\n    \"client\": {\n        \"did:key:zDnaeud7zQitUYp1MxZyRGSuweXfDXGdu6hbYpKvrM6Ws1Eva\": [\n            ...\n        ]\n    }\n},\n...\n</code></pre></li> <li>users &gt; <code>clientRoles</code>:     <pre><code>...\n\"users\": [\n    {\n        ...\n        \"clientRoles\": {\n            \"did:key:zDnaeud7zQitUYp1MxZyRGSuweXfDXGdu6hbYpKvrM6Ws1Eva\": [\n                \"OPERATOR\"\n            ],\n            ...\n        },\n        ...\n    }\n],\n...\n</code></pre></li> </ul>"},{"location":"documentation/verifiable_credentials/keycloak/#clients","title":"Clients","text":"<p>To create a new client, go to the <code>Clients</code> section in the Keycloak administration console and click on the <code>Create</code> button.</p> <p></p> <p>Warning</p> <p>The DID of the client is the DID of the provider that you want to connect to the Data Space. Also you need to set the roles for the client. The roles are used to authorize the client to access the Data Space.</p>"},{"location":"documentation/verifiable_credentials/keycloak/#users","title":"Users","text":"<p>To create a new user, go to the <code>Users</code> section in the Keycloak administration console and click on the <code>Add User</code> button.</p> <p></p> Step by step: <ol> <li>Fill in the user details (username, email, etc.).     </li> <li>Set a password for the user. You can set a temporal or permanent password.     </li> <li>Assign roles to the user.     <ol> <li>In this case we are filtering roles and selecting OPERATOR role.     </li> </ol> </li> <li>Save the user.</li> </ol>"},{"location":"documentation/verifiable_credentials/keycloak/#generate-verifiable-credential-for-new-user","title":"Generate Verifiable Credential for new user","text":""},{"location":"documentation/verifiable_credentials/keycloak/#user-credential","title":"User Credential","text":"<p>Get an AccessToken from Keycloak:</p> <pre><code>export ACCESS_TOKEN=$(curl -s -X POST http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/openid-connect/token \\\n--header 'Accept: */*' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data grant_type=password \\\n--data client_id=admin-cli \\\n--data username=citcom-test \\\n--data password=citcom-test \\\n--data scope='openid'| jq '.access_token' -r); echo ${ACCESS_TOKEN}\n</code></pre> <p>Get a credential offer uri(for the `user-credential), using the retrieved AccessToken:</p> <pre><code>export OFFER_URI=$(curl -s -X GET 'http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/oid4vc/credential-offer-uri?credential_configuration_id=user-credential' --header \"Authorization: Bearer ${ACCESS_TOKEN}\" | jq '\"\\(.issuer)\\(.nonce)\"' -r); echo ${OFFER_URI}\n</code></pre> <p>Use the offer uri(e.g. the issuer and nonce fields), to retrieve the actual offer:</p> <pre><code>export PRE_AUTHORIZED_CODE=$(curl -s -X GET ${OFFER_URI} \\\n--header \"Authorization: Bearer ${ACCESS_TOKEN}\" | jq '.grants.\"urn:ietf:params:oauth:grant-type:pre-authorized_code\".\"pre-authorized_code\"' -r); echo ${PRE_AUTHORIZED_CODE}\n</code></pre> <p>Exchange the pre-authorized code from the offer with an AccessToken at the authorization server:</p> <pre><code>export CREDENTIAL_ACCESS_TOKEN=$(curl -s -X POST http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/openid-connect/token \\\n--header 'Accept: */*' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data grant_type=urn:ietf:params:oauth:grant-type:pre-authorized_code \\\n--data pre-authorized_code=${PRE_AUTHORIZED_CODE} | jq '.access_token' -r); echo ${CREDENTIAL_ACCESS_TOKEN}\n</code></pre> <p>Use the returned access token to get the actual credential:</p> <pre><code>export VERIFIABLE_CREDENTIAL=$(curl -s -X POST http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/oid4vc/credential \\\n--header 'Accept: */*' \\\n--header 'Content-Type: application/json' \\\n--header \"Authorization: Bearer ${CREDENTIAL_ACCESS_TOKEN}\" \\\n--data '{\"credential_identifier\":\"user-credential\", \"format\":\"jwt_vc\"}' | jq '.credential' -r); echo ${VERIFIABLE_CREDENTIAL}\n</code></pre> <p>You will receive a jwt-encoded credential to be used within the data space.</p> <p>Decoded jwt credential for citcom-test user example: <pre><code>{\n  \"nbf\": 1752477064,\n  \"jti\": \"urn:uuid:cc974c65-b99c-4a92-9279-b01717e85a0b\",\n  \"iss\": \"did:key:zDnaeomDNv18fjUmK6pgfL5fpAkE1LyfHo1oJb6yG6Z7Sf9ft\",\n  \"vc\": {\n    \"type\": [\n      \"UserCredential\"\n    ],\n    \"issuer\": \"did:key:zDnaeomDNv18fjUmK6pgfL5fpAkE1LyfHo1oJb6yG6Z7Sf9ft\",\n    \"issuanceDate\": 1752477064.948,\n    \"credentialSubject\": {\n      \"firstName\": \"Citcom\",\n      \"lastName\": \"Test\",\n      \"email\": \"citcom-test@example.com\"\n    },\n    \"@context\": [\n      \"https://www.w3.org/2018/credentials/v1\",\n      \"https://www.w3.org/ns/credentials/v1\"\n    ]\n  }\n}\n</code></pre></p>"},{"location":"documentation/verifiable_credentials/keycloak/#operator-credential","title":"Operator Credential","text":"<p>Get an AccessToken from Keycloak:</p> <pre><code>export ACCESS_TOKEN=$(curl -s -X POST http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/openid-connect/token \\\n--header 'Accept: */*' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data grant_type=password \\\n--data client_id=admin-cli \\\n--data username=citcom-test \\\n--data password=citcom-test \\\n--data scope='openid'| jq '.access_token' -r); echo ${ACCESS_TOKEN}\n</code></pre> <p>Get a credential offer uri(for the `operator-credential), using the retrieved AccessToken:</p> <pre><code>export OFFER_URI=$(curl -s -X GET 'http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/oid4vc/credential-offer-uri?credential_configuration_id=operator-credential' --header \"Authorization: Bearer ${ACCESS_TOKEN}\" | jq '\"\\(.issuer)\\(.nonce)\"' -r); echo ${OFFER_URI}\n</code></pre> <p>Use the offer uri(e.g. the issuer and nonce fields), to retrieve the actual offer:</p> <pre><code>export PRE_AUTHORIZED_CODE=$(curl -s -X GET ${OFFER_URI} \\\n--header \"Authorization: Bearer ${ACCESS_TOKEN}\" | jq '.grants.\"urn:ietf:params:oauth:grant-type:pre-authorized_code\".\"pre-authorized_code\"' -r); echo ${PRE_AUTHORIZED_CODE}\n</code></pre> <p>Exchange the pre-authorized code from the offer with an AccessToken at the authorization server:</p> <pre><code>export CREDENTIAL_ACCESS_TOKEN=$(curl -s -X POST http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/openid-connect/token \\\n--header 'Accept: */*' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data grant_type=urn:ietf:params:oauth:grant-type:pre-authorized_code \\\n--data pre-authorized_code=${PRE_AUTHORIZED_CODE} | jq '.access_token' -r); echo ${CREDENTIAL_ACCESS_TOKEN}\n</code></pre> <p>Use the returned access token to get the actual credential:</p> <pre><code>export VERIFIABLE_CREDENTIAL=$(curl -s -X POST http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/oid4vc/credential \\\n--header 'Accept: */*' \\\n--header 'Content-Type: application/json' \\\n--header \"Authorization: Bearer ${CREDENTIAL_ACCESS_TOKEN}\" \\\n--data '{\"credential_identifier\":\"operator-credential\", \"format\":\"jwt_vc\"}' | jq '.credential' -r); echo ${VERIFIABLE_CREDENTIAL}\n</code></pre> <p>You will receive a jwt-encoded credential to be used within the data space.</p> <p>Decoded jwt credential for citcom-test user example:</p> <pre><code>{\n  \"nbf\": 1752560178,\n  \"jti\": \"urn:uuid:adfa1e98-28d0-4a6f-8a53-d9c6f0294ca4\",\n  \"iss\": \"did:key:zDnaeomDNv18fjUmK6pgfL5fpAkE1LyfHo1oJb6yG6Z7Sf9ft\",\n  \"vc\": {\n    \"type\": [\n      \"OperatorCredential\"\n    ],\n    \"issuer\": \"did:key:zDnaeomDNv18fjUmK6pgfL5fpAkE1LyfHo1oJb6yG6Z7Sf9ft\",\n    \"issuanceDate\": 1752560178.136,\n    \"credentialSubject\": {\n      \"firstName\": \"Citcom\",\n      \"lastName\": \"Test\",\n      \"roles\": [\n        {\n          \"names\": [\n            \"OPERATOR\"\n          ],\n          \"target\": \"did:key:zDnaeud7zQitUYp1MxZyRGSuweXfDXGdu6hbYpKvrM6Ws1Eva\"\n        }\n      ],\n      \"email\": \"citcom-test@example.com\"\n    },\n    \"@context\": [\n      \"https://www.w3.org/2018/credentials/v1\",\n      \"https://www.w3.org/ns/credentials/v1\"\n    ]\n  }\n}\n</code></pre>"},{"location":"documentation/verifiable_credentials/keycloak/#register-new-provider-as-client","title":"Register new Provider as client","text":"<p>Each new provider in the data space has to be instantiated in the verifiable credentials issuer (Keycloak) as a <code>client</code>.</p> <p>Currently, it is not possible to fully configure a client for the issuance of Verifiable Credentials via the Keycloak graphical interface. To achieve this, we will leverage the client import functionality and complete the necessary information.</p> 1. Instantiate the JSON file to be imported from <code>template.client_import.json</code> <pre><code>cp template.client_import.json client_import.json\nsed -i \"s/{% raw %}{{PROVIDER-DID}}{% endraw %}/&lt;fillme&gt;/g\" client_import.json\n</code></pre> 2. Define the types of Verifiable Credentials to be issued. <p><pre><code>\"attributes\": {\n  ...\n  \"vc.verifiable-credential.format\": \"jwt_vc\",\n  \"vc.verifiable-credential.scope\": \"VerifiableCredential\"\n},\n</code></pre> In the code snippet above, a type of Verifiable Credential named VerifiableCredential is defined.</p> <p>Additional credential types can be defined following the same structure:</p> <pre><code>\"vc.operator-credential.format\": \"jwt_vc\",\n\"vc.operator-credential.scope\": \"OperatorCredential\"\n</code></pre> <p>The above example defines a credential type named OperatorCredential.</p> 3. Adjust the information to be included in the VC JWT via mappers. <pre><code>{\n...\n\"protocolMappers\": [\n    {\n    \"name\": \"first-name\",\n    \"protocol\": \"oid4vc\",\n    \"protocolMapper\": \"oid4vc-user-attribute-mapper\",\n    \"consentRequired\": false,\n    \"config\": {\n    \"subjectProperty\": \"firstName\",\n    \"userAttribute\": \"firstName\",\n    \"aggregateAttributes\": \"false\"\n    }\n    },\n    {\n    \"name\": \"target-role-mapper\",\n    \"protocol\": \"oid4vc\",\n    \"protocolMapper\": \"oid4vc-target-role-mapper\",\n    \"consentRequired\": false,\n    \"config\": {\n    \"subjectProperty\": \"roles\",\n    \"clientId\": \"{% raw %}{{PROVIDER-DID}}{% endraw %}\",\n    \"supportedCredentialTypes\": \"VerifiableCredential\"\n    }\n    },\n    {\n    \"name\": \"context-mapper\",\n    \"protocol\": \"oid4vc\",\n    \"protocolMapper\": \"oid4vc-context-mapper\",\n    \"consentRequired\": false,\n    \"config\": {\n    \"context\": \"https://www.w3.org/2018/credentials/v1\",\n    \"supportedCredentialTypes\": \"VerifiableCredential\"\n    }\n    }    \n],\n...\n}\n</code></pre> <p>The mappers allow us to configure the information that will be added to the <code>credentialSubject</code> of the generated Verifiable Credential.</p> <p>In the example provided, the following mappers have been created:</p> <ul> <li><code>first-name</code>: adds the user's firstName field to the <code>credentialSubject</code>.</li> <li><code>target-role-mapper</code>: adds the user\u2019s roles for the client with ID <code>PROVIDER_IR</code>.</li> <li><code>context-mapper</code>: adds the static context with the specified value.</li> </ul> <p>For each mapper, a list of <code>supportedCredentialTypes</code> is specified, indicating the credential types for which this mapper should be applied.</p> <p>Example of the generated <code>credentialSubject</code>:</p> <pre><code>\"credentialSubject\": {\n    \"firstName\": \"Citcom\",\n    \"lastName\": \"Test\",\n    \"roles\": [\n    {\n        \"names\": [\n        \"OPERATOR\"\n        ],\n        \"target\": \"did:key:zDnaeud7zQitUYp1MxZyRGSuweXfDXGdu6hbYpKvrM6Ws1Eva\"\n    }\n    ],\n    \"email\": \"citcom-test@example.com\"\n}\n</code></pre> 4. Import client into Keycloak. <p>Navigate to Clients &gt; Click the Import client button: </p> <p></p> <p>Click Browse &gt; Select the <code>client_import.json</code> file:</p> <p></p> 5. Define client_roles. <p></p> 6. Assign client_role to a user. <p></p> 7. Generate a VerifiableCredential. <p>Get an AccessToken from Keycloak:</p> <p><pre><code>export ACCESS_TOKEN=$(curl -s -X POST http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/openid-connect/token \\\n--header 'Accept: */*' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data grant_type=password \\\n--data client_id=admin-cli \\\n--data username=test-user \\\n--data password=test \\\n--data scope='openid'| jq '.access_token' -r); echo ${ACCESS_TOKEN}\n</code></pre> Get a credential offer uri(for the `verifiable-credential), using the retrieved AccessToken:</p> <pre><code>export OFFER_URI=$(curl -s -X GET 'http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/oid4vc/credential-offer-uri?credential_configuration_id=verifiable-credential' --header \"Authorization: Bearer ${ACCESS_TOKEN}\" | jq '\"\\(.issuer)\\(.nonce)\"' -r); echo ${OFFER_URI}\n</code></pre> <p>Use the offer uri(e.g. the issuer and nonce fields), to retrieve the actual offer:</p> <pre><code>export PRE_AUTHORIZED_CODE=$(curl -s -X GET ${OFFER_URI} \\\n--header \"Authorization: Bearer ${ACCESS_TOKEN}\" | jq '.grants.\"urn:ietf:params:oauth:grant-type:pre-authorized_code\".\"pre-authorized_code\"' -r); echo ${PRE_AUTHORIZED_CODE}\n</code></pre> <p>Exchange the pre-authorized code from the offer with an AccessToken at the authorization server:</p> <pre><code>export CREDENTIAL_ACCESS_TOKEN=$(curl -s -X POST http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/openid-connect/token \\\n--header 'Accept: */*' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data grant_type=urn:ietf:params:oauth:grant-type:pre-authorized_code \\\n--data pre-authorized_code=${PRE_AUTHORIZED_CODE} | jq '.access_token' -r); echo ${CREDENTIAL_ACCESS_TOKEN}\n</code></pre> <p>Use the returned access token to get the actual credential:</p> <pre><code>export VERIFIABLE_CREDENTIAL=$(curl -s -X POST http://keycloak.consumer-raw.local/realms/CitcomAI/protocol/oid4vc/credential \\\n--header 'Accept: */*' \\\n--header 'Content-Type: application/json' \\\n--header \"Authorization: Bearer ${CREDENTIAL_ACCESS_TOKEN}\" \\\n--data '{\"credential_identifier\":\"verifiable-credential\", \"format\":\"jwt_vc\"}' | jq '.credential' -r); echo ${VERIFIABLE_CREDENTIAL}\n</code></pre> <p>You will receive a jwt-encoded credential to be used within the data space.</p> <p>Decoded jwt credential for citcom-test user example: <pre><code>{\n    \"nbf\": 1752560178,\n    \"jti\": \"urn:uuid:adfa1e98-28d0-4a6f-8a53-d9c6f0294ca4\",\n    \"iss\": \"did:key:zDnaeomDNv18fjUmK6pgfL5fpAkE1LyfHo1oJb6yG6Z7Sf9ft\",\n    \"vc\": {\n    \"type\": [\n        \"OperatorCredential\"\n    ],\n    \"issuer\": \"did:key:zDnaeomDNv18fjUmK6pgfL5fpAkE1LyfHo1oJb6yG6Z7Sf9ft\",\n    \"issuanceDate\": 1752560178.136,\n    \"credentialSubject\": {\n        \"firstName\": \"Citcom\",\n        \"lastName\": \"Test\",\n        \"roles\": [\n        {\n            \"names\": [\n            \"OPERATOR\"\n            ],\n            \"target\": \"did:key:zDnaeud7zQitUYp1MxZyRGSuweXfDXGdu6hbYpKvrM6Ws1Eva\"\n        }\n        ],\n        \"email\": \"citcom-test@example.com\"\n    },\n    \"@context\": [\n        \"https://www.w3.org/2018/credentials/v1\",\n        \"https://www.w3.org/ns/credentials/v1\"\n    ]\n    }\n}\n</code></pre></p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"How can I edit a page on the Citcom.ai Hub? <p>This website is publicly accessible, so anyone can edit its content by following a series of steps.</p> <ol> <li>Create a Fork of the project.  </li> <li>Create a Pull-Resquest with your changes.</li> </ol> <p>More details...</p> What types of elements can I add to Markdown? <p>This website uses pure Markdown as well as the plugin Material for MkDocs. You can find all the information about the available references here.</p> What icons can I use? <p>To use an icon, you need to write <code>:material-&lt;icon-name&gt;:</code>, for example: <code>:material-&lt;icon-name&gt;:</code> =&gt; :material-account-question:</p> <p>You can find all the icons here: icons</p>"},{"location":"faq/pull_request_procedure/","title":"Request changes: Pull Request Procedure","text":"<p>By default, Citcom.ai Hub is an open portal where anyone can publish changes. However, these changes must be made in a controlled manner through a review process (pull-request).</p>"},{"location":"faq/pull_request_procedure/#1-fork-of-the-citcomai-hub-repo","title":"1. Fork of the Citcom.ai Hub repo","text":"<p>The first step to editing the content of Citcom.ai Hub is to have a copy of it in our personal GitHub account. This is done by creating a Fork from the GitHub repository.</p> <ol> <li>While logged into GitHub, click on the icon labeled <code>Fork</code>.<ol> <li>(Optional) Deselect the checkbox if you want all branches of the repository to be copied.</li> </ol> </li> <li>Create the fork.</li> </ol> <p> </p>"},{"location":"faq/pull_request_procedure/#2-selecting-the-branch-to-edit","title":"2. Selecting the branch to edit","text":"<p>From our fork of the Citcom.ai Hub repository, if needed, we can switch to any branch.</p> <p></p> <p>Verification of the fork</p> <p>You can verify that you are in the fork by checking that you are in your personal account, and below the repository name, it states that it has been forked from citcom.ai.</p>"},{"location":"faq/pull_request_procedure/#3-editing-the-content","title":"3. Editing the content","text":"<p>Make the necessary changes and commit them.</p> <p></p>"},{"location":"faq/pull_request_procedure/#4-publishing-the-changes-for-review-pull-request","title":"4. Publishing the changes for review (Pull Request)","text":"<p>From our forked repository, go to the <code>Pull requests</code> tab (1) and select the option to create a <code>New pull request</code> (2).</p> <p></p> <p>IMPORTANT</p> <p>Select the correct branch to which you want to publish the changes. In this case:</p> Repository name Branch name Destination <code>CitComAI-Hub/CitComAI-Hub.github.io</code> <code>&lt;branch_name&gt;</code> Origin <code>&lt;github_account_name&gt;/CitComAI-Hub.github.io</code> <code>&lt;branch_name&gt;</code> <p>Verify that the names of the destination branch and the source branch are the same.</p> <p></p> <p>Lastly, verify that the commits containing all the changes you made are visible, then create the pull request. If it has been correctly created, you will see your pull-request appear in the repository <code>CitComAI-Hub/CitComAI-Hub.github.io</code> for review and publication on the official Citcom.ai Hub portal.</p> <p></p>"},{"location":"getting_started/","title":"Getting started","text":"<p>Embarking on your journey with CitCom.ai is made simple through our comprehensive Getting Started guide. This section is designed to help users familiarize themselves with the essential aspects of the CitCom.ai project. To facilitate your learning, we present a roadmap that outlines key steps and resources, ensuring you have a clear path to follow as you dive into the principles and functionalities of CitCom.ai.</p> <p>[timeline center alternate(docs/getting_started/roadmap.yaml)]</p> <p>Next steps</p> <p>Ready to get started? Just click \"Next\" on the bottom navigation bar to continue!</p>"},{"location":"getting_started/interoperability/","title":"Interoperability","text":"<p>Interoperability is a key aspect of CitCom.ai\u2019s approach to ensuring that data can be securely and effectively shared across different systems and organizations. </p>"},{"location":"getting_started/interoperability/#idsa-documentation","title":"IDSA Documentation","text":"<ul> <li> <p>Key Layers: The IDS documentation emphasizes four primary layers\u2014technical, semantic, organizational, and legal\u2014that collectively underpin effective interoperability.</p> </li> <li> <p>Intra Data Space Interoperability: Within a single Data Space, a unified governance framework ensures that all participants adhere to the same protocols and models.</p> </li> <li> <p>Cross-Data Space Interoperability: When operating across multiple Data Spaces, additional coordination is required to bridge varying protocols and legal frameworks.</p> </li> </ul> <p>For a comprehensive explanation and additional context, please refer to the original IDS documentation on Interoperability in Data Spaces.</p>"},{"location":"getting_started/interoperability/#mims","title":"MIMs","text":"<p>MIMs stands for \"Minimal Interoperability Mechanisms\". These guidelines and standards were developed by the Open &amp; Agile Smart Cities (OASC) initiative to promote interoperability among different city systems and technologies, such as traffic management systems, waste management systems, and energy distribution systems. CitCom.ai project embraces minimal interoperability mechanisms (MIMs) as part of its approach. </p>"},{"location":"getting_started/interoperability/#interoperability-levels","title":"Interoperability levels","text":"<p>Interoperability in data spaces defines how diverse systems can seamlessly exchange, interpret, and use data. Interoperability can be conceptualized as a maturity model with three levels:</p>"},{"location":"getting_started/interoperability/#level-0-custom-integration","title":"Level 0 - Custom Integration","text":"<p>At Level 0, no standard exists for data exchange. Each system is integrated via wholly customized solutions. This results in interfaces that are highly specific to each data platform. Although functional, such integration is often brittle and difficult to scale because it lacks a common vocabulary or consistent protocols. The absence of shared standards limits the potential for cross-organizational data reuse.</p>"},{"location":"getting_started/interoperability/#level-1-pivotal-interoperability-points","title":"Level 1 - Pivotal Interoperability Points","text":"<p>At Level 1, the focus shifts to identifying and adopting pivotal interoperability points among different data platforms. Key mechanisms such as MIM1 NGSI-LD and MIM2 Smart Data Models serve as the foundational standards at this stage.</p> <ul> <li> <p>MIM1 NGSI-LD provides a standardized API for context information management, enabling different systems to share and retrieve structured data consistently.</p> </li> <li> <p>MIM2 Smart Data Models standardize how entities and their attributes are defined, ensuring that data from disparate sources uses a common language.</p> </li> </ul> <p>This level establishes core interoperability through shared data models and context management, even though individual systems may retain internal heterogeneity. </p>"},{"location":"getting_started/interoperability/#level-2-common-interface-with-integrated-security","title":"Level 2 - Common Interface with Integrated Security","text":"<p>At Level 2, interoperability is further enhanced by defining a standard interface typically through deploying a data space connector. This connector not only leverages the standards from Level 1 but also integrates a comprehensive security layer that includes:</p> <ul> <li> <p>Trust Frameworks: Mechanisms to establish and maintain trust among participants.</p> </li> <li> <p>Identity Management: Standardized approaches to manage and verify participant identities.</p> </li> <li> <p>Authorization and Trust Services: Policies and registries that enforce data usage rules and access control.</p> </li> </ul> <p>This unified interface simplifies plug-and-play integration and ensures that all data transactions are secure, standardized, and governed under a common framework</p> <p>Learn more about this</p> <p>Check Interoperability in Data Spaces section from IDSA. Also check MIMs Toolkit section or OASC MIMs 2024 for more details.</p>"},{"location":"getting_started/nodes_countries/","title":"Nodes &amp; Countries","text":"<p>CitCom is organized as three \"supernodes\" Nordic, Central, and South, with satellites and sub-nodes located across 11 countries in the European Union: Denmark, Sweden, Finland, the Netherlands, Belgium, Luxembourg, France, Germany, Spain, Poland, and Italy. </p> <ul> <li>Nordic Supernode is focused on the theme \"POWER.\" Nordic nodes work in areas that support smart cities and communities and focus on energy, environmental solutions, cyber security, ethics, and edge learning.</li> <li>Central Supernode revolves around the \"MOVE\" theme. The node focuses more specifically on challenges related to mobility and logistics in cities and communities.</li> <li>Southern Supernode revolves around the \"CONNECT\" theme. The supernode focuses more specifically on the need to securely connect citizens, infrastructures, AI, and robotics services in cities and communities. This theme and supernode will focus on innovations that provide intelligence to local infrastructures and city cross-sectoral services.</li> </ul> <p>Supernodes serve as regional hubs that aggregate expertise, resources, and best practices, thereby providing strategic oversight and ensuring that the testing environments remain closely aligned with the unique challenges and opportunities of their respective regions.</p> The Nordic Supernode The Central Supernode The Southern Supernode Focus Areas POWER MOBILITY CONNECTIVITY Lead Denmark Belgium Spain Members Denmark, Sweden, Finland Belgium, France, Luxembourg, The Netherlands Spain, Italy, Poland, Germany <p>Learn more about this</p> <p>Check the TEF nodes section for further details.</p>"},{"location":"getting_started/data_spaces/","title":"Data Spaces","text":""},{"location":"getting_started/data_spaces/#overview","title":"Overview","text":"<p>Data spaces (DS) refer to structured and managed environments where data from various sources is securely stored, shared, and utilized for AI and robotics applications within smart and sustainable cities. These data spaces are the project's core technology, enabling participants to access and leverage high-quality data for testing, experimentation, and validation of AI technologies.</p> <p>Data spaces support interoperability, ensuring that data from different sources can be combined and used while complying with regulations such as the GDPR and other EU directives. They provide the necessary infrastructure for managing data in a way that supports ethical considerations, cybersecurity, and the broader goals of creating a more digital and sustainable urban environment.</p> <p></p> More information <ul> <li>Data Space Support Center (DSSC): <ul> <li>Data Space Definition</li> </ul> </li> <li>Data Spaces for Smart Cities (DS4SCC):<ul> <li>Interactive portal for building data spaces in Smart Communities</li> </ul> </li> </ul>"},{"location":"getting_started/data_spaces/#minimum-viable-data-space","title":"Minimum Viable Data Space","text":"<p>A Minimum Viable Data Space (MVDS) is a basic configuration of a data space that includes only the essential components required (Trust Framework and Connector) to ensure interoperability and enable the secure and sovereign exchange of information between organisations. Its minimal approach aims to reduce initial complexity, support technological adoption, and provide a way to test the ecosystem\u2019s functionality before scaling to more comprehensive solutions.</p> <ul> <li> <p>Trust Anchor (TA): Responsible for managing trust in the data space. It is the manager of the identities of the different elements of the data space and of managing the trust in them. At least one TA shall exist in the data space, managed by the organization in charge of the data space. </p> <p>More details</p> <p>Overview of open-source trust frameworks: here</p> </li> <li> <p>Data Space Connector (DSC): Responsible for managing the communication between the different elements of the data space. It oversees managing authentication, authorization and data access control. There must be at least two DSCs, one per organization, to be able to affirm that a data space exists.</p> <p>More details</p> <p>Overview of open-source data spaces connectors: here</p> </li> </ul> <p>This type of data space serves as a testing environment that facilitates the validation of data exchange models and a gradual migration from existing systems. Thanks to its streamlined structure, the MVDS is especially well-suited for demonstrations, pilots, or early implementation stages in collaborative settings where data sharing is expected to be trustworthy and controlled.</p> <p>Interoperability Levels</p> <p>Following the Interoperability Levels, the MVDS aims to provide the minimal set of tools required to progress from interoperability level 1 to level 2.</p>"},{"location":"getting_started/data_spaces/#citcomai-data-space","title":"CitCom.ai Data Space","text":"<p>Data spaces are pivotal in accelerating innovation by facilitating collaboration among different stakeholders. They offer a secure and compliant framework for data exchange, ensuring that the AI solutions developed within the project are both reliable and aligned with European standards.</p> <p>The following figure provides a high-level overview of a general architecture illustrating how CitCom.ai integrates with current data platforms, and how data spaces will serve as a linking nexus between: nodes, data platforms, and the AI and robotics solutions.</p> <p></p> <p>At the bottom of the figure, we see the data platforms that exist in many cities and communities throughout the EU (Nodes &amp; Countries). These data platforms are operated by both public and private sector organisations. Much of these data sets are locked in silos, meaning that it is hard to get the data out of the databases in which they are stored, to be shared with other parties. </p> <p>Data spaces are currently being created to counter this siloed EU data landscape. Multiple data space initiatives at local, national and EU level are currently underway. These data spaces will be contributed to by the CitCom.ai TEF sites and leveraged by the AI and robotics solutions that will be brought to the TEF sites by CitCom.ai customers. The solutions will connect to the data spaces through the principles of the Minimal Interoperability Mechanisms (MIMs). </p> <p>In addition to leveraging existing and emerging data space initiatives, CitCom.ai will also deploy its own experimental data space environment. This tailored infrastructure will interconnect as many project sites as possible, enabling the seamless exchange of data across diverse platforms. By simulating real-world interoperability scenarios, it will serve as a living lab where AI and robotics solutions can be tested in a multi-site setup, ensuring that interoperability and compliance with Minimal Interoperability Mechanisms (MIMs) are validated and enhanced in practice.</p> <p></p>"},{"location":"getting_started/get_involved/","title":"Get involved","text":"<p>CitCom.ai is a project open to the European society, allowing any interested company or institution to join this ecosystem. CitCom.ai's core technology is data spaces, which guarantee secure environments for data exchange.  </p>"},{"location":"getting_started/get_involved/#what-data-are-available","title":"What data are available?","text":"<p>CitCom.ai brings together a network of sites distributed across Europe (Nodes &amp; Countries), each contributing data from distinct domains of smart cities \u2014 including mobility, power and connect. These datasets originate from a variety of stakeholders such as cities, research centers, and private entities.</p> <p>To explore what\u2019s currently available, we invite you to browse our data catalog, where you\u2019ll find detailed descriptions and thematic classifications. If a dataset sparks your interest, you can reach out to its owner directly to request access. In many cases, data access can also be facilitated via our experimental data space layer, ensuring secure and standardized retrieval in compliance with European data-sharing protocols.</p> <ul> <li> <p>:material-store-search-outline:{ .lg .middle } Data catalog</p> <p>Explore available data within currently deployed data spaces.</p> <p>:octicons-arrow-right-24: Learn more</p> </li> </ul>"},{"location":"getting_started/get_involved/#how-to-join-the-citcomai-data-space","title":"How to join the Citcom.ai data space?","text":"<p>The process of joining an existing data space will be outlined, including details on membership requirements, access controls, and collaboration tools within that environment.  </p> <p>In addition, access to a data space does not have to be only as a consumer of data, but you can also access it as a provider. Depending on the role you want to play in the data space, the way you join may vary. Basically, you need to deploy a data space connector and request certified access to the data space trust anchor manager.</p> <ul> <li> <p>:material-database-arrow-right-outline:{ .lg .middle } Join a Data Space</p> <p>Understand the process of joining to an existing data space.</p> <p>:octicons-arrow-right-24: Learn more</p> </li> </ul>"},{"location":"getting_started/get_involved/join/","title":"Join the Data Space","text":"<p>Warning</p> <p>CitCom.ai uses FIWARE technology for its data spaces, although in the future it will evolve to a combination of Fiware and Eclipse technology.</p> <p>The initial adoption of the FIWARE Data Space technology within the CitCom.ai project is a strategic decision that aligns with the Data Space Business Alliance (DSBA) and the Data Spaces for Smart Cities (DS4SCC) recommendations, ensuring a robust and interoperable framework for data exchange across Testing and Experimentation Facilities (TEFs).</p> <p>To access to a data space, you mainly need: </p> <ol> <li> <p>A digital certificate (Verifiable Credential): To be able to identify yourself as an organization within the data space. </p> </li> <li> <p>A data space connector: To be able to communicate with the data space.</p> </li> </ol>"},{"location":"getting_started/get_involved/join/#vc-issuer","title":"VC Issuer","text":"<p>The VC Issuer is a component that issues Verifiable Credentials (VCs) to entities within the data space. These credentials are used to authenticate and authorize access to resources in the data space.</p> <ul> <li> <p>:material-cog-outline:{ .lg .middle } Issue your credentials</p> <p>Verifiable Credential details configuration.</p> <p>:octicons-arrow-right-24: More info</p> </li> </ul>"},{"location":"getting_started/get_involved/join/#trust-anchor","title":"Trust Anchor","text":"<p>The Trust Anchor (TA) is a critical component in the data space ecosystem. It serves as a trusted entity that issues and manages digital certificates (Verifiable Credentials) for organizations and individuals participating in the data space. The TA ensures that all participants are authenticated and authorized to access the resources within the data space.</p> <p>More details</p> <p>Overview of open-source trust frameworks: here</p>"},{"location":"getting_started/get_involved/join/#sign-up","title":"Sign Up","text":"<p>Depending on the configuration of the data space, the registration process may vary. Currently, most commonly, you will need to contact the data space TA administrator for information on the type of certificate you need and how to provide it so that they can authorize you as an authorized entity in the data space. </p> <p>In the future, this process will be automated, and you will be able to do it directly from the data space platform. Using the European digital identity, you will be able to register in the data space in a simple and secure way.</p> TA Endpoint <p>To be part of the CitCom.ai data space, you need to register in the CitCom.ai Trust Anchor. The endpoint for the CitCom.ai Trust Anchor is: <code>https://xxxx</code></p>"},{"location":"getting_started/get_involved/join/#data-space-connector","title":"Data Space Connector","text":"<p>The Data Space Connector (DSC) is a software component that is responsible for managing the communication between the different elements of the data space. It oversees managing authentication, authorization, and data access control. Fiware provides a reference implementation of the DSC, which is available in the Fiware GitHub repository.</p> <p>In all cases, you will need to deploy a Data Space Connector in your organization to be able to share data in the data space. Depending if you want consume or provide data, you will need to deploy a different type of connector.</p> <p>More details</p> <p>Overview of open-source data spaces connectors: here</p> <ul> <li> <p>:material-cog-outline:{ .lg .middle } Consumer</p> <p>The Consumer Role is responsible for consuming data from the data space. This role requires a Data Space Connector that is configured to access and retrieve data from the data space.</p> <p>:octicons-arrow-right-24: AWS Deployment</p> <p>:octicons-arrow-right-24: Technical Details</p> </li> <li> <p>:material-cog-outline:{ .lg .middle } Provider</p> <p>The Provider Role is responsible for providing data to the data space. This role requires a Data Space Connector that is configured to share data with the data space.</p> <p>:octicons-arrow-right-24: AWS Deployment</p> <p>:octicons-arrow-right-24: Technical Details</p> </li> </ul>"},{"location":"getting_started/get_involved/join/#data-federation","title":"Data Federation","text":"<p>The Data Federation is a more complex scenario where multiple Data Spaces or data platform are federated to share data. Depending on the technology used, the federation process can be different. Reference</p>"},{"location":"getting_started/get_involved/join/#verifiable-credentials-management","title":"Verifiable Credentials management","text":"<p>The Verifiable Credentials (VCs) management is a crucial aspect of the data space, as it ensures that all participants are authenticated and authorized to access resources. The management of VCs is typically handled by an Identity Management system, such as Keycloak.</p> <ul> <li> <p>:material-cog-outline:{ .lg .middle } Keycloak Configuration</p> <p>The Keycloak Configuration is responsible for managing the authentication and authorization of users and services in the data space. This configuration is crucial for issuing Verifiable Credentials.</p> <p>:octicons-arrow-right-24: About Keycloak</p> <p>:octicons-arrow-right-24: Hands-On Configuration</p> </li> </ul>"},{"location":"tef/","title":"TEF Nodes","text":"<p>Overview of the current Testing and Experimentation Facility (TEF) nodes / sites.</p>"},{"location":"tef/#nordic-supernode-power","title":"Nordic Supernode | POWER","text":"TEF Site Node Data Broker API Broker DS4SSCC Ref. Architecture DOLL Living Lab Denmark {{ config.extra.labels.data_brokers.fiware }} {{ config.extra.labels.api_brokers.ngsi_ld }} Aarhus City Lab Denmark {{ config.extra.labels.data_brokers.fiware }} {{ config.extra.labels.api_brokers.ngsi_ld }} Net Zero Innovation Hub Denmark DTI Denmark Center Denmark Denmark {{ config.extra.labels.data_brokers.kafka }} {{ config.extra.labels.api_brokers.custom }} GATE21 Denmark {{ config.extra.labels.data_brokers.fiware }} {{ config.extra.labels.api_brokers.ngsi_ld }} Tampere Finland {{ config.extra.labels.data_brokers.iot_ticket }}\u200b {{ config.extra.labels.api_brokers.custom }} RISE Sweden {{ config.extra.labels.data_brokers.fiware }} {{ config.extra.labels.api_brokers.ngsi_v2 }}  {{ config.extra.labels.api_brokers.ngsi_ld }}"},{"location":"tef/#central-supernode-move","title":"Central Supernode | MOVE","text":"TEF Site Node Data Broker API Broker DS4SSCC Ref. Architecture Mechelen Belgium Brussels Belgium Eindhoven Netherlands {{ config.extra.labels.data_brokers.fiware }} Paris France LIST Luxembourg"},{"location":"tef/#south-supernode-connect","title":"South Supernode | CONNECT","text":"TEF Site Node Data Broker API Broker DS4SSCC Ref. Architecture Valencia Spain {{ config.extra.labels.data_brokers.fiware }} {{ config.extra.labels.api_brokers.ngsi_v2 }} Milano Italy {{ config.extra.labels.data_brokers.fiware }} {{ config.extra.labels.api_brokers.ngsi_v2 }} Warsaw Poland"},{"location":"tef/central_move/brussels/","title":"Brussels","text":""},{"location":"tef/central_move/brussels/#overview","title":"Overview","text":"<p>The testing zone is located in Brussels, Belgium and is centred around innovations for dynamic traffic management and traffic sensor solutions. The leading partner is the region of Brussels' transport authority Bruxelles Mobilit\u00e9, supported by Paradigm, FARI, imec and Digitaal Vlaanderen. </p> <p>The Charles Quint avenue and Annie Cordy tunnel are offered as primary testing location. Charles Quint is a major entry axis connecting the ring road of Brussels with its city center, the Annie Cordy tunnel is an extention to this axis. This axis provides an interesting and challenging traffic situation, common for large cities. Incidents (or measures) can have a profound and cascading effect on the entire regional traffic. The testing zone consists of: - 9 crossings - 1 tunnel, 2.9 km long - making it the longest tunnel in Belgium</p> <p>Along the entire acis, multiple sensors and technologies are deployed to monitor and act upon the traffic situation. Innovators are offered access to the data these sensors produce to create, test and improve their solutions, e.g. validating their models or calibrate their sensors. Technologies that are currently deployes on the avenue are (or will be) offered through the TEF. - ANPR cameras  - XStream cameras - Traffic light controllers - Magnetometers - DAI (automatic incident detection) cameras - Air quality sensors - and more...</p> <p>All data is offered to innovators through the regional data platform.</p> <p>Last but not least, innovators are also offered the ability to temporarily deploy their novel (traffic) sensors and / or solutions on the avenue.</p>"},{"location":"tef/central_move/brussels/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai Services Catalog. Provide a brief description of each service, and include any relevant links or documentation.</p> <ul> <li>Access to a wide range of traffic data for AI training: Access to real-time or historic traffic data (e.g. counting and categorisation of road users) coming from the Charles Quint axis</li> <li>Deploy and test traffic solutions: Temporary deployment of your solution or sensor in a real-life dynamic situation (the Charles Quint avenue or Annie Cordy tunnel)</li> <li>Integrate traffic solutions: Integrate your solution for testing purposes and have it used by Bruxelles Mobilit\u00e9's traffic operators. Receive real-life validation, feedback and / or recommendations by end-users and validate your solution's effectiveness and / or accuracy.</li> <li>CAVE: CAVE or Computer Augmented Virtual Environment is a state-of-the-art immersive space that can visualize and simulate digital content and virtual objects to mimic real-world phenomena.</li> <li>Local Digital Twin scenario development: Develop a digital twin of a city, building or environment or employ the digital twin to simulate scenarios (e.g. on traffic, resilience, ...)</li> </ul>"},{"location":"tef/central_move/brussels/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        &lt;no_specified&gt; - API: &lt;no_specified&gt; - Version: &lt;no_specified&gt;      Data Source &lt;no_specified&gt; IdM &amp; Auth &lt;no_specified&gt; Data Publication &lt;no_specified&gt;"},{"location":"tef/central_move/brussels/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p>"},{"location":"tef/central_move/brussels/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/central_move/brussels/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/central_move/brussels/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: [Name and description of the stakeholder, e.g., university, research institute, industry partner]</li> <li>Stakeholder 2: [Description]</li> <li>Stakeholder 3: [Description]</li> </ul>"},{"location":"tef/central_move/brussels/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>TEF Belgium Coordinator: [Thomas De Meester, thomas.demeester@imec.be]</li> <li>Site Coordinator: [Mohamed Aarab, maarab@sprb.brussels]</li> <li>Technical Support: [Name and contact details]</li> <li>General Inquiries: [Name and contact details]</li> </ul>"},{"location":"tef/central_move/brussels/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/central_move/brussels/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/central_move/eindhoven/","title":"Eindhoven","text":""},{"location":"tef/central_move/eindhoven/#overview","title":"Overview","text":"<p>Provide a brief overview of the TEF Site, including its location, objectives, and role within the CitCom.ai ecosystem. Describe any relevant background information that stakeholders might find helpful.</p> <p>Example:</p> <p>The [TEF Site Name] is located in [City, Country] and is dedicated to advancing research and development in [relevant domain]. This site is equipped with state-of-the-art infrastructure and is a key site in the CitCom.ai project, facilitating collaboration between cities and comunities, industrial partners (AI innovators) and research institutions.</p>"},{"location":"tef/central_move/eindhoven/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai Services Catalog. Provide a brief description of each service, and include any relevant links or documentation.</p> <ul> <li>Service 1: [Description of Service 1]</li> <li>Service 2: [Description of Service 2]</li> <li>Service 3: [Description of Service 3]</li> </ul>"},{"location":"tef/central_move/eindhoven/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        {{ config.extra.labels.data_brokers.fiware }} - API: &lt;no_specified&gt; - Version: &lt;no_specified&gt;      Data Source &lt;no_specified&gt; IdM &amp; Auth &lt;no_specified&gt; Data Publication &lt;no_specified&gt;"},{"location":"tef/central_move/eindhoven/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/central_move/eindhoven/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/central_move/eindhoven/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/central_move/eindhoven/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: [Name and description of the stakeholder, e.g., university, research institute, industry partner]</li> <li>Stakeholder 2: [Description]</li> <li>Stakeholder 3: [Description]</li> </ul>"},{"location":"tef/central_move/eindhoven/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: [Name and contact details]</li> <li>Technical Support: [Name and contact details]</li> <li>General Inquiries: [Name and contact details]</li> </ul>"},{"location":"tef/central_move/eindhoven/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/central_move/eindhoven/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/central_move/list/","title":"Luxembourg","text":""},{"location":"tef/central_move/list/#overview","title":"Overview","text":"<p>The LIST TEF Site is located in Belval, Luxembourg and is dedicated to advancing research and development in Digital Twin technologies. This site is equipped with state-of-the-art infrastructure and is a key site in the CitCom.ai project, facilitating collaboration between cities and comunities, industrial partners (AI innovators) and research institutions.</p>"},{"location":"tef/central_move/list/#services-offered","title":"Services Offered","text":"<ul> <li>Digital maturity Assessment: Guidance and support to cities and AI innovators in scoping the best-fitted AI approach, according to their needs, context and maturity. This service, which outcome is a tailored development plan for a proofof-concept, is delivered through two participatory working sessions with relevant profiles of the customer.</li> <li>Digital Twin Scoping: Guidance and support to cities and AI innovators in scoping the best-fitted DT approach, according to their needs, context and maturity. This service, which outcome is a tailored development plan for a proofof-concept, is delivered through two participatory working sessions with relevant profile of the customer.</li> <li>Exploratory data Analysis: Assessing the fit for purpose of existing data sets of the customer. The customer gets an analysis this set of data: i.e. the data fitness for their purpose and its visualization either in the form of charts or of a dashboard.</li> <li>AI sandbox: Supporting decision-making for cities to select a LLM for their chatbot or improve the existing.</li> <li>Large-Scale visualization: A customized service to allow for the exploration and analysis of large and complex datasets, supporting collaborative decision-making.</li> <li>Proof-of-concept: Tailored development of AI and data analytic proof-of-concept.</li> <li>Smart cities club Luxembourg: Smart cities club Luxembourg network of experts in Smart cities involving city representatives for reflection and common proposal around their challenges and needs.</li> <li>Local Digital Twin Electromobility toolbox: A true digital twin in electromobility to determine the best location for BEV charging station. This LDT can be customised on demand according to specific cities needs and challenges.</li> <li>ViPV Simulator: A simulator tool to quantify real potential of PV energy generation by running vehicles using weather and built-up data, that can be used by e.g. public transportation providers to support their decisionmaking towards adopting on-board photovoltaic technology.</li> <li>Optimal EV: A service to assess the opportunity and feasibility of solar panel on fleet.</li> </ul>"},{"location":"tef/central_move/list/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        &lt;no_specified&gt; - API: &lt;no_specified&gt; - Version: &lt;no_specified&gt;      Data Source &lt;no_specified&gt; IdM &amp; Auth &lt;no_specified&gt; Data Publication &lt;no_specified&gt;"},{"location":"tef/central_move/list/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/central_move/list/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/central_move/list/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: SWIO Charging Station data</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/central_move/list/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>City of Differdange: The city provides data about their current EV charging and PV generation infrastructure.</li> <li>TICE: Bus operator company for the South of Luxembourg country.</li> <li>SWIO: Charging Station provider for the city of Esch-sur-Alzette.</li> </ul>"},{"location":"tef/central_move/list/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: Andr\u00e9s Mel\u00e9ndez Imaz, andres.melendez@list.lu</li> <li>Technical Support: Andr\u00e9s Mel\u00e9ndez Imaz, andres.melendez@list.lu</li> <li>General Inquiries: Thomas Tamisier, thomas.tamisier@list.lu</li> </ul>"},{"location":"tef/central_move/list/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/central_move/list/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/central_move/mechelen/","title":"Mechelen","text":""},{"location":"tef/central_move/mechelen/#overview","title":"Overview","text":"<p>The TEF Site is located in Mechelen, Belgium and is dedicated to advancing research and development in mobility. This site provides local personel who will help provide space on the public domain to test state-of-the-art infrastructure and is a key site in the CitCom.ai project, facilitating collaboration between cities and comunities, industrial partners (AI innovators) and research institutions.</p>"},{"location":"tef/central_move/mechelen/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai Services Catalog. Provide a brief description of each service, and include any relevant links or documentation.</p> <ul> <li>Service 1: Provide small grants, in the form of open calls, to test experimental, smart city, technology which tackles mobility problems in an urban environment</li> <li>Service 2: Provide public services to private parties who want to test their technology on the public domain </li> <li>Service 3: Provide raw mobility data in a collaborative way</li> </ul>"},{"location":"tef/central_move/mechelen/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: Sensor data platform (no open data portal yet) and GEO data platform</li> <li>Local Digital Twins: This map provides an overview of all the active sensors in Mechelen</li> <li>Specific Hardware: 1 CCTV camera which can be placed on the public domain. Two Telraam sensors which can be placed anywhere in the city on the interior side of a building</li> <li>IoT Platforms: Sensor data platform (no open data portal yet)</li> <li>Visualization platforms: SaaS environment called 'Mobilize' a product of Cegeka which provides floating car data</li> </ul> Specifications Data Broker        &lt;no_specified&gt; - API: &lt;no_specified&gt; - Version: &lt;no_specified&gt;      Data Source &lt;no_specified&gt; IdM &amp; Auth &lt;no_specified&gt; Data Publication &lt;no_specified&gt;"},{"location":"tef/central_move/mechelen/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/central_move/mechelen/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/central_move/mechelen/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: Local weather and WBGT in Mechelen link</li> <li>Dataset_2: Parking occupation of parking garages in Mechelen (privatly owned data - can only be shared with consent of owner) </li> <li>Dataset_3: Count data on several locations in Mechelen where the passing amount of pedestrians, cyclists, cars and trucks are measured (link)</li> </ul>"},{"location":"tef/central_move/mechelen/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: Cronos Public Services: Private company specialized in a number ICT related skillsets</li> <li>Stakeholder 2: IGEMO: Intercommunal regonal public organisation in which Mechelen is situated</li> <li>Stakeholder 3: imec: Worldleader in microchipresearch, as chiplabo of the world</li> </ul>"},{"location":"tef/central_move/mechelen/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: Roos Lowette - roos.lowette@mechelen.be</li> <li>Technical Support: Benjamin Vermeulen - benjamin.vermeulen@mechelen.be</li> <li>General Inquiries: data@mechelen.be</li> <li>TEF Belgium Coordinator: Thomas De Meester - thomas.demeester@imec.be</li> </ul>"},{"location":"tef/central_move/mechelen/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> </ul>"},{"location":"tef/central_move/paris/","title":"Paris","text":""},{"location":"tef/central_move/paris/#overview","title":"Overview:","text":"<p>The French test site is located in the Paris region at 5 dedicated sites (Trappes, Satory, Linas, Saclay and Champs-sur-Marne) and is dedicated to advancing research and development in autonomous mobility. This site is equipped with state-of-the-art infrastructure (test tracks, XiL simulation test bench...) and is a key site in the CitCom.ai project, facilitating the integration and validation of its high-risk AI devices to help them integrate into the urban environment.</p>"},{"location":"tef/central_move/paris/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai</p> <ul> <li>Virtual facility services: The French node can cover all simulation needs (HIL, MIL, SIL, DIL, VIL) for the virtual design of transport systems and vehicles, evaluating multimodal Human-Computer Interactions, AI evaluation, driving simulation, intelligent sensors evaluation...</li> <li>Physcial facility services: We will provide test tracks for tests as close as possible to real conditions (urban or suburban traffic areas, urban road infrastructure, etc.) for example at UTAc with TEQMO or UGE at Satory. They will also be able to draw on the approval and certification expertise and the upstream research expertise to support them, depending on their stage of development. We can also provide the full range of regulatory test benches for vehicle certification  (Charging, battery, EMC, acoustic, emission, ...) or sensor characterization physical bench in addition to track tests.</li> </ul> <p>The UGE will also be able to make available the Sense-City physical test facility that allows controlled weather conditions in a bench simulating a small urban area. \\</p> <p>The UGE provides the real design of transport systems and vehicles, thanks to the ImPACT 3D AV which is a real automated vehicle for the prototyping, testing and evaluation of automated or communicating systems. This real prototype will make it possible to study, among other things, the impact of new technologies on the environment. ImPACT 3D AV embed a full hardware architecture with various sensor technologies to test algorithms on tracks</p> <ul> <li>Algorithm creation &amp; validation: The French node will be able to carry out customised testing of all AI algorithms on software-in-the-loop benches for urban mobility applications. From the conceptualisation of the test plan, to the creation of the database (Tests can be carried out by simulation or using real data), to the choice or creation of metrics and tools, all the parameters for evaluating algorithms can be discussed.</li> </ul> <p>In an upstream approach, IRT SystemX offers one of its reference technological platforms: DebiAI, open source platform for improving the quality of datasets and their use in learning models. This platform provides support in the specialisation of the data necessary for a use case and the representativeness of the data available or used.</p> <p>We also offer cybersecurity tests.</p> <ul> <li>Compliance &amp; ethics assistance: The LNE (the oldest body dedicated to AI evaluation in Europe) can also help innovators obtain certification or regulatory support for their AI algorithms (the LNE was the first in the world to provide a process certification for IA solution). The LNE can also accompany on the ethical aspects and preparation to the AI Act. At UTAC, we can also provide the full range of regulatory test benches for vehicle certification  (Charging, battery, EMC, acoustic, emission, ...). UTAC has developed its own certification focused on autonomous vehicles. Potential customers can also access the UTAC and LNE training catalogue on AI and autonomous vehicles</li> </ul>"},{"location":"tef/central_move/paris/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        &lt;no_specified&gt; - API: &lt;no_specified&gt; - Version: &lt;no_specified&gt;      Data Source &lt;no_specified&gt; IdM &amp; Auth &lt;no_specified&gt; Data Publication &lt;no_specified&gt;"},{"location":"tef/central_move/paris/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/central_move/paris/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/central_move/paris/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/central_move/paris/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: [Name and description of the stakeholder, e.g., university, research institute, industry partner]</li> <li>Stakeholder 2: [Description]</li> <li>Stakeholder 3: [Description]</li> </ul>"},{"location":"tef/central_move/paris/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: [Name and contact details]</li> <li>Technical Support: [Name and contact details]</li> <li>General Inquiries: [Name and contact details]</li> </ul>"},{"location":"tef/central_move/paris/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/central_move/paris/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/nordic_power/aarhus_city_lab/","title":"Aarhus City Lab","text":""},{"location":"tef/nordic_power/aarhus_city_lab/#overview","title":"Overview","text":"<p>Provide a brief overview of the TEF Site, including its location, objectives, and role within the CitCom.ai ecosystem. Describe any relevant background information that stakeholders might find helpful.</p> <p>Example:</p> <p>The [TEF Site Name] is located in [City, Country] and is dedicated to advancing research and development in [relevant domain]. This site is equipped with state-of-the-art infrastructure and is a key site in the CitCom.ai project, facilitating collaboration between cities and comunities, industrial partners (AI innovators) and research institutions.</p>"},{"location":"tef/nordic_power/aarhus_city_lab/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai Services Catalog. Provide a brief description of each service, and include any relevant links or documentation.</p> <ul> <li>Service 1: [Description of Service 1]</li> <li>Service 2: [Description of Service 2]</li> <li>Service 3: [Description of Service 3]</li> </ul>"},{"location":"tef/nordic_power/aarhus_city_lab/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        {{ config.extra.labels.data_brokers.fiware }} - API: {{ config.extra.labels.api_brokers.ngsi_ld }} - Version: Scorpio      Data Source OS2IoT IdM &amp; Auth &lt;no_specified\\&gt; Data Publication &lt;no_specified\\&gt;"},{"location":"tef/nordic_power/aarhus_city_lab/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/nordic_power/aarhus_city_lab/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":"<p>{{ config.extra.labels.ds4ssc_compliant.yes_comp.data_sources }} {{ config.extra.labels.ds4ssc_compliant.yes_comp.data_broker }} {{ config.extra.labels.ds4ssc_compliant.yes_comp.data_api }} {{ config.extra.labels.ds4ssc_compliant.no_comp.data_idm_auth }} {{ config.extra.labels.ds4ssc_compliant.no_comp.data_publication }}</p> <p></p>"},{"location":"tef/nordic_power/aarhus_city_lab/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/nordic_power/aarhus_city_lab/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: [Name and description of the stakeholder, e.g., university, research institute, industry partner]</li> <li>Stakeholder 2: [Description]</li> <li>Stakeholder 3: [Description]</li> </ul>"},{"location":"tef/nordic_power/aarhus_city_lab/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: [Name and contact details]</li> <li>Technical Support: [Name and contact details]</li> <li>General Inquiries: [Name and contact details]</li> </ul>"},{"location":"tef/nordic_power/aarhus_city_lab/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/nordic_power/aarhus_city_lab/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/nordic_power/center_denmark/","title":"Center Denmark","text":""},{"location":"tef/nordic_power/center_denmark/#overview","title":"Overview","text":"<p>Center Denmark is located in Fredericia, Denmark, and serves as a national hub for digitalization across the utility sector. As a TEF Site in the CitCom.ai project, Center Denmark focuses on enabling AI-driven innovation by providing access to high-quality, real-world data from electricity, water, and heating systems (including gas and PtX where relevant).</p> <p>Center Denmark provides secure and structured data infrastructure that supports collaboration between utilities, AI innovators, and researchers. The site plays a central role in bridging the gap between data availability and AI application by making validated utility datasets accessible for development, testing, and scaling of smart solutions that support the green transition.</p> <p>In short: Center Denmark provides access to high-quality utility data through a dedicated data portal, offering both downloadable datasets and API-based access \u2013 ready for use in AI, analytics, and research.</p>"},{"location":"tef/nordic_power/center_denmark/#services-offered","title":"Services Offered","text":"<p>The following services provided by Center Denmark as part of the CitCom.ai ecosystem are focused on enabling secure, structured, and value-driven access to utility data. Whether for collaboration between utilities and service providers, academic research, or early-stage experimentation, the services make it possible to work with real-world data from electricity, water, heating, gas, and PtX \u2013 all compliant with GDPR and tailored to different needs.</p>"},{"location":"tef/nordic_power/center_denmark/#available-services-and-example-datasets","title":"Available Services and Example Datasets","text":"<p>Center Denmark builds customised datasets based on specific needs and can also provide predefined datasets for common use cases. To create tailored datasets, the platform combines a wide range of trusted data sources from utilities and public registers, ensuring they are structured, cleaned, and ready for use in AI training, research, forecasting, or infrastructure planning.</p> <p>In addition to customised datasets, Center Denmark offers predefined datasets representing some of the most frequently requested data domains:</p> <ul> <li>District Heating Data \u2013 time-series consumption and production data, typically aggregated to daily values.  </li> <li>Water Data \u2013 metered water consumption datasets, supporting both operational and analytical applications.  </li> <li>Electricity Data \u2013 household or aggregated consumption data, enabling load analysis, forecasting, and optimization use cases.  </li> </ul> <p>A full overview of available data sources and datasets can be explored via portal.centerdenmark.com or in the Data Catalouge. </p>"},{"location":"tef/nordic_power/center_denmark/#service-1-access-to-utility-data-for-co-developed-digital-services","title":"Service 1: Access to Utility Data for Co-developed Digital Services","text":"<p>This service supports collaboration between utility companies and service providers by granting access to structured and enriched utility data. The data is typically used for AI and analytics purposes such as forecasting, predictive maintenance, or grid planning. Center Denmark manages the entire data process \u2013 from collection and cleansing to enrichment and delivery \u2013 ensuring high data quality and full GDPR compliance.</p> <p>\ud83d\udcc4 Documentation: Link to documentation will be inserted later</p>"},{"location":"tef/nordic_power/center_denmark/#service-2-build-your-own-utility-dataset","title":"Service 2: Build-Your-Own Utility Dataset","text":"<p>This modular service enables data consumers (e.g. researchers, startups) to configure and request customized datasets based on available utility data. Users select a base dataset (electricity, water, heating) and can combine it with open data sources such as weather data, building information (BBR), electricity spot prices, or demographic data. The final dataset is delivered at the desired processing level (e.g. anonymised or pseudonymised), and pricing depends on the complexity and scope of the request.</p> <p>\ud83d\udcc4 Documentation: Link to documentation will be inserted later</p>"},{"location":"tef/nordic_power/center_denmark/#service-3-open-utility-datasets","title":"Service 3: Open Utility Datasets","text":"<p>This service provides free access to pre-approved, anonymised datasets via Center Denmark\u2019s data portal. It is designed for early-stage experimentation, prototyping, education, and exploratory analysis. The datasets are ready to use and can be combined with publicly available data sources such as weather and building data, without any need for customization.</p> <p>\ud83d\udcc4 Documentation: Link to documentation will be inserted later</p>"},{"location":"tef/nordic_power/center_denmark/#infrastructure-components","title":"Infrastructure Components","text":"<ul> <li>Data Platforms: Data is available from Center Denmark\u2019s secure data platform, providing structured, GDPR-compliant datasets from utilities and public sources. Data can be accessed via RESTful APIs or by downloading datasets through the Data Portal (portal.centerdenmark.com).  </li> <li>Local Digital Twins: Center Denmark does not deliver a digital twin itself but provides the data infrastructure and standardized interfaces (aligned with Smart Data Models format) that enable third parties to build digital twins on top of the platform.  </li> <li>Specific Hardware: No physical sensors or field devices are operated directly by Center Denmark. Data is sourced from utility partners\u2019 smart meters, SCADA/SRO systems, and IoT devices. High-performance compute resources and secure servers are used for data ingestion, processing, and structuring.  </li> <li>IoT Platforms: Data streams from IoT sources, such as smart meters, SCADA systems, weather stations, and building sensors, are securely integrated via partner-provided connections and standardized for use on the platform.  </li> <li>Visualization Platforms: Visualization is primarily based on open-source tools such as Apache Superset, offering flexible dashboards and analytics. The platform also supports integration with Power BI and other external visualization environments as needed.  </li> </ul> Specifications Data Broker        {{ config.extra.labels.data_brokers.kafka }} - API: {{ config.extra.labels.api_brokers.custom }} - Version:&lt;no_specified\\&gt;      Data Source Nifi IdM &amp; Auth &lt;no_specified\\&gt; Data Publication MQTT, AMQP"},{"location":"tef/nordic_power/center_denmark/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/nordic_power/center_denmark/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":"<p>{{ config.extra.labels.ds4ssc_compliant.yes_comp.data_sources }} {{ config.extra.labels.ds4ssc_compliant.yes_comp.data_broker }} {{ config.extra.labels.ds4ssc_compliant.yes_comp.data_api }} {{ config.extra.labels.ds4ssc_compliant.no_comp.data_idm_auth }} {{ config.extra.labels.ds4ssc_compliant.yes_comp.data_publication }}</p> <p></p>"},{"location":"tef/nordic_power/center_denmark/#available-data-sources","title":"Available Data Sources","text":"<p>Since Center Denmark builds customised datasets based on specific needs, there are no predefined or fixed datasets available. Instead, the platform provides access to a wide range of trusted data sources from both utilities and public registers. These sources can be combined and processed to create datasets tailored to individual use cases \u2013 whether for AI training, research, forecasting, or infrastructure planning.  </p> <p>The full overview of available data sources and formats can be explored at portal.centerdenmark.com.</p> Category Examples Description Utility data Electricity, water, heating, gas, PtX Consumption and production data from utility partners Grid topology Electricity grids, district heating networks Aggregated network structures where permitted Weather data DMI, local weather stations Temperature, wind speed/direction, solar radiation Building data BBR (Building and Dwelling Register) Building type, energy label, heating system, floor area Price data Nord Pool spot prices Hourly electricity market prices Geospatial references DAR, GIS layers Geographical mapping and address-based enrichment Demographic data Statistics Denmark Population, household types, income levels (when permitted and relevant) <p>All data is processed in compliance with GDPR and access is granted based on appropriate legal agreements with data owners.</p>"},{"location":"tef/nordic_power/center_denmark/#contact-information","title":"Contact Information","text":"<p>For any inquiries or collaboration opportunities related to the TEF Site at Center Denmark, please contact the relevant team member:</p> <ul> <li> <p>Site Coordinator Britt Reenberg   Head of PMO   \ud83d\udce7 brir@centerdenmark.com</p> </li> <li> <p>Technical Support Thomas Thue S\u00f8rensen   Senior Data Engineer   \ud83d\udce7 thomas.soerensen@centerdenmark.com</p> </li> <li> <p>General Inquiries Andreas Okkels O. Thomsen   Senior Business Developer   \ud83d\udce7 andreas.thomsen@centerdenmark.com</p> </li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/nordic_power/doll_living_lab/","title":"DOLL Living Lab","text":""},{"location":"tef/nordic_power/doll_living_lab/#overview","title":"Overview","text":"<p>Digital Outdoor Living Lab (DOLL) is located in Albertslund, Denmark and is dedicated to advancing research and development in Smart City technologies, including Intelligent Traffic Systems, Outdoor Lighting and Structural Health. This site is equipped with state-of-the-art infrastructure and is a key site in the CitCom.ai project, facilitating collaboration between cities and communities, industrial partners (AI innovators) and research institutions. The site hosts a Local Digital Twin with a network of data-providing partners</p>"},{"location":"tef/nordic_power/doll_living_lab/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai Services Catalog. Provide a brief description of each service, and include any relevant links or documentation.</p> <ul> <li>DOLL Basic Membership: Testing and Experimentation of ONE single Use-Case/Device Type. Exposure and networking provided through DOLL Visitor Service and European project activities. Integration of data into Local Digital Twin.</li> <li>DOLL Expanded Membership: Test and Experimentation of MULTIPLE Use-Cases/Device Types. Exposure and networking provided through DOLL Visitor Service and European project activities. Integration of data into Local Digital Twin. Access to Data from Site through RESTful API from LDT. Data-economy matchmaking with relevant consumers. Additional KPIs regarding Visitor Service Services.</li> <li>DOLL Expanded ITS Membership: As Expanded Membership, but aimed specifically at Intelligent Traffic Systems operators/providers. Includes regulatory approval, integration to local Traffic Light Controller by state-approved system integrator. </li> </ul>"},{"location":"tef/nordic_power/doll_living_lab/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: Data is availible from the LDT via a RESTful interface.</li> <li>Local Digital Twins: LDT based on Scorpio Context Broker, providing data in Smart Data Model / NGSI-LD format.</li> <li>Specific Hardware: Rich network of sensors, cameras. No specific Compute resources.</li> <li>IoT Platforms: Partner-provided platforms used for demonstration such as Sensative's Yggio. OS2-IoT for publicically-available data.</li> <li>Visualization platforms: Basic Grafana and Streamlit dashboards and demo apps. Unreal Engine-based Virtual Lab, fully open-world photorealistic virtual environment.</li> <li>Other: DOLL has free reign to install, test and integrate any type of device in the lab. DOLL has the Municipal mandate in the area, and therfore operates on behalf of the local municipality in this area.</li> </ul> Specifications Data Broker        {{ config.extra.labels.data_brokers.fiware }} - API: {{ config.extra.labels.api_brokers.ngsi_ld }} - Version: Scorpio      Data Source Scorpio Context Broker IdM &amp; Auth KeyCloak Data Publication RESTful API"},{"location":"tef/nordic_power/doll_living_lab/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/nordic_power/doll_living_lab/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":"<p>{{ config.extra.labels.ds4ssc_compliant.yes_comp.data_sources }} {{ config.extra.labels.ds4ssc_compliant.yes_comp.data_broker }} {{ config.extra.labels.ds4ssc_compliant.yes_comp.data_api }} {{ config.extra.labels.ds4ssc_compliant.yes_comp.data_idm_auth }} {{ config.extra.labels.ds4ssc_compliant.no_comp.data_publication }}</p> <p></p>"},{"location":"tef/nordic_power/doll_living_lab/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/nordic_power/doll_living_lab/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: Albertslund Municipality, Local Municipality, operationally responsible in tandem with DOLL.</li> <li>Stakeholder 2: We Build Denmark, National Cluster for Buildings and Civil Engineering, parent organisation and owner of DOLL. </li> <li>Stakeholder 3: Denmarks Technical University, National Technical University, key stakeholder in research-driven projects.</li> <li>Stakeholder 4: Aleksandra Institute, FORCE Technology, National RTOs with project-based engagement in DOLL, knowledge partners.</li> </ul>"},{"location":"tef/nordic_power/doll_living_lab/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: Line Johansen, line.nykjaer.johansen@webuilddenmark.dk</li> <li>Technical Support: Ben Cahill, ben.cahill@webuilddenmark.dk</li> <li>General Inquiries: Ben Cahill, ben.cahill@webuilddenmark.dk</li> </ul>"},{"location":"tef/nordic_power/doll_living_lab/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in Intelligent Traffic Systems, Outdoor Lighting, Environmental Monitoring, and it is open to collaboration with other EU projects in the area of Infrastructural Health Monitoring, Dataspaces, Data Economy, Edge AI with Computer Vision.</p>"},{"location":"tef/nordic_power/doll_living_lab/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/nordic_power/dti/","title":"DTI","text":""},{"location":"tef/nordic_power/dti/#overview","title":"Overview","text":"<p>Provide a brief overview of the TEF Site, including its location, objectives, and role within the CitCom.ai ecosystem. Describe any relevant background information that stakeholders might find helpful.</p> <p>Example:</p> <p>The [TEF Site Name] is located in [City, Country] and is dedicated to advancing research and development in [relevant domain]. This site is equipped with state-of-the-art infrastructure and is a key site in the CitCom.ai project, facilitating collaboration between cities and comunities, industrial partners (AI innovators) and research institutions.</p>"},{"location":"tef/nordic_power/dti/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai Services Catalog. Provide a brief description of each service, and include any relevant links or documentation.</p> <ul> <li>Service 1: [Description of Service 1]</li> <li>Service 2: [Description of Service 2]</li> <li>Service 3: [Description of Service 3]</li> </ul>"},{"location":"tef/nordic_power/dti/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        &lt;no_specified\\&gt; - API:&lt;no_specified\\&gt; - Version:&lt;no_specified\\&gt;      Data Source &lt;&lt;no_specified\\&gt; IdM &amp; Auth &lt;no_specified\\&gt; Data Publication &lt;no_specified\\&gt;"},{"location":"tef/nordic_power/dti/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/nordic_power/dti/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":"<p>{{ config.extra.labels.ds4ssc_compliant.yes_comp.data_sources }} {{ config.extra.labels.ds4ssc_compliant.no_comp.data_broker }} {{ config.extra.labels.ds4ssc_compliant.no_comp.data_api }} {{ config.extra.labels.ds4ssc_compliant.no_comp.data_idm_auth }} {{ config.extra.labels.ds4ssc_compliant.no_comp.data_publication }}</p> <p></p>"},{"location":"tef/nordic_power/dti/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/nordic_power/dti/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: [Name and description of the stakeholder, e.g., university, research institute, industry partner]</li> <li>Stakeholder 2: [Description]</li> <li>Stakeholder 3: [Description]</li> </ul>"},{"location":"tef/nordic_power/dti/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: [Name and contact details]</li> <li>Technical Support: [Name and contact details]</li> <li>General Inquiries: [Name and contact details]</li> </ul>"},{"location":"tef/nordic_power/dti/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/nordic_power/dti/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/nordic_power/gate21/","title":"GATE21","text":""},{"location":"tef/nordic_power/gate21/#overview","title":"Overview","text":"<p>Provide a brief overview of the TEF Site, including its location, objectives, and role within the CitCom.ai ecosystem. Describe any relevant background information that stakeholders might find helpful.</p> <p>Example:</p> <p>The [TEF Site Name] is located in [City, Country] and is dedicated to advancing research and development in [relevant domain]. This site is equipped with state-of-the-art infrastructure and is a key site in the CitCom.ai project, facilitating collaboration between cities and comunities, industrial partners (AI innovators) and research institutions.</p>"},{"location":"tef/nordic_power/gate21/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai Services Catalog. Provide a brief description of each service, and include any relevant links or documentation.</p> <ul> <li>Service 1: [Description of Service 1]</li> <li>Service 2: [Description of Service 2]</li> <li>Service 3: [Description of Service 3]</li> </ul>"},{"location":"tef/nordic_power/gate21/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        {{ config.extra.labels.data_brokers.fiware }} - API: {{ config.extra.labels.api_brokers.ngsi_ld }} - Version: Scorpio      Data Source OS2IoT IdM &amp; Auth &lt;no_specified\\&gt; Data Publication &lt;no_specified\\&gt;"},{"location":"tef/nordic_power/gate21/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/nordic_power/gate21/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/nordic_power/gate21/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/nordic_power/gate21/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: [Name and description of the stakeholder, e.g., university, research institute, industry partner]</li> <li>Stakeholder 2: [Description]</li> <li>Stakeholder 3: [Description]</li> </ul>"},{"location":"tef/nordic_power/gate21/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: [Name and contact details]</li> <li>Technical Support: [Name and contact details]</li> <li>General Inquiries: [Name and contact details]</li> </ul>"},{"location":"tef/nordic_power/gate21/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/nordic_power/gate21/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/nordic_power/net_zero_innovation_hub/","title":"Net Zero Innovation Hub","text":""},{"location":"tef/nordic_power/net_zero_innovation_hub/#overview","title":"Overview","text":"<p>Provide a brief overview of the TEF Site, including its location, objectives, and role within the CitCom.ai ecosystem. Describe any relevant background information that stakeholders might find helpful.</p> <p>Example:</p> <p>The [TEF Site Name] is located in [City, Country] and is dedicated to advancing research and development in [relevant domain]. This site is equipped with state-of-the-art infrastructure and is a key site in the CitCom.ai project, facilitating collaboration between cities and comunities, industrial partners (AI innovators) and research institutions.</p>"},{"location":"tef/nordic_power/net_zero_innovation_hub/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai Services Catalog. Provide a brief description of each service, and include any relevant links or documentation.</p> <ul> <li>Service 1: [Description of Service 1]</li> <li>Service 2: [Description of Service 2]</li> <li>Service 3: [Description of Service 3]</li> </ul>"},{"location":"tef/nordic_power/net_zero_innovation_hub/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        &lt;no_specified\\&gt; - API: &lt;no_specified\\&gt; - Version: &lt;no_specified\\&gt;      Data Source &lt;no_specified\\&gt; IdM &amp; Auth &lt;no_specified\\&gt; Data Publication &lt;no_specified\\&gt;"},{"location":"tef/nordic_power/net_zero_innovation_hub/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p>"},{"location":"tef/nordic_power/net_zero_innovation_hub/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/nordic_power/net_zero_innovation_hub/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/nordic_power/net_zero_innovation_hub/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: [Name and description of the stakeholder, e.g., university, research institute, industry partner]</li> <li>Stakeholder 2: [Description]</li> <li>Stakeholder 3: [Description]</li> </ul>"},{"location":"tef/nordic_power/net_zero_innovation_hub/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: [Name and contact details]</li> <li>Technical Support: [Name and contact details]</li> <li>General Inquiries: [Name and contact details]</li> </ul>"},{"location":"tef/nordic_power/net_zero_innovation_hub/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/nordic_power/net_zero_innovation_hub/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/nordic_power/rise/","title":"RISE","text":""},{"location":"tef/nordic_power/rise/#overview","title":"Overview","text":"<p>Provide a brief overview of the TEF Site, including its location, objectives, and role within the CitCom.ai ecosystem. Describe any relevant background information that stakeholders might find helpful.</p> <p>Example:</p> <p>The [TEF Site Name] is located in [City, Country] and is dedicated to advancing research and development in [relevant domain]. This site is equipped with state-of-the-art infrastructure and is a key site in the CitCom.ai project, facilitating collaboration between cities and comunities, industrial partners (AI innovators) and research institutions.</p>"},{"location":"tef/nordic_power/rise/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai Services Catalog. Provide a brief description of each service, and include any relevant links or documentation.</p> <ul> <li>Service 1: [Description of Service 1]</li> <li>Service 2: [Description of Service 2]</li> <li>Service 3: [Description of Service 3]</li> </ul>"},{"location":"tef/nordic_power/rise/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        {{ config.extra.labels.data_brokers.fiware }} - API: {{config.extra.labels.api_brokers.ngsi_v2}} {{config.extra.labels.api_brokers.ngsi_ld}} - Version: &lt;no_specified&gt;      Data Source &lt;no_specified&gt; IdM &amp; Auth &lt;no_specified\\&gt; Data Publication &lt;no_specified\\&gt;"},{"location":"tef/nordic_power/rise/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/nordic_power/rise/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/nordic_power/rise/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/nordic_power/rise/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: [Name and description of the stakeholder, e.g., university, research institute, industry partner]</li> <li>Stakeholder 2: [Description]</li> <li>Stakeholder 3: [Description]</li> </ul>"},{"location":"tef/nordic_power/rise/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: [Name and contact details]</li> <li>Technical Support: [Name and contact details]</li> <li>General Inquiries: [Name and contact details]</li> </ul>"},{"location":"tef/nordic_power/rise/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/nordic_power/rise/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/nordic_power/tampere/","title":"Tampere","text":""},{"location":"tef/nordic_power/tampere/#overview","title":"Overview","text":"<p>Provide a brief overview of the TEF Site, including its location, objectives, and role within the CitCom.ai ecosystem. Describe any relevant background information that stakeholders might find helpful.</p> <p>Example:</p> <p>The [TEF Site Name] is located in [City, Country] and is dedicated to advancing research and development in [relevant domain]. This site is equipped with state-of-the-art infrastructure and is a key site in the CitCom.ai project, facilitating collaboration between cities and comunities, industrial partners (AI innovators) and research institutions.</p>"},{"location":"tef/nordic_power/tampere/#services-offered","title":"Services Offered","text":"<p>List the services available at the TEF Site related to the CitCom.ai Services Catalog. Provide a brief description of each service, and include any relevant links or documentation.</p> <ul> <li>Service 1: [Description of Service 1]</li> <li>Service 2: [Description of Service 2]</li> <li>Service 3: [Description of Service 3]</li> </ul>"},{"location":"tef/nordic_power/tampere/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        {{ config.extra.labels.data_brokers.iot_ticket }} - API: {{ config.extra.labels.api_brokers.custom }} - Version: &lt;no_specified&gt;      Data Source &lt;no_specified&gt; IdM &amp; Auth &lt;no_specified\\&gt; Data Publication &lt;no_specified\\&gt;"},{"location":"tef/nordic_power/tampere/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/nordic_power/tampere/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/nordic_power/tampere/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/nordic_power/tampere/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: [Name and description of the stakeholder, e.g., university, research institute, industry partner]</li> <li>Stakeholder 2: [Description]</li> <li>Stakeholder 3: [Description]</li> </ul>"},{"location":"tef/nordic_power/tampere/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: [Name and contact details]</li> <li>Technical Support: [Name and contact details]</li> <li>General Inquiries: [Name and contact details]</li> </ul>"},{"location":"tef/nordic_power/tampere/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/nordic_power/tampere/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/south_connect/milano/","title":"Milano","text":""},{"location":"tef/south_connect/milano/#overview","title":"Overview","text":"<p>The Milano TEF site belongs to the south supernode. It includes two key locations: the UpTown District from Euromilano and the Olympic Village from Covivio. Both areas sit at the heart of major urban regeneration projects, where public and private stakeholders work together to deliver new buildings that blend housing, commercial, and recreational facilities to improve quality of life and promote environmental sustainability. The UpTown District rises in the Cascina Merlata area of Milano, the former Expo 20215 venue. It is a modern neighborhood that hosts over 15,000 residents, with housing and services integrated into the urban fabric. One of Milano's largest parks surrounds the area, offering 300,000 square meters of green space rich in biodiversity. The Olympic Village is taking shape in Porta Romana, a central district of Milano. It will host athletes during the 2026 Winter Olympic Games before being repurposed for student housing and public use. These two sites offer a unique opportunity: test advanced AI in the city's heart. From automated monitoring and predictive maintenance to energy optimization, urban planning, and biodiversity tracking, the UpTown District and Olympic Village serve as real-world labs for intelligent facility management at the metropolitan scale.</p> <p>Politecnico di Milano, Italy's top-ranking technical university, leads the site through the joint work of two departments: DEIB, focused on electronics and information technology, and ABC, focused on architecture, built environment, and construction engineering. Two key partners ensure a direct connection with the urban landscape. Euromilano, a real estate developer specializing in urban regeneration, manages the UpTown testing areas, providing practical integration of AI solutions in real estate and infrastructure projects. Covivio contributes expertise in sustainable urban development and innovative facility management.</p> <p>The Milano TEF site is part of the CitCom.ai project. It features state-of-the-art infrastructure, facilitating collaboration between cities and communities, industrial partners, and research institutions and bringing AI innovation into everyday urban operations. By combining AI-driven sensor networks, facility automation, and edge/cloud computing solutions, the Milano TEF enables real-world validation of AI technologies, unlocking long-term regeneration projects, boosting sustainability, improving quality of life, and strengthening collaboration between academia, industry, and public stakeholders.</p>"},{"location":"tef/south_connect/milano/#services-offered","title":"Services Offered","text":""},{"location":"tef/south_connect/milano/#it-services","title":"IT Services","text":"<ul> <li>Embedded AI/IoT Sofware Design and Development: design and development of embedded AI/IoT software for sensing, computing, and communication; including hardware-specific customization. Not necessarily customer-facing.</li> <li>Networking IoT Devices: design and deployment of low-power wireless sensing technologies, for example, LoRAWAN, including network planning and operation testing. Not necessarily customer-facing.</li> <li>Edge and Cloud Deployment: design and development of edge and cloud software, possibly including concrete operation on Politecnico's data center, enabling efficient and large-scale execution of AI processing. Not necessarily customer-facing.</li> <li>Time Series Storage and Sharing: storage and sharing of temporal data from client-specified or public sources, enabling accesses on time series datasets.</li> <li>FIWARE Big Data Processing: deployment of our FIWARE custom component that enables aggregation and computation of custom metrics based on big data collected from sensors.</li> <li>Large-scale Distributed Sensing: design and deployment of large scale distributed sensing facilities, accounting for requirements of the deployment locations where specific sampling may be necessary.</li> <li>Bare-metal Compute: deployment and access to a bare-metal computing infrastructure hosted in Politecnico di Milano, which provides full control on infrastructure configuration, components, and offers various customization options.</li> </ul>"},{"location":"tef/south_connect/milano/#facility-management-services","title":"Facility Management Services","text":"<ul> <li>Integration of Facilities in Terms of Management: creation of a framework for tender documents, which includes a comprehensive needs analysis developed using the Kano model to help define priorities for public administrations, asset owners, building managers, and facility managers. The framework supports the preparation of tenders for the integrated management of building facilities and neighborhoods. By unifying facilities under a single tender, it enables more efficient service delivery and better coordination across different operational areas.</li> <li>Global service approach: creation of a detailed inventory of all services provided by the project\u2019s creators and providers, grouping them into services for individuals, businesses, and buildings or neighborhoods. This structured mapping unlocks the identification of the services that align best with the needs of key clients such as Facility Management companies, Asset Management firms, Property companies, Utility providers, and Municipalities.</li> <li>Feasibility of the implementation of a service for the Italian market: study of the legal and technical, feasibility of implementing client's services designed and tested abroad for the Italian market. This enables the creation of a framework for local testing and certification path, if required. The service will be provided considering the time-cost-quality triangle.</li> <li>Market Attractiveness Study of Services for the Italian Market: study of the commercial and market viability of the client's services for the local Italian market.</li> </ul>"},{"location":"tef/south_connect/milano/#infrastructure-components","title":"Infrastructure Components","text":"<p>Milano TEF site uses the following technologies:</p> <ul> <li>Data visualization: Grafana, Apache2 webserver with custom web pages</li> <li>Data transmission and interconnection: MQTT, The Things Network (LoRaWAN), FIWARE</li> <li>Data processing and automations: NodeRED, Apache Spark</li> <li>Database: InfluxDB, MySQL/MariaDB, MongoDB</li> <li>AI Tools: Tensorflow, PyTorch </li> <li>Virtualization: Proxmox, Docker</li> <li>Specific Hardware: Raspberry PIs, bare-metal servers, customized sensors hardware</li> </ul> Specifications Data Broker        {{ config.extra.labels.data_brokers.fiware }} - API: {{config.extra.labels.api_brokers.ngsi_v2}} - Version: Orion      Data Source &lt;no_specified&gt; IdM &amp; Auth &lt;no_specified&gt; Data Publication &lt;no_specified&gt;"},{"location":"tef/south_connect/milano/#architecture","title":"Architecture","text":"<p>Milano TEF infrastructure enables seamless integration and real-time processing of IoT data across multiple urban deployment sites located in Italy. The sensors deployed in Cascina Merlata, Porta Romana, and the Mithraeum of Rome transmit data using heterogeneous communication protocols (MQTT, ZenoH, and LoRaWAN via The Things Network). Data incoming from our site locations is unified by corresponding FIWARE IoT Agents and delivered to a central FIWARE Context Broker, representing our infrastructure's central data exchange location. Our infrastructure supports reliable data storage (InfluxDB, MongoDB), continuous analytics, data aggregation using our custom streaming component (EsperTech), and access through Traefik and authentication layers. Finally, visualization and analysis tools (Grafana, Spark, Node-RED, Snap4City) enable real-time monitoring of our infrastructure and allow end users to extract environmental conditions insights, urban infrastructure usage, and system health.</p> <p></p>"},{"location":"tef/south_connect/milano/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/south_connect/milano/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Milano TEF site offers the following datasets:</p> <ul> <li>Air quality: air quality readings from sensors deployed across the Milano area, reporting temperature, humidity, and pollutant levels along with time and location.</li> <li>Biodiversity: data from biodiversity sensors deployed in UpTown, reporting device status, energy levels, and bird species detections over time.</li> <li>Energy distribution: data from heating systems deployed in UpTown, reporting energy use, fluid temperatures, and heating power.</li> <li>Archaeological site: environmental data from sensors deployed in an underground archaeological site, reporting air quality, temperature, humidity, and vibration levels over time.</li> </ul>"},{"location":"tef/south_connect/milano/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<ul> <li>Euromilano: real estate development and consulting company, established in Milan in 1986. The company specializes in forward-thinking urban regeneration and sustainable building projects, collaborating with governments, investors, and corporate partners to create secure, green, and socially engaging spaces that prioritize quality and functionality.</li> <li>Yes Milano, Promos Milano, Chamber of Commerce: Milano FDI ecosystem that is working on the promotion and Foreign Direct Investments attraction in Milano and Lombardia Region together with the Regional Government. The industrial ecosystem integration will be a key success factor for CitCom.AI in terms of Companies involved into TEF\u2019s activity. </li> <li>Ministero delle Infrastrutture (MIT): the Italian Infrastrcutre Ministry. They are keen to support CitCom.AI as part of a wider vision on digitalization of the italian nation, thanks to the cooperation with members of European Parliament and Italian Parliament who endorsed the CitCom.AI mission. Several support services has been provided such as assistance on State Aid rules, open to all the members of the CitCom.AI consortium.</li> </ul>"},{"location":"tef/south_connect/milano/#contact-information","title":"Contact Information","text":"<ul> <li>Site Coordinator: Luca Mottola luca.mottola@polimi.it</li> <li>Technical Support: Andrea Maioli andrea1.maioli@polimi.it</li> <li>General Inquiries: Alberto Celani alberto.celani@polimi.it</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/south_connect/valencia/","title":"Valencia","text":""},{"location":"tef/south_connect/valencia/#overview","title":"Overview","text":"<p>The Valencia TEF site, part of the South Supernode, supports AI-driven innovations in pollution management, noise monitoring, waste and water management, urban development, and tourism. Led by the Valencia City Council (VALE) and Val\u00e8ncia Innovation Capital (LNAV), the TEF provides essential physical and virtual infrastructures, smart city datasets, ecosystem engagement, algorithm expertise, and regulatory compliance support. Key facilities include La Harinera and Las Naves, both serving as innovation hubs, while a citywide sensor network and the VLCi Data Platform offer real-time and historical data for AI experimentation. </p>"},{"location":"tef/south_connect/valencia/#services-offered","title":"Services Offered","text":"<p>Valencia stands at the forefront of AI innovation within the South Supernode, offering a unique combination of resources and expertise. Key assets include the Las Naves facilities, which foster ecosystem engagement and collaboration, and the Urban Sandbox for AI, which supports real-world testing and regulatory compliance. The city's universities drive AI algorithm creation and validation, while S2Grupo provides specialized cybersecurity services. Additionally, the VLCi Data Platform offers access to a rich array of urban data, and the collaborative environment facilitates use case scoping and definition for impactful AI solutions. </p> <ul> <li> <p>Service 1: Dissemination and communication of AI activities    This service focuses on promoting the achievements of the Valencia TEF through events such as workshops, webinars, and presentations. It attracts stakeholders and fosters a community of interest, showcasing AI innovations effectively.</p> </li> <li> <p>Service 2: Development and testing of AI algorithms    The public universities within the consortium offer comprehensive development and testing of AI algorithms, with a focus on data privacy and system optimization, for stakeholders like startups and research institutions.</p> </li> <li> <p>Service 3: Valencia VLCi Platform and datasets    For CitCom.ai project, an independent instance of the VLCi platform (Valencia Urban Platform) has been created and shared with the partners with the aim of testing and experimenting in a controlled environment not only restricted to the partners but also to AI     innovators. A guidelines manual has been created for that purpose. </p> </li> </ul>"},{"location":"tef/south_connect/valencia/#infrastructure-components","title":"Infrastructure Components","text":"<p>Describe the key infrastructure components available at the TEF Site, including data platforms, local digital twins, specific hardware, IoT platforms, or any other relevant technologies.</p> <ul> <li>Data Platforms: [Description of the data platforms available]</li> <li>Local Digital Twins: [Details about any local digital twin infrastructure]</li> <li>Specific Hardware: [Details about specialized hardware available, such as sensors, servers, etc.]</li> <li>IoT Platforms: [Information about IoT systems or platforms in use at the site]</li> <li>Visualization platforms: [Information about large scale visualisation components]</li> <li>Other: [Any other relevant infrastructure to showcase]</li> </ul> Specifications Data Broker        {{config.extra.labels.data_brokers.fiware}} - API: {{config.extra.labels.api_brokers.ngsi_v2}} - Version: Scorpio      Data Source Fiware IoT-Agent IdM &amp; Auth Keycloak Data Publication CKAN"},{"location":"tef/south_connect/valencia/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/south_connect/valencia/#european-data-space-for-smart-communities-ds4sscc","title":"European Data Space for Smart Communities (DS4SSCC)","text":""},{"location":"tef/south_connect/valencia/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_2: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> <li>Dataset_3: [Description of the data set and link to Data Catalog: eg https://citcomai-hub.github.io/data_catalog/metadata_datasets/south_spain_valencia/]</li> </ul>"},{"location":"tef/south_connect/valencia/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: [Name and description of the stakeholder, e.g., university, research institute, industry partner]</li> <li>Stakeholder 2: [Description]</li> <li>Stakeholder 3: [Description]</li> </ul>"},{"location":"tef/south_connect/valencia/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: [Name and contact details]</li> <li>Technical Support: [Name and contact details]</li> <li>General Inquiries: [Name and contact details]</li> </ul>"},{"location":"tef/south_connect/valencia/#additional-information","title":"Additional Information","text":"<p>Any other relevant information that might be useful to collaborators or developers working with the TEF Site, such as specific protocols, access instructions, or unique capabilities.</p> <p>Example: The TEF Site offers unique capabilities in [specific field], and it is open to collaboration with other EU projects in the area of [related field].</p>"},{"location":"tef/south_connect/valencia/#documentation-and-resources","title":"Documentation and Resources","text":"<p>Link to any relevant documentation or resources, such as technical specifications, API documentation, or guides for using services at the TEF Site.</p> <ul> <li>Documentation Link 1</li> <li>Documentation Link 2</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"tef/south_connect/warsaw/","title":"Warsaw","text":""},{"location":"tef/south_connect/warsaw/#overview","title":"Overview","text":"<p>The Warsaw TEF site, part of the South Supernode,\u200b is a computing center dedicated to geospatial data analytics and physical infrastructure for testing devices and applications. The center is based on an advanced IT infrastructure that allows geospatial analysis and satellite calculations.\u200b Warsaw TEF site is led by Warsaw University of Technology and provides computing infrastructure supporting spatial big data and GeoAI computations, diverse geospatial datasets and expertise and support in spatial big data, smart city, GeoAI, indoor navigation and geospatial analysis.</p>"},{"location":"tef/south_connect/warsaw/#services-offered","title":"Services Offered","text":"<p>The Warsaw TEF Site focuses on the use of spatial data and advanced tools for processing very large spatial datasets and artificial intelligence methods to support smart cities and research in wide range of scientific fields. The Site consists of two facilities. The Warsaw Univesrity of Technology main campus facility is the main node of computing infrastructure providing computing capacity within the virtualisation and big data subsystems. The J\u00f3zefoslaw centre houses the physical laboratories for testing tools and applications and the development part of the computer cluster.</p> <ul> <li> <p>Geospatial data for AI training: Access to a wide range of geospatial data covering all of Poland for AI training and model validation. The repository follows FAIR principles to ensure data accessibility and interoperability. </p> </li> <li> <p>Laboratories for testing devices and applications: Laboratories for testing and certifying measurement instruments used in geoinformation acquisition, as well as for geospatial application testing. </p> </li> <li> <p>Computing center for geospatial data analytics: Advanced IT infrastructure for large-scale geospatial analysis and satellite computing, supporting spatial big data applications. </p> </li> <li> <p>Spatial Big Data research platform: Jupyter-based research platform that enables country-scale distributed computations on geospatial big data supporting AI models training and development tailored for fast experimenting and deployment. </p> </li> </ul>"},{"location":"tef/south_connect/warsaw/#infrastructure-components","title":"Infrastructure Components","text":"<ul> <li>Data Platforms: CENAGIS Catalog data management platform based on CKAN system provides information on data available within system.</li> <li>Local Digital Twins: -</li> <li>Specific Hardware: Server infrastructure consisting of three subsystems: 1. big data subsystem with total resources of 1808 vCPUs, 10.6TB RAM, 1440TB HDD, 60 NVIDIA T4 GPUs 2. virtualization subsystem with total resources of 1092 vCPUs, 4.2TB RAM, 365TB HDD, 2*NVIDIA V100, NVIDIA A100 GPUs 3. development subsystem with total resources of 624 vCPUs, 2.4TB RAM, 346TB HDD</li> <li>IoT Platforms: - </li> <li>Visualization platforms: CENAGIS MapViewer spatial data visualization platform based on TerriaMap allows the visualisation and comparative analysis of a wide variety of spatial datasets. It supports visualisation of very large datasets and supports AI-based extensions of functionality.</li> <li>Other: -</li> </ul>"},{"location":"tef/south_connect/warsaw/#architecture","title":"Architecture","text":"<p>Provide a high-level overview of the architecture of the TEF Site, including the key components and technologies used. Include any relevant diagrams or visualizations to help stakeholders understand the infrastructure.</p> <p></p>"},{"location":"tef/south_connect/warsaw/#relevant-datasets-of-the-site","title":"Relevant datasets of the site","text":"<p>Describe the relevant datasets available at the site</p> <ul> <li>Dataset_1: Road network in Poland in form of vector data from Topographic Objects Database (BDOT10k)</li> <li>Dataset_2: Aerial laser scanning point cloud (LIDAR) for part of the city of Warsaw</li> <li>Dataset_3: BIM model of Astronomical-Geodetic Observatory in J\u00f3zefos\u0142aw - location of part of CENAGIS inrastructure, labolatories and test sites</li> </ul>"},{"location":"tef/south_connect/warsaw/#key-stakeholders-and-partners","title":"Key Stakeholders and Partners","text":"<p>Provide a list of the key stakeholders and partners involved in the TEF Site. Include any academic institutions, industry collaborators, and other stakeholders.</p> <ul> <li>Stakeholder 1: Warsaw University of Technology - Creator and host of infrastructure</li> <li>Stakeholder 2: Opegieka Sp. z o.o. - Main partner during system creation process</li> <li>Stakeholder 3: Scientific Network for Geospatial Analysis - Consortium of nearly 30 research units with the aim of consolidating the geoinformation research program in Poland</li> </ul>"},{"location":"tef/south_connect/warsaw/#contact-information","title":"Contact Information","text":"<p>Provide contact details for those responsible for the TEF Site or who can provide more information to collaborators or users.</p> <ul> <li>Site Coordinator: Dariusz Gotlib (email: dariusz.gotlib@pw.edu.pl)</li> <li>Technical Support: Kamil Choroma\u0144ski (email: kamil.choromanski@pw.edu.pl)</li> <li>General Inquiries: Agnieszka Wendland (email: agnieszka.wendland@pw.edu.pl)</li> </ul>"},{"location":"tef/south_connect/warsaw/#additional-information","title":"Additional Information","text":"<p>The TEF Site offers unique capabilities in geospatial analysis, and it is open to collaboration with other EU projects in the area of big data processing, artificial intelligence and smart cities.</p>"},{"location":"tef/south_connect/warsaw/#documentation-and-resources","title":"Documentation and Resources","text":"<ul> <li>CENAGIS Official Site</li> </ul> <p>Info</p> <p>This page is part of the documentation hub for the CitCom.ai project. Please ensure that the information is up-to-date and accurate.</p>"},{"location":"toolbox/","title":"Toolbox","text":"<ul> <li> <p>:material-cogs:{ .lg .middle } AI Toolkit</p> <p>Here we list a series of projects that can significantly help in managing these services.</p> <p>:octicons-arrow-right-24: More Details</p> </li> <li> <p>:material-cogs:{ .lg .middle } AI Logging Monitor</p> <p>The Logging module tracks the model's performance in production.</p> <p>:octicons-arrow-right-24: More Details</p> </li> <li> <p>:material-cogs:{ .lg .middle } MIMs Toolkit</p> <p>List of tools and projects that can help services comply with OASC Minimal Interoperability Mechanisms.</p> <p>:octicons-arrow-right-24: More Details</p> </li> <li> <p>:material-toolbox:{ .lg .middle } Others</p> <p>Other useful tools and projects.</p> <p>:octicons-arrow-right-24: More Details</p> </li> </ul>"},{"location":"toolbox/ai_logging_monitor/","title":"AI system Logging &amp; Monitor","text":"<p>Repository :simple-github:</p>"},{"location":"toolbox/ai_logging_monitor/#how-to-use","title":"How to use","text":"<ol> <li>Download or clone repository.</li> <li>Run <code>pip install -r requeriments.txt</code> + Enter.</li> <li>Run the <code>deploy.sh</code> file, which deploys each service independently:</li> <li>The logging (external) can be consulted at WhyLabs.</li> <li>The monitor interface (internal) can be consulted on port <code>5004</code>.</li> </ol>"},{"location":"toolbox/ai_logging_monitor/#previous-considerations","title":"Previous considerations","text":"<p>For the operation of the Logging and Monitor modules, the ones exposed in this repository, it is necessary a database that stores the predictions and revisions in the expected way, this is:</p> <ul> <li><code>filename</code>: (string) base name of the image.</li> <li><code>output_confidence</code>: (float) (optional) confidence of the max class predicted.</li> <li><code>output_prediction</code>: (int) predicted class.</li> <li><code>output_validated</code>: (bool) if it is validated the prediction.</li> <li><code>timestamp</code>: (string/timestamp) timestamp of the prediction.</li> <li><code>trained</code>: (bool) (optional) if it is already used for retraining.</li> <li><code>logged</code>: (bool) if is is already logged to the external logging.</li> <li><code>timestamp_validated</code>: (string/timestamp) timestamp of the validation.</li> </ul> <p>A MongoDB has been used in the implementation. If you wish to use another database, it would be necessary to update the corresponding part of the code, maintaining the described structure.</p>"},{"location":"toolbox/ai_logging_monitor/#logging-external","title":"Logging (External)","text":"<p>The Logging module tracks the model's performance in production. It logs various performance metrics, including accuracy, precision, recall, and the number of predictions made over time. A monitoring platform visualizes these metrics, helping identify trends and potential issues. Additionally, this module monitors the system for any anomalies or significant drops in performance, triggering alerts when necessary to ensure the model remains reliable and any problems are promptly addressed.</p>"},{"location":"toolbox/ai_logging_monitor/#key-features","title":"Key features","text":"<p>Key features of the module include performance logging, drift detection, data storage, and scheduling.</p> <ul> <li>Performance logging involves logging various aspects of the classification model's performance.</li> <li>Drift detection identifies any changes in the model's performance over time.</li> <li>Data storage ensures that logged performance metrics and related metadata are stored in a database, providing a robust system for retrieving and analyzing historical performance data.</li> <li>A scheduler ensures regular monitoring tasks, ensuring continuous and timely monitoring of the model's performance.</li> </ul>"},{"location":"toolbox/ai_logging_monitor/#functionalities","title":"Functionalities","text":"<p>The module's functionalities include environment configuration, database connection, monitoring tasks, scheduler initialization, and error handling.</p> <ul> <li>Environment configuration involves setting environment variables for the monitoring platform.</li> <li>Database connection uses the provided URI and database name to establish a connection to the database, ensuring reliable database access.</li> <li>The monitoring task fetches new entries from the database and logs their performance metrics, integrating with the logging platform to create and store performance logs.</li> <li>Scheduler initialization sets up a blocking scheduler to run the monitoring task regularly, ensuring the monitoring process runs continuously without manual intervention.</li> <li>Error handling entails logging errors encountered during database connection and monitoring tasks, and providing feedback on the status of the monitoring process.</li> </ul>"},{"location":"toolbox/ai_logging_monitor/#implementation","title":"Implementation","text":"<p>The module has been implemented using WhyLogs and WhyLabs. WhyLogs is a logging library that captures various performance metrics, and WhyLabs is a platform that helps identify trends and potential issues.</p> <p></p>"},{"location":"toolbox/ai_logging_monitor/#monitor-internal","title":"Monitor (Internal)","text":"<p>The Monitor module is designed to track the performance of the classification model in production. Using charts, it provides a web interface that visualizes various performance metrics over a selected time range. The module utilizes a web framework for the backend, a database for data storage, and a charting library for creating interactive charts.</p>"},{"location":"toolbox/ai_logging_monitor/#key-features_1","title":"Key features","text":"<p>Key features of this module include a web interface, data fetching, and charts.</p> <ul> <li>The web interface allows users to monitor model performance through various tabs.</li> <li>Data fetching retrieves logs based on user-specified date ranges, and performance metrics such as precision, accuracy, recall, and F1 score are calculated.</li> <li>Interactive charts visualize these metrics, including line charts for overall metrics and bar charts for class-specific metrics.</li> </ul>"},{"location":"toolbox/ai_logging_monitor/#functionalities_1","title":"Functionalities","text":"<p>The detailed functionalities include environment configuration, database connection, metrics calculation, chart initialization, and web interface tabs.</p> <ul> <li>Environment configuration involves setting up connection details for the database.</li> <li>A connection to the database is established using the provided URI and database name, and data is fetched based on the user-specified date range.</li> <li>Metrics calculation computes daily precision, accuracy, recall, and F1 score, which are then aggregated to provide an overall performance view.</li> <li>Chart initialization visualizes these metrics, with line charts for daily metrics and bar charts for metrics by class.</li> </ul>"},{"location":"toolbox/ai_logging_monitor/#web-interface-tabs","title":"Web interface tabs","text":"<p>The web interface includes several tabs:</p> <ul> <li>The main tab provides an overview of the validated samples and metrics.</li> <li>The metrics tab displays separate line charts for precision, accuracy, recall, and F1 score.</li> <li>The samples by class tab features a pie chart of validated samples by class.</li> <li>The metrics by class tab displays bar charts of precision, recall, and F1 score by class.</li> <li>The training data tab includes a pie chart of samples by class from the training dataset.</li> </ul>"},{"location":"toolbox/ai_logging_monitor/#implementation_1","title":"Implementation","text":"<p>The module has been implemented using Flask and Chart.js, a JavaScript library for creating interactive charts.</p> <p></p>"},{"location":"toolbox/ai_logging_monitor/#alternatives","title":"Alternatives","text":"<p>Additionally, other libraries and platforms were considered:</p> <ul> <li>Evidently, a tool for evaluating and monitoring the performance of machine learning models, providing detailed reports and visualizations of model metrics.</li> <li>Grafana + Prometheus, where Grafana is an open-source platform for monitoring and observability, providing dashboards and visualizations, and Prometheus is a monitoring system and time-series database that collects metrics and data.</li> <li>alibi-detect, a library focused on detecting outliers, adversarial instances, and concept drift in machine learning models, ensuring the robustness and reliability of the deployed models.</li> </ul>"},{"location":"toolbox/ai_toolkit/","title":"AI Toolkit","text":"<p>For the development and implementation of different AI services, here we list a series of projects that can significantly help in managing these services.</p>"},{"location":"toolbox/ai_toolkit/#machine-learning","title":"Machine Learning","text":"Framework Ray Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads. ZenML Develop ML pipelines locally that run on any MLOps stack. Prefect Modern workflow orchestration for data and ML engineers. Platform Kubeflow Machine Learning Toolkit for Kubernetes. Weights &amp; Biases Weights &amp; Biases helps AI developers build better models faster. Quickly track experiments, version and iterate on datasets, evaluate model performance, reproduce models, and manage your ML workflows end-to-end. MLflow Open source platform for the machine learning lifecycle. Library SciKit-Learn Machine learning in Python XGBoost Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library, for Python, R, Java, Scala, C++ and more. Runs on a single machine, Hadoop, Spark, Dask, Flink and DataFlow. Darts A python library for user-friendly forecasting and anomaly detection on time series. OpenCV Open Source Computer Vision Library."},{"location":"toolbox/ai_toolkit/#model","title":"Model","text":"Format &amp; Interface ONNX Open standard for machine learning interoperability. Workflow Airflow A platform to programmatically author, schedule, and monitor workflows. Nifi NiFi automates cybersecurity, observability, event streams, and generative AI data pipelines and distribution for thousands of companies worldwide across every industry."},{"location":"toolbox/ai_toolkit/#deep-learning","title":"Deep Learning","text":"Framework Tensorflow Pytorch Tensors and Dynamic neural networks in Python with strong GPU acceleration. Library Keras Deep Learning for humans. Pytorch Lightning Deep learning framework to train, deploy, and ship AI products Lightning fast. RAPIDS RAPIDS provides unmatched speed with familiar APIs that match the most popular PyData libraries. Built on state-of-the-art foundations like NVIDIA CUDA and Apache Arrow, it unlocks the speed of GPUs with code you already know. OpenMMLab Covers a wide range of research topics of computer vision, e.g., classification, detection, segmentation and super-resolution."},{"location":"toolbox/ai_toolkit/#programming","title":"Programming","text":"Language Python The Python programming language. Library Dask Parallel computing with task scheduling. Numpy The fundamental package for scientific computing with Python. Hydra Hydra is a framework for elegantly configuring complex applications SciPy SciPy library main repository."},{"location":"toolbox/ai_toolkit/#notebook-environment","title":"Notebook Environment","text":"Notebook Environment Jupyter Jupyter Interactive Notebook. Colab Python libraries for Google Colaboratory."},{"location":"toolbox/ai_toolkit/#distributed-computing","title":"Distributed Computing","text":"Computing &amp; Management Docker Podman A tool for managing OCI containers and pods. Kubernetes An open-source system for automating deployment, scaling, and management of containerized applications. Spark A unified analytics engine for large-scale data processing. Portainer Portainer is the most versatile container management software that simplifies your secure adoption of containers with remarkable speed. OpenShift Unified platform to build, modernize, and deploy applications at scale. Work smarter and faster with a complete set of services for bringing apps to market on your choice of infrastructure. ArgoCD Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes."},{"location":"toolbox/ai_toolkit/#data","title":"Data","text":"Relation DB MySQL MySQL Server, the world's most popular open source database, and MySQL Cluster, a real-time, open source transactional database. Postgres Develop ML pipelines locally that run on any MLOps stack. Storage &amp; Format Delta Lake An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs. influxdb Scalable datastore for metrics, events, and real-time analytics. pandas Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more. Versioning DVC ML Experiments Management with Git. Operations Whylogs An open-source data logging library for machine learning models and data pipelines. Provides visibility into data quality &amp; model performance over time. Supports privacy-preserving data collection, ensuring safety &amp; robustness. AI system Logging &amp; Monitor AI system Logging &amp; Monitor (RECICLAI) Hive The Apache Hive \u2122 is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage using SQL. ETL Airbyte The leading data integration platform for ETL / ELT data pipelines from APIs, databases &amp; files to data warehouses, data lakes &amp; data lakehouses. Both self-hosted and Cloud-hosted. Feature Engineering tsfresh tsfresh is a python package. It automatically calculates a large number of time series characteristics, the so called features. Further the package contains methods to evaluate the explaining power and importance of such characteristics for regression or classification tasks. Stream Processing Kafka Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. Flink Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. Visualization D3 Bring data to life with SVG, Canvas and HTML. Plotly-Dash Data Apps &amp; Dashboards for Python. No JavaScript Required. Grafana The open and composable observability and data visualization platform. Visualize metrics, logs, and traces from multiple sources like Prometheus, Loki, Elasticsearch, InfluxDB, Postgres and many more. Prometheus The Prometheus monitoring system and time series database. Streamlit A faster way to build and share data apps. Kibana Run data analytics at speed and scale for observability, security, and search with Kibana. Powerful analysis on any data from any source, from threat intelligence to search analytics, logs to application monitoring, and much more. Gradio Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere! Pipeline Management TPOT TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. Labeling &amp; Annotation Label Studio Label Studio is a multi-type data labeling and annotation tool with standardized output format. CVAT Annotate better with CVAT, the industry-leading data engine for machine learning. Used and trusted by teams at any scale, for data of any scale. Supervisely Develop AI faster and better with on-premise, enterprise-grade end-to-end solution for every task: from labeling to building production models."},{"location":"toolbox/ai_toolkit/#validation","title":"Validation","text":"Validation Evidently AI Evidently helps analyze and monitor the quality of machine learning models in production. It generates detailed reports on data drift and model performance, using visualizations to identify significant changes in input data or model performance. Whylogs Whylogs is a lightweight and scalable library for logging and monitoring ML data in production. It provides statistical profiles of input and output data, facilitating the detection of data drift and anomalies in real-time or batch data. Promehteus &amp; Grafana Although not specific to ML, they can be adapted to monitor specific ML model metrics, including production accuracy. By defining custom metrics that reflect model performance, they can be used to capture and visualize data drift or model drift, though this requires manual configuration and clear metric definitions. Alibi Detect Specialized in anomaly and data drift detection, Alibi Detect offers a series of techniques and algorithms designed specifically to identify changes in input data and model behavior, which may indicate the need for retraining. MLPerf (and MLCommons) MLPerf is a suite of benchmarks that evaluates the performance of hardware, software, and machine learning models. It provides standardized metrics that allow comparing different implementations and configurations of ML, helping to identify best practices and optimizations in the field of machine learning."},{"location":"toolbox/mims_toolkit/","title":"MIMs Tools","text":"<p>List of tools and projects that can help services comply with OASC Minimal Interoperability Mechanisms.</p>"},{"location":"toolbox/mims_toolkit/#mim1-context-information","title":"MIM1 - Context Information","text":"Tool Features Orion Context Broker Orion is a widely adopted and mature context broker implementation developed by the FIWARE community. Scorpio Broker Scorpio Broker is another context broker implementation that conforms to the NGSI-LD standard. Djane.io Djane.io is an open-source context broker implementation that also supports the NGSI-LD standard. It aims to provide a flexible and extensible platform for managing context information Stellio Broker Stellio is a context broker implementation specifically designed for high-performance and large-scale deployments. Lepus An NGSI-LD wrapper for NGSI-v2 Context Brokers, which facilitates the transition or interoperability between both versions of the NGSI specification. QuantumLeap A FIWARE Generic Enabler to support the use of NGSIv2 (and NGSI-LD experimentally) data in time-series databases tutorials NGSI-LD Collection of tutorials for the FIWARE ecosystem designed for NGSI-LD developers. Orange-OpenSource/python-ngsild-client A Python library dedicated to NGSI-LD, which serves both as an NGSI-LD API client and a toolbox for efficiently creating and manipulating NGSI-LD entities. OGC SensorThings API The OGC SensorThings API is a geospatially enabled framework that connects Internet of Things (IoT) devices, data, and applications via the web. This API facilitates interoperability and seamless integration within IoT ecosystems. ngsild-client Python ngsi-ld client for UPV-CitCom.ai projects. A guide for connecting NGSI-V2 Broker to NGSI-LD via FIWARE IoT-Agent Local environment for testing subscriptions between FIWARE Orion brokers NGSI-V2 to NGSI-LD via FIWARE IoT-Agent."},{"location":"toolbox/mims_toolkit/#mim2-data-models","title":"MIM2 - Data Models","text":"Tool Features Smart Data Models An Umbrella Repository for collecting Data Models based on real world use-cases. pySmartDataModels The pysmartdatamodels package provides a comprehensive collection of open-licensed, free data models suitable for digital twin implementations, data spaces, and smart application development. This Python package includes functions for utilizing these data models in development, ensuring compatibility and simplification in digital solutions creation\u200b. SAREF (Smart Applications REFerence) ontology It is a consensus-based model designed to streamline the integration of smart applications by providing a set of modular building blocks. SAREF introduces core concepts, relationships, and axioms for the smart applications domain, emphasizing reuse, modularity, extensibility, and maintainability. DTDL DTDL is based on JSON-LD and is programming-language independent. DTDL isn't exclusive to Azure Digital Twins. It is also used to represent device data in other IoT services such as IoT Plug and Play."},{"location":"toolbox/other_toolkit/","title":"Other tools","text":"<p>List of other useful tools.</p> Tool Features NGSI-LD Grafana datasource plugin It is a Grafana datasource plugin for integrating with FIWARE/NGSI-LD context brokers. It enables the visualization of temporal, geographical, and graph data within Grafana, enhancing the analysis and monitoring capabilities in smart and connected environments. Aq\u00fceducte It is a microservice designed to extract data from diverse sources, convert them to the NGSI-LD format, and import this data for any application utilizing the NGSI-LD protocol. ngsi-ld-client It is a Python-based client for the NGSI-LD API, created from its OpenAPI specification. It is designed to interact with NGSI-LD compliant systems, facilitating operations with the NGSI-LD API through Python. Digital Twin of Industry Fusion It encompasses the development of a Digital Twin Concept aimed at facilitating NGSI-LD based entity management, along with StreamingSQL and SHACL based descriptions of processes. It involves an architectural design that integrates various components such as Kafka, EMQX/MQTT, Scorpio (NGSI-LD Broker), and others to support the digital twin framework. ngsi-ld-deployment It provides sample docker-compose files for deploying NGSI-LD environments. This is useful for setting up and configuring NGSI-LD compliant systems using Docker, streamlining the deployment process for applications that utilize the NGSI-LD protocol. streamlit-app It showcases a web interface for preprocessing data, training, and testing a machine learning model, with data provisioned in real-time via REST API and by means of deploying the FIWARE Context Broker."}]}